[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mathematical Biology",
    "section": "",
    "text": "Preface\nThe course ‚ÄúMathematical Modeling‚Äù has a dual purpose: on one hand, to introduce students to some basic mathematical models in various areas of biology (demography, ecology, infectious diseases, enzyme reactions, physiology, molecular networks); on the other hand, to provide fundamental knowledge in the analysis and numerical simulation of ordinary and partial differential equations.\nSpecifically, the first part of the course is dedicated to modeling using ordinary differential equations and introduces various analytical techniques (linearization, equilibria and their stability, bifurcation, regular and singular perturbations).\n\nOverview of ordinary differential equations (ODEs): Solution of linear equations; equilibria and linearized stability; phase plane, limit cycles; numerical schemes for solving ODEs.\nOne- or two-dimensional models in demography, ecology, epidemiology, and immunology. Non-dimensionalization of variables and parameters.\nSlow-fast systems, enzyme reaction models and their simplification using perturbative methods.\nBifurcation of equilibria and application to predator-prey systems and molecular networks. Simplified models of important biological phenomena, such as the cell cycle and glucose-insulin oscillations.\nExcitable systems: Hodgkin-Huxley equations (overview) and FitzHugh-Nagumo equations.\nParameter estimation for differential models.\n\nIn the second part, partial differential equation models and some techniques for constructing or approximating solutions will be studied. Additionally, some of the most interesting phenomena of reaction-diffusion equations (traveling wave solutions, Turing mechanism) will be presented in a biological context (morphogenesis).\n\nDynamical systems on networks. Examples in epidemiology.\nIntroduction to partial differential equations (PDEs): Solutions by separation of variables. Fourier series. The heat equation and Brownian motion. Eigenfunctions of the Laplacian. Numerical approximation.\nSkellam and Fisher equations: Waveform solutions; stationary solutions of the boundary value problem.\nStability of stationary solutions of reaction-diffusion systems and Turing‚Äôs mechanism for morphogenesis. Conditions for its validity and examples. Chemotaxis: The Keller-Segel model.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01_Bathtub.html",
    "href": "01_Bathtub.html",
    "title": "1¬† Bathtub model",
    "section": "",
    "text": "1.1 The bathtub üõÅ model\nThe models of Newtonian physics are made of differential equations built starting from the second law of the dynamics. The structure of the models discussed here is instead simpler; they are based on the ‚Äúbalance equation of the bathtub‚Äù: if \\(Q(t)\\) is the quantity of a substance in the bathtub we have \\[\n\\frac{\\mathrm{d}Q}{\\mathrm{d}t} = Q'(t) = I(t) - O(t),\n\\]\nwhere\nTo be more precise, the assumption is that, if \\(I_{(t,t+\\Delta t)}\\) is the quantity that enters in the interval \\((t,t+\\Delta t)\\), we have \\(I_{(t,t+\\Delta t)} = I(t)\\Delta t + o(\\Delta t)\\), where \\(o(\\Delta t)\\) is a higher order infinitesimal than \\(\\Delta t\\). Hence: \\[\nI(t) = \\lim_{\\Delta t\\to 0} \\frac{I_{(t,t+\\Delta t)}}{\\Delta t}.\n\\]\nThe input rate \\(I(t)\\) is like an instantaneous velocity: the quantity entered in a given time, when that time becomes very small. Hence \\(I(t)\\) is measured in \\([C][t^{‚àí1}]\\) units where \\([C]\\) represents the concentration of the quantity \\(Q\\). Similarly for the exit rate \\(O(t)\\).\nLet us start from a very simple example. Assume \\(I(t) = \\Lambda\\) constant input flux; \\(O(t) = \\gamma Q(t)\\), i.e.¬†exit flux is proportional to the quantity present at the moment; the proportionality constant \\(\\gamma\\) is often called the exit rate and has the dimension \\([t^{‚àí1}]\\), the inverse of time. From these assumptions we get: \\[\nQ'(t) = \\Lambda - \\gamma Q(t),\n\\tag{1.1}\\]\nsupplemented with some initial condition \\[\nQ(0) = Q_0.\n\\]\nThe solution is: \\[\nQ(t) = e^{-\\gamma t} Q_0 + \\frac{\\Lambda}{\\gamma}\\Bigl( 1 - e^{-\\gamma t} \\Bigr).\n\\]\nNote that if \\(\\Lambda = 0\\) (no input), the solution is simply \\[\nQ(t) = Q_0 e^{-\\gamma t}.\n\\]\nThis means that the survival time of a molecule initially present follows the exponential distribution: \\[\n\\mathbb{P}[\\text{a molecule present at time 0 is present at time $t &gt; 0$}] =\n\\frac{Q(t)}{Q_0} = e^{-\\gamma t}.\n\\]\nFrom the properties of the exponential distribution, we obtain that the mean survival time \\(\\mathbb{E}[T] = 1/\\gamma\\); hence the exit rate \\(\\gamma\\) can be interpreted as the inverse of the mean survival time.\nTo be more precise, let us define a continuous random variable \\(T\\), which measures the lifetime of a particle present in the bathtub. Then, the cumulative distribution \\(F(t)\\) of \\(T\\) is given by \\[\n\\begin{split}\nF(t) &= \\mathbb{P}[T \\le t] \\\\\n&= 1 - \\mathbb{P}[T &gt; t] \\\\\n&= 1 - \\mathbb{P}[\\text{a molecule present at time 0 is present at time $t &gt; 0$}] \\\\\n&= 1 - e^{-\\gamma t}.\n\\end{split}\n\\]\nSo, we indeed have an exponential distribution. The probability density function is: \\[\nf(t) = F'(t) = \\gamma e^{-\\gamma t},\n\\]\nand the expectation is: \\[\n\\mathbb{E}[T] = \\int_0^\\infty t f(t) \\mathrm{d}t = \\frac{1}{\\gamma}.\n\\]",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Bathtub model</span>"
    ]
  },
  {
    "objectID": "01_Bathtub.html#the-bathtub-model",
    "href": "01_Bathtub.html#the-bathtub-model",
    "title": "1¬† Bathtub model",
    "section": "",
    "text": "\\(I(t)\\) is the input rate (quantity that enters per unit time)\n\\(Q(t)\\) is the output rate (quantity that leaves per unit time).\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nSolve Equation¬†1.1 with the method you prefer.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nSolve Equation¬†1.1 with the general formula for linear ODEs, by first defining the matrix exponential (here, just a scalar function).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCompute the above integral explicitly.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Bathtub model</span>"
    ]
  },
  {
    "objectID": "01_Bathtub.html#malthus-equation",
    "href": "01_Bathtub.html#malthus-equation",
    "title": "1¬† Bathtub model",
    "section": "1.2 Malthus equation",
    "text": "1.2 Malthus equation\nThe metaphor of the bathtub can be used to model the dynamics of a population. Neglecting all differences among individuals (due to age, sex, genetic,‚Ä¶) we can represent a population through its size \\(N(t)\\); this will increase through inputs due to births and outputs due to deaths (if immigration and emigration are not considered). Hence \\[\nN'(t) = B(t) - D(t),\n\\]\nwhere \\(B(t) = \\text{births}\\) and \\(D(t) = \\text{deaths}\\).\nMalthus model assumes\n\nwithin a (short) time period of length \\(\\Delta t\\), each individual gives, on average, birth to \\(\\beta\\Delta t\\) new individuals; hence \\(B(t) = \\beta N(t)\\);\nwithin the same time period \\(\\Delta t\\), each individual has probability \\(\\mu\\Delta t\\) of dying; hence \\(D(t) = \\mu N(t)\\).\n\nWe get the following equation \\[\nN'(t) = \\beta N(t) - \\mu N(t) = (\\beta - \\mu)N(t),\n\\]\nthat represents the Malthus model. The parameter \\(\\beta\\) is known as fertility rate, while \\(\\mu\\) is the mortality rate. Finally, \\[\nr = \\beta - \\mu\n\\]\nis the (instantaneous) growth rate and is also called Malthus parameter or biological potential of the population.\nWith the initial condition \\[\nN(0) = N_0,\n\\]\nthe evolution of the population is completely determined. In fact, the solution is \\[\nN(t) = N_0 e^{rt},\n\\]\nand we see that the population will go to extinction or will grow without limits if \\(r &lt; 0\\) or \\(r &gt; 0\\), respectively. If instead \\(r = 0\\), the population size is constant (births and deaths compensate.)\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_theme(\"notebook\", style=\"whitegrid\")\nt = np.linspace(0,1,100)\n\nplt.plot(t,np.exp(1*t),label='r &gt; 0')\nplt.plot(t,np.exp(-1*t),label='r &lt; 0')\nplt.plot(t,np.exp(0*t),label='r = 0')\nplt.grid()\nplt.legend()\nplt.xlabel('Time')\nplt.ylabel('Population')\nplt.show()\n\n\n\n\n\nExample of solutions of Malthus equation",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Bathtub model</span>"
    ]
  },
  {
    "objectID": "02_Population.html",
    "href": "02_Population.html",
    "title": "2¬† Population dynamics",
    "section": "",
    "text": "2.1 Population growth model\nNote: this lecture is based on (Iannelli and Pugliese 2014, secs. 1.1‚Äì1.6).\nWe have seen, using the bathtub analogy, that the fundamental balance equation of population dynamics take the form \\[\nN'(t) = B(t) - D(t) + I(t) - E(t),\n\\]\nwhere at time \\(t\\) we have that\nThere is also an integral interpretation, in fact: \\[\nN(t) - N(t_0) =\n\\underbrace{\\int_{t_0}^t B(s)\\,\\mathrm{d}s}_\\text{births} - \\underbrace{\\int_{t_0}^t D(s)\\,\\mathrm{d}s}_\\text{deaths} + \\underbrace{\\int_{t_0}^t I(s)\\,\\mathrm{d}s}_\\text{immigration} - \\underbrace{\\int_{t_0}^t E(s)\\,\\mathrm{d}s}_\\text{emigration}.\n\\]\nA population growth model is an ODE of the above form with some specific form for each term above. Note that we could provide functions above explicitly, as function of time alone. However, it should be clear that some of them like \\(B(t)\\) or \\(D(t)\\) should depend on \\(N\\).",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Population dynamics</span>"
    ]
  },
  {
    "objectID": "02_Population.html#population-growth-model",
    "href": "02_Population.html#population-growth-model",
    "title": "2¬† Population dynamics",
    "section": "",
    "text": "\\(N(t)\\) is the population,\n\\(B(t)\\) is the birth rate,\n\\(D(t)\\) is the death rate,\n\\(I(t)\\) is the immigration rate, and\n\\(E(t)\\) is the emigration rate.\n\n\n\n\n2.1.1 Malthus model\nQuoting Thomas R. Malthus (1766-1834):\n\nPopulation, when unchecked, increases in a geometrical ratio. Subsistence increases only in an arithmetical ratio. A slight acquaintance with numbers will show the immensity of the first power in comparison of the second.\nEssay on the Principle of Population, 1798\n\nMalthus‚Äô model is mathematical formulation of the above statement. We already derived Malthus model, but let us recall the hypotheses.\n\nThe population is homogeneous, that is all individuals are identical. We have a single class to represent them, that is \\(N(t)\\).\nThe population is isolated, so \\(E(t)=I(t)=0\\).\nThe habitat is invariant, so resources and life conditions are not affected by the environment nor the population itself.\nThe population is very large, so we can consider continuous functions.\nOn a short time scale \\(\\Delta t\\), each individual gives birth to \\(\\beta \\Delta t\\) new individuals, \\(B(t) = \\beta N(t)\\).\nOn a short time scale \\(\\Delta t\\), each individual has probability \\(\\mu \\Delta t\\) of dying, \\(D(t) = \\mu N(t)\\).\n\nThe non-negative parameters \\(\\beta\\) and \\(\\mu\\) are the fertility and mortality rate, respectively. From hypothesis 4, we have that \\(\\mu\\) and \\(\\beta\\) are constant. We introduce the growth rate \\[\nr = \\beta - \\mu,\n\\]\nalso called Malthus parameter or biological potential.\nThe Malthus‚Äô model reads: \\[\nN' = rN, \\quad\\Rightarrow\\quad N(t) = e^{rt}N(0),\n\\]\nthus, when \\(r&gt;0\\), the growth of population is geometrical and unbounded, as predicted by Malthus.\n\n\n2.1.2 Expected life\nWe have seen that, in absence of births, the population goes like \\[\nN(t) = e^{-\\mu t} N_0,\n\\]\nthus, we could say that the probability of surviving up to time \\(t\\) is \\(e^{-\\mu t}\\). Specifically, the life expectancy \\(L\\) is a random variable such that \\[\n\\mathbb{P}[L &gt; t] = e^{-\\mu t}.\n\\]\nThus, the cumulative distribution of \\(L\\) is \\[\nF_L(t) = \\mathbb{P}[L &lt; t] = 1 - \\mathbb{P}[L &gt; t] = 1-e^{-\\mu t},\n\\]\nand the probability density function is \\[\nf_L(t) = F_L'(t) = \\mu e^{-\\mu t}.\n\\]\nWe conclude that \\(L\\sim \\mathrm{Exp}[\\mu]\\), the exponential distribution. The average life expectancy is: \\[\n\\mathbb{E}[L] = \\int_0^\\infty s f_L(s)\\:\\mathrm{d}s = \\int_0^\\infty s \\mu e^{-\\mu s}\\:\\mathrm{d}s = \\frac{1}{\\mu}.\n\\]\nWe have another interpretation of \\(\\mu\\): it is the reciprocal of the expected life time.\n\n\n2.1.3 Basic reproduction number\nLet us rescale the equation and put it in non-dimensionalized form. This is a fundamental step in general, because\n\nit reduces the number of parameters,\nit removed scale effects (units are removed),\nit highlights the determining factors of the model (maybe what matters is not this or that parameter, by their ratio or sum).\n\nHere, we rescale as follows: \\[\n\\tau = \\mu t, \\quad u = N,\n\\]\nso that time is now in units of ‚Äúexpected life time‚Äù: \\(\\tau=1\\) means \\(t=\\mu^{-1} = \\mathbb{E}[L]\\). We have that \\[\nN' = \\frac{\\mathrm{d}N}{\\mathrm{d}t} = \\frac{\\mathrm{d}u}{\\mu^{-1}\\mathrm{d}\\tau} = \\mu\\frac{\\mathrm{d}u}{\\mathrm{d}\\tau} = \\mu\\dot{u}.\n\\]\nWe use the ‚Äúdot‚Äù notation \\(\\dot{u}\\) for the derivative for the non-dimensional form, just to remember that now the time is \\(\\tau\\) and not \\(t\\). We finally obtain: \\[\n\\dot{u} = \\mu^{-1}N' = \\frac{\\beta}{\\mu} N - N = (R_0 - 1) u,\n\\]\nwhere we defined the basic reproduction number \\[\nR_0 = \\frac{\\beta}{\\mu} = \\beta\\mathbb{E}[L].\n\\]\nWe could interpret it as the average number of newborns produced by one individual during his whole life. Note that \\(R_0\\) is non-dimensional.\n\n\n\n\n\n\nExercise\n\n\n\nWhy it does not make much sense to use the scaling \\(\\tau = r t\\)?\n\n\n\n\n2.1.4 Migration\nIn the presence of migration, say with a constant rate, we have the ODE: \\[\nN' = rN + m = f(N),\n\\tag{2.1}\\]\nwhere \\(m = I - E\\). If positive, there is a net immigration, otherwise emigration.\nIn order to study how the model will behave, we have 3 options:\n\nSolve the problem analytically, that is finding \\(N(t) = \\ldots\\) explicitly.\nSolve the problem numerically, which is always possible.\nStudy the problem qualitatively.\n\nThe last option has the advantage that we can be generic, there is no need for a specific value of the parameters or the initial condition. The qualitative study consists in the following steps:\n1) Fixed points of the system. A fixed point or equilibrium (see Definition¬†A.12) is a constant solution of the ODE. We can find it by setting the right hand side to zero: \\[\nN'=0 \\quad\\Leftrightarrow\\quad r N + m = 0.\n\\]\nThe model has a single equilibrium for \\[N = N^* = -\\frac{m}{r} = \\frac{m\\mathbb{E}[L]}{1-R_0}\\]\n2) Biological feasibility. Equilibria must be biologically feasible. For this model, we need to check that \\(N^*\\) is non-negative, otherwise it doesn‚Äôt make sense biologically speaking. Therefore \\[\nN^* \\ge 0, \\quad\\Leftrightarrow\\quad \\text{$m$ and $r$ have opposite sign and $r\\neq 0$.}\n\\]\n3) Local stability. Informally, an equilibrium is locally stable when, starting from a neighborhood of it, the solution stays close to it for \\(t\\to\\infty\\). It is asymptotically stable when the solution converges toward the equilibrium for \\(t\\to\\infty\\). It is unstable otherwise. See Definition¬†A.13 for a more precise statement.\nThe local stability is determined by the sign of the derivative of the right hand side, that is \\(f'(N^*)\\) for \\(f(N)=rN+m\\). In general (see Section A.5.5),\n\nwhen \\(f'(N^*)&lt;0\\), the equilibrium is asymptotically stable, and\nwhen \\(f'(N^*)&gt;0\\), the equilibrium is unstable.\n\nTo see this in this specific case, let us define \\(w(t)=N(t)-N^*\\). Then, \\[\nw' = N' = rN + m = r(N - N^*) = rw.\n\\]\nNote that \\(f'(N) = r\\). We have that: \\[\nw(t) = w_0 e^{rt},\n\\]\nhence,\n\nif \\(r &lt; 0\\), then \\(w \\to 0\\) for all \\(w_0\\), so \\(N(t)\\to N^*\\). The equilibrium is locally asymptotically stable.\nif \\(r &gt; 0\\), then \\(w \\to \\infty\\) and the equilibrium is unstable.\n\n4) Global stability. What if we start very far away from the equilibrium? In this particular case, with a linear ODE, the local stability argument applies also globally, thus the equilibrium is globally attractive. But for general, nonlinear ODEs this may not be the case, so we perform the analysis anyway. Note that if \\(N(0)=N_0 &gt; N^*\\), then \\[\nN'(0) = rN(0) + m = r (N_0 - N^*) &lt; 0,\n\\]\nso the derivative of the solution is negative (assuming \\(r&lt;0\\)). Furthermore, for \\(N(t) &gt; N^*\\), the derivative is always negative. So, the solution must be monotonically decreasing. But the solution is bounded from below by the equilibrium \\(N(t)=N^*\\), so we conclude that: \\[\nN(t) \\to N^*\n\\]\nfor all \\(N_0 \\ge N^*\\). Symmetrically, when \\(N_0 &lt; N^*\\), the derivative is positive and stays positive for all \\(t\\), so the solution is monotonically increasing. Hence: \\[\nN(t) \\to N^*\n\\]\nfor all \\(N^0\\). The equilibrium is therefore globally stable when \\(r&lt;0\\).\n5) Phase portrait. The phase portrait of a dynamical system is the collection of all possible orbits. Here, the phase space is \\(\\Omega=[0,\\infty)\\). The only equilibrium we have, \\(N^*\\), is a barrier to other orbits, because orbits cannot intersect (See Proposition¬†A.2). Therefore:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.reset_defaults()\nsns.set_context('notebook')\nr,m = -0.5,1.7\nNeq = -m/r\n\nf = lambda N: r*N + m\nN = np.linspace(1.5,5,10)\n\nfig, ax = plt.subplots()\nax.spines[[\"bottom\"]].set_position((\"data\", 0))\nax.spines[[\"top\", \"right\", \"left\"]].set_visible(False)\nax.xaxis.set_ticks([])\nax.yaxis.set_ticks([])\nax.plot(1, 0, \"&gt;k\", transform=ax.get_yaxis_transform(), clip_on=False)\nax.set_xlabel('N', loc='right', labelpad=10.0)\n\nax.plot(Neq,0,'r.',markersize=16, zorder=99)\nax.text(Neq, 3e-3, r'$N^*$', fontsize=12, ha='center', va='bottom')\n\nax.quiver(N,0*N, f(N), 0.0, color='blue', zorder=80)\nax.set_xlim((0,6))\nax.set_ylim((-1e-2,1e-2))\nfig.subplots_adjust(left=0, right=1, top=0.1, bottom=0.05)\nplt.show()\n\n\n\n\n\nPhase space\n\n\n\n\nFor this problem we actually have the general solution \\[\nN(t) = (N_0 - N^*)e^{rt} + N^*.\n\\]\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nr,m = -0.5,1.7\nNeq = -m/r\n\nsns.set_theme(\"notebook\", style=\"whitegrid\")\nt = np.linspace(0,10,100)\nN = lambda N0: (N0-Neq)*np.exp(r*t) + Neq\n\nfig, ax = plt.subplots()\nax.plot(t,N(Neq - 1),'b',label='$N_0 &lt; N^*$')\nax.plot(t,N(Neq + 1),'b',label='$N_0 &gt; N^*$')\nfor delta in np.arange(0.1,0.9,0.1):\n    ax.plot(t,N(Neq + delta),'b',lw=0.4)\n    ax.plot(t,N(Neq - delta),'b',lw=0.4)\nax.plot(t,N(Neq),'r-',lw=2,label='$N^*$')\nax.grid()\nax.legend()\nax.set_xlabel('Time')\nax.set_ylabel('Population')\nplt.show()\n\n\n\n\n\nExample of trajectories\n\n\n\n\nThe case \\(m&lt;0\\) and \\(r&gt;0\\) is also interesting. Here, the equilibrium \\(N^*\\) is positive, but unstable. For \\(N_0 &gt; N^*\\) the population still grows exponentially, so emigration as no effect on the overall population. But for \\(N_0 &lt; N^*\\) the solution will become negative in finite time: the model is not correct. The reason is simple: the emigration \\(m\\) cannot be constant (like the immigration), it must depend on \\(N\\) as well.\n\n\n\n\n\n\nExercise\n\n\n\nSolve Equation¬†2.1 with separation of variables.\n\n\n\n\n2.1.5 Exogenous variability\nBy exogenous variability we mean variability in the parameter \\(r\\) that does not depend on the population. (Endogenous variability is when the parameters depend on internal variables like \\(N\\).) Thus, we consider the problem: \\[\nN' = r(t)N,\n\\]\nfor \\(r(t)\\) continuous function. An example could births and deaths that depend on climate or temperature. The solution is: \\[\nN(t) = N_0 e^{\\int_{t_0}^t r(s)\\,\\mathrm{d}s}\n\\]\nWe can also rewrite the solution as: \\[\nN(t) = N_0 e^{(t-t_0)\\frac{1}{t-t_0}\\int_{t_0}^t r(s)\\,\\mathrm{d}s},\n\\]\nshowing that if the limit \\[\nr^* = \\lim_{t\\to\\infty}\\frac{1}{t-t_0}\\int_{t_0}^t r(s)\\,\\mathrm{d}s\n\\]\nexists then the asymptotic behavior of the solution is: \\[\nN(t) \\approx e^{r^*(t-t_0)}, \\quad t \\gg 1.\n\\]\nAn interesting case is when \\(r(t)\\) is periodic, that is there exists \\(T&gt;0\\) such that \\(r(t+T)=r(t)\\). In this case the above formula still applies, but we can ‚Äúforget‚Äù the limit (check Assignment 4 for a proof) and take: \\[\n\\bar{r} = \\frac{1}{T} \\int_0^T r(s)\\,\\mathrm{d}s,\n\\]\nand write the solution for any \\(t \\ge t_0\\) as: \\[\nN(t) = e^{\\bar{r}(t-t_0)}N_\\pi(t),\n\\]\nfor some \\(T\\)-periodic function \\(N_\\pi\\). Note that \\[\nN(t_0 + kT) = e^{\\bar{r}kT}N_\\pi(t_0) = e^{k\\bar{r}}N(t_0),\n\\]\nso after \\(k\\) periods the solution grows by a factor \\(e^{k\\bar{r}}\\). This factor is called Floquet multiplier.\n\n\n\n\nIannelli, Mimmo, and Andrea Pugliese. 2014. An Introduction to Mathematical Population Dynamics. Springer. https://strutture-provincia.primo.exlibrisgroup.com/permalink/39SBT_INST/ghkipk/alma991006883209706186.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Population dynamics</span>"
    ]
  },
  {
    "objectID": "Assignments/01_recap_ODEs.html",
    "href": "Assignments/01_recap_ODEs.html",
    "title": "Assignment 1: Solving ODEs",
    "section": "",
    "text": "The logistic model\nConsider the following model for a population: \\[\nN' = \\beta(N)N - \\mu(N) N,\n\\]\nwhere \\(\\beta(N)\\) is the fertility rate and \\(\\mu(N)\\) is the mortality rate. We assume here both are function of the population size \\(N\\) as follows: \\[\n\\begin{aligned}\n\\beta(N) &= \\beta_0 - \\tilde{\\beta} N, \\\\\n\\mu(N)   &= \\mu_0   + \\tilde{\\mu} N.\n\\end{aligned}\n\\]",
    "crumbs": [
      "Assignments",
      "Assignment 1: Solving ODEs"
    ]
  },
  {
    "objectID": "Assignments/01_recap_ODEs.html#the-logistic-model",
    "href": "Assignments/01_recap_ODEs.html#the-logistic-model",
    "title": "Assignment 1: Solving ODEs",
    "section": "",
    "text": "Show that the model can be written as \\[N' = r\\Bigl(1-\\frac{N}{K}\\Bigr)N.\\tag{1}\\]\n\n\nProve that the ODE, supplemented with an initial condition \\(N(0)=N_0\\), has a unique local solution. Hint: verify the hypotheses of the Cauchy-Lipschitz theorem.\n\n\nConsider \\(u(t) = 1/N(t)\\). Show that \\(u(t)\\) satisfies a linear ODE. Solve it, then use the solution to solve the original ODE in \\(N(t)\\) with the initial condition \\(N(0) = N_0 &gt; 0\\).\n\n\nMake the change of variables as follows: \\(\\tau = r t\\) and \\(y = N/K\\). Find the corresponding equation in \\(y(\\tau)\\).",
    "crumbs": [
      "Assignments",
      "Assignment 1: Solving ODEs"
    ]
  },
  {
    "objectID": "Assignments/01_recap_ODEs.html#blow-up-of-solutions",
    "href": "Assignments/01_recap_ODEs.html#blow-up-of-solutions",
    "title": "Assignment 1: Solving ODEs",
    "section": "Blow-up of solutions",
    "text": "Blow-up of solutions\nIt may be considered reasonable that, for a sexual species, births are proportional to the number of encounters, hence, disregarding the mortality process, we have the equation \\[N'(t) = \\beta N^2(t).\\]\n\nThis equation can be solved by the method of separation of variables. Show that the solutions of this equation with \\(N(0) &gt; 0\\) tend to infinity in a finite time. (Blow-up of solution.)\n\n\nLet us correct the equation, introducing deaths: \\[N'(t) = \\beta N^2(t) ‚àí \\mu N(t).\\]\n\nDiscuss how the dynamics changes. In particular, does the equation still have the problem of solutions going to infinity in a finite time? Hint: do not solve analytically the equation.\n\nSolve the above equation analytically and confirm the qualitative analysis of the previous point.",
    "crumbs": [
      "Assignments",
      "Assignment 1: Solving ODEs"
    ]
  },
  {
    "objectID": "Assignments/01_recap_ODEs.html#mrna",
    "href": "Assignments/01_recap_ODEs.html#mrna",
    "title": "Assignment 1: Solving ODEs",
    "section": "mRNA",
    "text": "mRNA\nZeisel et al. considered the following system for the concentration of mRNA. The variables are \\(M(t)\\), the concentration of mature mRNA, and \\(P(t)\\), the concentration of precursor mRNA: \\[\n\\left\\{\\begin{aligned}\nP'(t) &= b(t) ‚àí \\alpha_1 P(t), \\\\\nM'(t) &= \\alpha_1 P(t) ‚àí \\alpha_2(t) M(t).\n\\end{aligned}\\right.\n\\]\nwhere \\(b(t)\\) is the (gene-specific and time-dependent) production rate, \\(\\alpha_1\\) is the conversion (splicing) rate of pre-mRNA to mRNA, \\(\\alpha_2(t)\\) the (time-dependent) degradation rate of mRNA.\n\nAssume \\(b(t) \\equiv b\\) and \\(\\alpha_2(t) \\equiv \\alpha_2\\) are constant. Find the explicit solution given \\(P(0)=P_0\\) and \\(M(0)=M_0\\). Hint: use the method of variation of constants.\n\n\nAssume now that \\(\\alpha_2(t) \\equiv \\alpha_2\\) and \\[\nb(t) = \\begin{cases}\n\\bar{b} & t &lt; \\bar{t}, \\\\ 0, & t &gt; \\bar{t}.\n\\end{cases}\n\\]\n\nShow that \\(M(t)\\) tends to \\(0\\) as \\(t\\to\\infty\\). At which rate does it decay?",
    "crumbs": [
      "Assignments",
      "Assignment 1: Solving ODEs"
    ]
  },
  {
    "objectID": "Assignments/01_recap_ODEs.html#thorium-uranium-dating",
    "href": "Assignments/01_recap_ODEs.html#thorium-uranium-dating",
    "title": "Assignment 1: Solving ODEs",
    "section": "Thorium-Uranium dating",
    "text": "Thorium-Uranium dating\nThe thorium-uranium method for dating rocks is based on the fact that Uranium-234 decays into Thorium-230 which in turn decays into other elements. Set \\(t = 0\\) the rock formation time and denoting \\(U(t)\\) (resp. \\(T(t)\\)) the amount of Uranium-234 (resp. Thorium-230) in the rock at time \\(t\\) (measured in years), the following differential equation system is written: \\[\n\\left\\{\\begin{aligned}\nU'(t) &= -a U(t), \\\\\nT'(t) &= a U(t) - b T(t), \\\\\nU(0) &= U_0, \\\\\nT(0) &= 0,\n\\end{aligned}\\right.\n\\]\nwhere \\(a \\approx 5.9\\cdot 10^{-6} \\:\\text{years}^{-1}\\), \\(b \\approx 1.9\\cdot 10^{-5} \\:\\text{years}^{-1}\\), \\(U_0\\) represents the initial (generally unknown) amount of Uranium-234. Note that, based on geological principles, it is believed that there was no thorium at the time of rock formation.\n\nSolve the equation for \\(U(t)\\).\n\n\nHow can the quantities of \\(a\\) and \\(b\\) be interpreted? From the data provided can we infer the half-life of Uranium-234 and Thorium-230?\n\n\nCalculate \\(T(t)\\), solution of the second differential equation.\n\n\nCompute \\[\n\\lim_{t\\to\\infty}\\frac{T(t)}{U(t)}.\n\\]\n\n\nExplain why it is possible to estimate the rock age from the knowledge of \\(T / U\\) at current time, but it is not possible from the knowledge of \\(T\\) alone. Hint: study the function \\(T(t)/U(t)\\).",
    "crumbs": [
      "Assignments",
      "Assignment 1: Solving ODEs"
    ]
  },
  {
    "objectID": "Laboratories/01 ODE Integration/lab_ODEinteg.html",
    "href": "Laboratories/01 ODE Integration/lab_ODEinteg.html",
    "title": "Lab 01: Numerical integration",
    "section": "",
    "text": "Stability and convergence\nConsider the IVP (in numerics, this is also called test problem): \\[\\begin{cases}\ny' = \\lambda y, \\\\\ny(t_0) = y_0. &\n\\end{cases}\\]",
    "crumbs": [
      "Labs",
      "Lab 01: Numerical integration"
    ]
  },
  {
    "objectID": "Laboratories/01 ODE Integration/lab_ODEinteg.html#stability-and-convergence",
    "href": "Laboratories/01 ODE Integration/lab_ODEinteg.html#stability-and-convergence",
    "title": "Lab 01: Numerical integration",
    "section": "",
    "text": "Write down the forward Euler scheme and solve for the equation for various values of \\(h\\), with \\(\\lambda = -1\\). Is it always true that \\(y_n \\to 0\\) for \\(n\\to\\infty\\)? Use \\(t_0=0\\) and \\(y_0=1\\).\n\n\nWrite down the backward Euler scheme and repeat the previous step.\n\n\nSolve the equation with the forward Euler scheme and compare the solution to the exact one. How does the absolute error decrease with respect to \\(h\\)? Plot the error in the logarithmic scale. Set \\(\\lambda=1\\) and \\(y(0)=1\\).\n\n\nRepeat the previous step with the Heun scheme: \\[y_{n+1}=y_n + \\tfrac{h}{2}\\bigl( f(y_n) + f(y_n + hf(y_n)) \\bigr).\\]",
    "crumbs": [
      "Labs",
      "Lab 01: Numerical integration"
    ]
  },
  {
    "objectID": "Laboratories/01 ODE Integration/lab_ODEinteg.html#nonlinear-equations",
    "href": "Laboratories/01 ODE Integration/lab_ODEinteg.html#nonlinear-equations",
    "title": "Lab 01: Numerical integration",
    "section": "Nonlinear equations",
    "text": "Nonlinear equations\nConsider the Nagumo or bistable model: \\[u' = a u (1-u)(u-\\alpha),\\]\nwith \\(\\alpha=0.2\\) and \\(a=10\\). Implement and solve the model in \\(t=0,\\ldots,2\\) with forward Euler for \\(h=10^{-3}\\) and various values of \\(u(0)\\).",
    "crumbs": [
      "Labs",
      "Lab 01: Numerical integration"
    ]
  },
  {
    "objectID": "Laboratories/01 ODE Integration/lab_ODEinteg.html#van-der-pol-equation",
    "href": "Laboratories/01 ODE Integration/lab_ODEinteg.html#van-der-pol-equation",
    "title": "Lab 01: Numerical integration",
    "section": "Van Der Pol equation",
    "text": "Van Der Pol equation\nHere a more complicate example of a system. We solve the Van Der Pol equation: \\[y'' - \\mu(1-y^2)y' + y = 0.\\]\n\nSolve the van der Pol equation with forward Euler, using \\(\\mu=5\\), \\(y(0)=2\\) and \\(y'(0)=0\\). How small \\(h\\) needs to be? Find the approximate stability limit by bisection.\n\n\nSolve the van der Pol equation with backward Euler. Design a strategy to solve the non-linear equation. Is the scheme always stable?",
    "crumbs": [
      "Labs",
      "Lab 01: Numerical integration"
    ]
  },
  {
    "objectID": "Laboratories/01 ODE Integration/lab_ODEinteg.html#conservation-of-energy",
    "href": "Laboratories/01 ODE Integration/lab_ODEinteg.html#conservation-of-energy",
    "title": "Lab 01: Numerical integration",
    "section": "Conservation of energy",
    "text": "Conservation of energy\nConsider the 2nd-order ODE \\(y'' + \\omega^2 y = 0\\) (linear oscillator).\n\nAfter recasting the equation to a first-order system, solve it with forward Euler using \\(\\omega = 1\\), \\(h=0.01\\), \\(y(0)=1\\), \\(y'(0)=0\\) and \\(t=0,\\ldots,100\\). Does the solution match the expected behavior?\n\n\nTry again with backward Euler. Hint: the system is linear, you can use the backslash to solve it at each iteration.\n\n\nCompute the discrete energy of the system \\(E(t) = \\tfrac{1}{2}\\bigl( (y'(t))^2 + \\omega^2 y(t)^2 \\bigr)\\) and inspect the problem.\n\n\nImplement the following strategy: first, update the velocity, then the position with the newly compute velocity. (This is called the symplectic Euler method.) Does the method conserve the (discrete) energy?",
    "crumbs": [
      "Labs",
      "Lab 01: Numerical integration"
    ]
  },
  {
    "objectID": "IntroODE.html",
    "href": "IntroODE.html",
    "title": "Appendix A ‚Äî Introduction to ODEs",
    "section": "",
    "text": "A.1 Basic definitions\nLet us start with some simple definitions about Ordinary Differential Equations (ODEs).\nLet‚Äôs have a look to some of the equations we are going to deal with during this course. We will analyze the equations for a purely mathematical point of view, with no biological insights: this will come later.\nIn general, an ODE does not tell anything about a specific solution; it is just the law of motion. We can have many solutions. As a rule of thumb, an \\(n\\)-order scalar ODE requires \\(n\\) constants for uniquely defining a solution. An Initial Value Problem (IVP) is an ODE supplemented wuth additional conditions. It is called ‚Äúinitial condition‚Äù because it sets the value of the solution at some initial time \\(t_0\\). (Please note that an IVP is also called ‚ÄúCauchy problem‚Äù on some books.)\nIn all the above cases, the ODEs were scalar. A system of ODEs is the natural generalization to \\(\\mathbb{R}^n\\). We will only consider first-order system of ODEs, because their extension to higher-order ODEs is superfluous (we will see why in a moment.)\nConsider the system of first-order ODEs: \\[\n\\left\\{\\begin{aligned}\ny_1' &= f_1(t,y_1,y_2,\\ldots,y_n), \\\\\ny_2' &= f_2(t,y_1,y_2,\\ldots,y_n), \\\\\n& \\cdots  \\\\\ny_n' &= f_n(t,y_1,y_2,\\ldots,y_n). \\\\\n\\end{aligned}\\right.\n\\]\nWe set \\(\\boldsymbol{y}(t) = (y_1,\\ldots,y_n)^\\intercal\\) and \\(\\boldsymbol{f}(t,\\boldsymbol{y})=(f_1,\\ldots,f_n)^\\intercal\\) so the system rewrites to \\[\n\\boldsymbol{y}' = \\boldsymbol{f}(t,\\mathbf{y}).\n\\]\n(Please note that the time derivative of the vector is the vector of the derivatives.) In compact form, the IVP reads as: \\[\n\\left\\{\\begin{aligned}\n\\boldsymbol{y}' = \\boldsymbol{f}(t,\\mathbf{y}), \\\\\n\\boldsymbol{y}(t_0) = \\boldsymbol{y}_0. \\\\\n\\end{aligned}\\right.\n\\tag{A.1}\\]\nThe solution \\(\\boldsymbol{\\phi}\\in\\mathcal{C}^1(I,\\mathbb{R}^n)\\) of the system is a curve in \\(\\mathbb{R}^n\\) such that: \\[\n\\boldsymbol{\\phi}'(t) = \\boldsymbol{f}(t,\\boldsymbol{\\phi}(t)),\\quad\\text{for all $t\\in I$}.\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Introduction to ODEs</span>"
    ]
  },
  {
    "objectID": "IntroODE.html#basic-definitions",
    "href": "IntroODE.html#basic-definitions",
    "title": "Appendix A ‚Äî Introduction to ODEs",
    "section": "",
    "text": "Definition A.1 (ODE) An scalar ODE is an equation (generally nonlinear) involving a function \\(y(t)\\) and its derivatives. Let \\(f\\colon D\\to\\mathbb{R}\\), \\(D\\subseteq\\mathbb{R}^{n+1}\\) open set not empty. Then an ODE of order \\(n\\) is an equation of the type: \\[y^{(n)} = f(t,y,y',\\ldots,y^{(n-1)}).\\]\n\n\n\n\n\n\n\n\nExample: Malthusian growth model\n\n\n\n\\[N'(t) = r\\,N(t).\\]\nThis is a simple model of population growth where \\(N(t)\\) represents the population size at time \\(t\\). We have that \\(f(t,N) = rN\\). It is a linear ODE (\\(f\\) is linear with respect to \\(N\\)), with constant coefficients (\\(r\\) is a constant), first-order, and autonomous (\\(f\\) does not explictly depend on \\(t\\)).\n\n\n\n\n\n\n\n\nExample: Logistic Growth Model\n\n\n\n\\[\nN'(t) = r N\\left(1 - \\frac{N}{K}\\right)\n\\] The logistic model modifies the Malthusian growth by introducing a carrying capacity. We have that \\(f(t,N) = rN(1-N/K)\\). It is a nonlinear ODE (\\(f\\) is non-linear with respect to \\(N\\)), first-order, and autonomous.\n\n\n\n\n\n\n\n\nExample: Damped Harmonic Oscillator\n\n\n\n\\[\ny''(t) + 2\\beta y'(t) + \\omega_0^2 y(t) = F(t).\n\\] This is a second-order ODE, linear, constant coefficients, non-autonomous (because of the presence of \\(F(t)\\)).\n\n\n\n\nDefinition A.2 (Initial Value Problem) An initial value problem (IVP) is a system of the form: \\[\n\\left\\{\\begin{aligned}\ny^{(n)} &= f(t,y,y',\\ldots,y^{(n-1)}), &\\\\\ny(t_0) &= y_0, \\\\\ny'(t_0) &= y_1, \\\\\n&\\vdots \\\\\ny^{(n-1)}(t_0) &= y_{n-1}. \\\\\n\\end{aligned}\\right.\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Introduction to ODEs</span>"
    ]
  },
  {
    "objectID": "IntroODE.html#separation-of-variables",
    "href": "IntroODE.html#separation-of-variables",
    "title": "Appendix A ‚Äî Introduction to ODEs",
    "section": "A.2 Separation of variables",
    "text": "A.2 Separation of variables\nSolving ODE analytically is not always possible. As for integration, sometimes we cannot find a solution in closed form. However, we have alternatives to rescue us:\n\nQualitative analysis: we don‚Äôt solve the ODE explicitly, we rather analyse the behaviour of the trajectories as we would do for a function study. Very often we proceed graphically (in 1-D and 2-D). It is very helpful for parametric studies.\nNumerical integration: we solve the IVP on a computer. We have no limits, but we always need to provide all parameters and initial values. Parametric studies are more difficult.\n\nA combination of the above approaches in always wise. On the other hand, it is good to know how to integrate an ODE when it is possible.\nA simple yet effective technique to solve IVPs is the method of separation of variables. It applies to IVPs of the form: \\[\n\\begin{cases}\ny' = f(t)g(y), \\\\\ny'(t_0) = y_0.\n\\end{cases}\n\\]\nfor some functions \\(f\\) and \\(g\\). Using the Leibniz notation, we write the ODE as follows: \\[\n\\newcommand{\\dd}{\\mathrm{d}}\n\\frac{\\dd y}{\\dd t} = f(t)g(y),\n\\]\nwhich suggests to separate the variables \\(t\\) and \\(y\\): \\[\n\\newcommand{\\dd}{\\mathrm{d}}\n\\frac{\\dd y}{g(y)} = f(t)\\,\\dd t.\n\\]\nHere we consider \\(y\\) and \\(t\\) as independent variables. The advantage is that we can integrate both sides to get rid of differentials: \\[\n\\newcommand{\\dd}{\\mathrm{d}}\n\\int_{y_0}^y \\frac{\\dd \\tilde y}{g(\\tilde y)} = \\int_{t_0}^t f(\\tilde t)\\,\\dd \\tilde t.\n\\]\nPlease note that amongst all anti-derivatives, we select those satisfying the initial condition \\(y(t_0)=y_0\\). Suppose that \\(F(t)\\) and \\(G(y)\\) are respectively anti-derivatives of \\(f(t)\\) and \\(\\frac{1}{g(y)}\\), i.e.¬†\\(F'(t)=f(t)\\) and \\(G'(y)=\\frac{1}{g(y)}\\). Then, according to the Fundamental Theorem of Calculus, we have: \\[\nG(y) - G(y_0) = F(t) - F(t_0),\n\\]\nwhich gives an implicit expression for \\(y(t)\\): \\[\nG\\bigl(y(t)\\bigr) = F(t) + G(y_0) - F(t_0).\n\\]\nPlease note that the expression on the right hand side only contains the variable \\(t\\). When \\(G\\) is invertible, the expression of \\(y(t)\\) can be made explicit.\n\n\n\n\n\n\nExample (Malthusian growth model)\n\n\n\nWe aim at solving the following IVP: \\[\n\\begin{cases}\nN' = r N, \\\\\nN(t_0) = N_0.\n\\end{cases}\n\\tag{A.2}\\]\nWe apply the method of separation of variables. We have: \\[\n\\newcommand{\\dd}{\\mathrm{d}}\n\\int_{N_0}^N \\frac{\\dd\\tilde N}{\\tilde N} = \\int_{t_0}^t r\\,\\dd\\tilde{t}.\n\\]\nAfter integration (recall that \\(\\ln'(y) = 1/y\\)): \\[\n\\ln\\Bigl(\\tfrac{N}{N_0}\\Bigr) = r (t-t_0),\n\\]\nor, by inverting the logarithm, \\[\nN(t) = N_0 e^{r (t-t_0)}.\n\\]\n\n\n\n\n\n\n\n\nExercise (Time-dependent Malthus equation)\n\n\n\nCheck that \\(N(t) = N_0 e^{r (t-t_0)}\\) solves the problem (Equation¬†A.2).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Introduction to ODEs</span>"
    ]
  },
  {
    "objectID": "IntroODE.html#well-posedness",
    "href": "IntroODE.html#well-posedness",
    "title": "Appendix A ‚Äî Introduction to ODEs",
    "section": "A.3 Well-posedness",
    "text": "A.3 Well-posedness\nLet us recall this (simplistic) view of mathematical modeling:\n\n\n\n\n\nflowchart LR\n    id1[(Physical world)] ---|Approximation| id2[(Mathematical model)] ---|Approximation| id3[(Numerical model)]\n\n\n\n\n\n\nA good mathematical model shall approximate well the physical world. For instance, it is able to fit experimental data in a validation test. However, a validation test requires that we are able to solve the mathematical problem. For that, we require that the problem should satisfy well-posedness properties.\nWe are dealing with IVPs. Here, well-posedness translates into:\n\nExistence of a solution \\(\\boldsymbol{\\phi} \\in \\mathcal{C}^1([0,T],\\mathbb{R}^n)\\) for every choice of the initial data;\nUniqueness of the solution, in the sense that if \\(\\boldsymbol{\\phi}_1(t)\\) and \\(\\boldsymbol{\\phi}_2(t)\\) are both solutions of the IVP for the same initial data, then \\(\\boldsymbol{\\phi}_1(t)=\\boldsymbol{\\phi}_2(t)\\) for all \\(t\\).\nStability of the solution to perturbations, that is if \\(\\tilde{\\boldsymbol{\\phi}}(t)\\) is the solution of the IVP with some perturbation, e.g., applied to the initial datum or to the right hand side, then \\(\\|\\tilde{\\boldsymbol{\\phi}}-\\boldsymbol{\\phi}\\|\\to 0\\) as the perturbation goes to zero.\n\nFor IVPs, stability to perturbation is also called zero stability. The zero stability for initial data follows from the Cauchy-Lipschitz theorem, as shown below.\nFor the numerical realm, we have a similar definition of well-posedness. Additionally, we typically need to show that the numerical solution converges (in some sense) to the analytical solution. That is, the approximation error goes to zero as we refine the numerical problem. For an IVP, convergence means that that the error between the numerical solution and the true solution goes to zero as the time step goes to zero.\nWe start with a definition of a solution for the IVP.\n\nDefinition A.3 (Classic solution) A classic solution of the IVP (Equation¬†A.1) is a function \\(\\boldsymbol{\\phi}\\in\\mathcal{C}^1(I;\\mathbb{R}^n)\\), where \\(I\\subset\\mathbb{R}\\) is a closed interval, such that\n\n\\(\\boldsymbol{\\phi}'(t) = \\boldsymbol{f}(t,\\boldsymbol{\\phi}(t))\\) for all \\(t\\in I\\);\n\\(\\boldsymbol{\\phi}(t_0) = \\boldsymbol{y}_0\\);\n\\((t,\\boldsymbol{\\phi}(t))\\subset D\\) for all \\(t\\in I\\), where \\(D\\subset\\mathbb{R}^{n+1}\\) is the domain of \\(\\boldsymbol{f}\\).\n\n\nThe first 2 conditions are simply the IVP (equation and initial condition). The third condition concern the domain of definition of the right hand side of the ODE.\n\n\n\n\n\n\nExample (lack of existence)\n\n\n\nWhen the \\(\\boldsymbol{f}\\) is not continuous, we cannot expect existence in general. In fact, consider the IVP \\[\n\\begin{cases} y' = \\mathcal{H}(t) = \\begin{cases} 0, & t\\le 0,\\\\ 1, & t &gt; 0,\\end{cases} \\\\\ny(0) = y_0.\n\\end{cases}\n\\]\nA possible and intuitive solution is \\[\n\\phi(t) = y_0 + \\max\\{0, t\\} = \\begin{cases} y_0, & t\\le 0,\\\\ y_0 + t, & t&gt;0.\\end{cases}\n\\]\nThe function \\(\\phi(t)\\) is, however, not \\(\\mathcal{C}^1(I)\\) for any neighborhood \\(I\\) of the initial time \\(t_0=0\\). Therefore, the function cannot be a solution, at least according to the above definition of solution. Note that with a different initial condition, say \\(y(1) = y_0\\), we can find a \\(\\mathcal{C}^1(I)\\) solution, for \\(I\\) away from \\(t=0\\).\n\n\n\nA.3.1 Local well-posedness\nA general result for the local existence of a solution is due to Giuseppe Peano:\n\nTheorem A.1 (Peano) Let \\(\\mathbf{f}\\colon D \\to\\mathbb{R}^n\\), \\(D\\subseteq\\mathbb{R}^{n+1}\\) open set, be continuous. Then, for all \\((t_0,\\mathbf{y}_0)\\in D\\), there exists a neighborhood of \\(t_0\\), denoted by \\(I_\\delta = [t_0-\\delta,t_0+\\delta]\\) with \\(\\delta &gt; 0\\), on which is possible to construct a solution \\(\\boldsymbol{\\phi}\\in\\mathcal{C}^1( I_\\delta; \\mathbb{R}^n)\\) to the IVP.\n\nPeano‚Äôs theorem ensures the existence but not the uniqueness. He also provided a simple counterexample to uniqueness.\n\n\n\n\n\n\nPeano‚Äôs counterexample on uniqueness\n\n\n\nConsider the ODE \\[\n\\begin{cases}\ny' = \\sqrt[3]{y}, \\\\\ny(1) = 0.\n\\end{cases}\n\\]\nThe function \\(f(t,y) = \\sqrt[3]{y}\\) is continuous with respect to \\(t\\) and \\(y\\) (note that it does not explicitly depends on time), thus Peano‚Äôs Theorem ensures the existence of at least one solution.\nWe can find a solution by separation of variables. We have (please check!): \\[\n\\phi_1(t) = \\begin{cases}\n\\bigl(\\tfrac{2}{3} (t-1)\\bigr)^{3/2}, & t \\ge 1, \\\\\n0, & t &lt; 1.\n\\end{cases}\n\\]\nThis solution is however not the only one. Also the following function is a solution (again, please check!) \\[\n\\phi_2(t) = \\begin{cases}\n-\\bigl(\\tfrac{2}{3} (t-1)\\bigr)^{3/2}, & t \\ge 1, \\\\\n0, & t &lt; 1.\n\\end{cases}\n\\]\nYet another solution is \\(\\phi_3(t)\\equiv 0\\). Actually, the IVP is so subtle that we can build an infinite number of solutions with the power of continuum! In fact, for any \\(\\alpha\\ge 1\\), we have that \\[\n\\phi_{\\alpha^\\pm}(t) = \\begin{cases}\n\\pm\\bigl(\\tfrac{2}{3} (t-\\alpha)\\bigr)^{3/2}, & t \\ge \\alpha, \\\\\n0, & t &lt; \\alpha,\n\\end{cases}\n\\]\nare all solutions of the IVP. That‚Äôs a lot to deal with.\n\n\n\nPeano‚Äôs brush\n\n\n\n\nThe example shows that the lack of uniqueness is somehow related to a lack of stability of the solution. Consider a particle located, at \\(t=1\\), in \\(y=0\\). The velocity at some generic time \\(t\\) is \\(y'(t) = \\sqrt[3]{y(t)}\\). Then, the trajectory of the particle is exactly the solution of Peano‚Äôs example.\nThe example shows that we need some control on the growth rate of \\(\\boldsymbol{f}\\). The correct concept is the Lipschitz continuity:\n\nDefinition A.4 (Lipschitz continuity) Let \\(\\boldsymbol{f}\\colon \\Omega \\to \\mathbb{R}^n\\), \\(\\Omega\\subseteq\\mathbb{R}^n\\), a continuous function. We say that \\(\\boldsymbol{f}\\) is Lipschitz continuous if there exists a constant \\(L&gt;0\\) such that \\[\n\\| \\boldsymbol{f}(\\boldsymbol{y}) - \\boldsymbol{f}(\\boldsymbol{z}) \\| \\le L\\|\\boldsymbol{y}-\\boldsymbol{z}\\|,\n\\]\nfor all \\(\\boldsymbol{y},\\boldsymbol{z}\\in\\Omega\\).\n\n\nDefinition A.5 (Local Lipschitz continuity) Let \\(\\mathbf{f}\\colon \\Omega \\to \\mathbb{R}^n\\) a continuous function, with \\(\\Omega\\subseteq\\mathbb{R}^n\\) open domain. We say that \\(\\boldsymbol{f}\\) is locally Lipschitz continuous in \\(\\Omega\\) if for every \\(\\boldsymbol{y}_0\\in\\Omega\\) there exists a neighborhood of \\(\\boldsymbol{y}_0\\) in which \\(\\boldsymbol{f}\\) is Lipschitz continuous, with a constant \\(L\\) possibly depending on the neighborhood.\n\n\n\n\n\n\n\nExercise\n\n\n\nShow that \\(\\mathcal{C}^1\\) functions are always locally Lipchitz. For simplicity consider functions on \\(\\mathbb{R}\\).\n\n\n\nTheorem A.2 (Cauchy-Lipschitz) Let \\(\\boldsymbol{f}\\in\\mathcal{C}(D;\\mathbb{R}^n)\\), \\(D\\subseteq\\mathbb{R}^{n+1}\\) open domain. If \\(\\boldsymbol{f}\\) is locally Lipschitz in \\(D\\), with respect to \\(\\boldsymbol{y}\\) and uniformly in \\(t\\), then for every \\((t_0,\\boldsymbol{y}_0)\\in D\\) there exists an interval \\(I_\\delta = [t_0-\\delta,t_0+\\delta]\\), with \\(\\delta&gt;0\\), on which is possible to construct a solution \\(\\boldsymbol{\\phi}\\in\\mathcal{C}^1( I_\\delta; \\mathbb{R}^n)\\) to the IVP (Equation¬†A.1). Such solution is also unique, in the sense that every other solution of the IVP coincides with \\(\\boldsymbol{\\phi}(t)\\) on \\(I_\\delta\\).\n\nThe Cauchy-Lipschitz theorem has very important consequences on the study of ODEs. We mention here the following\n\nCorollary A.1 (Zero stability) Consider the following IVPs: \\[\n\\left\\{\\begin{aligned}\n\\boldsymbol{y}' &= \\boldsymbol{f}(t,\\mathbf{y}), \\\\\n\\boldsymbol{y}(t_0) &= \\boldsymbol{y}_0, \\\\\n\\end{aligned}\\right. \\quad\\text{and}\\quad\n\\left\\{\\begin{aligned}\n\\tilde{\\boldsymbol{y}}' &= \\boldsymbol{f}(t,\\tilde{\\mathbf{y}}), \\\\\n\\tilde{\\boldsymbol{y}}(\\tilde{t}_0) &= \\tilde{\\boldsymbol{y}}_0. \\\\\n\\end{aligned}\\right.\n\\]\nSuppose we are under the hypotheses of the Cauchy-Lipschitz theorem. Then, we have the following stability estimate: \\[\n\\| \\tilde{\\boldsymbol{y}}(t) - \\boldsymbol{y}(t) \\| \\le M |\\tilde{t}_0 - t_0| + e^{L|t-t_0|} \\| \\tilde{\\boldsymbol{y}}_0 - \\boldsymbol{y}_0 \\|,\n\\] where \\(L\\) is the Lipschitz constant of \\(\\boldsymbol{f}\\).\n\nIn other words, if we perturb the initial value of the IVP, the perturbed solution remains close to the unperturbed solution as long as the perturbation is small and small time. For large time, however, the initial perturbation may grow exponentially, with a rate proportional to the Lipschitz constant.\n\n\nA.3.2 Global well-posedness\nThe solution provided by the Cauchy-Lipschitz Theorem is only defined in a local interval, that is up to \\(t=t_0+\\delta\\), with \\(\\delta &gt; 0\\). Unfortunately, \\(\\delta\\) can be small, thus there is no guarantee that we can integrate the IVP for an arbitrarily long time.\nWe can try to circumvent the problem as follows. Let us call \\(\\boldsymbol{\\phi}_1(t)\\) the solution and \\(\\delta_1 = \\delta\\). Now we set up a new problem of the form: \\[\n\\begin{cases}\n\\boldsymbol{y}' = \\boldsymbol{f}(t,\\boldsymbol{y}), & \\\\\n\\boldsymbol{y}(t_1) = \\boldsymbol{y}_1,  & \\\\\n\\end{cases}\n\\]\nwhere \\(t_1 = t_0 + \\delta_1\\) and \\(\\boldsymbol{y}_1 = \\boldsymbol{\\phi}_1(t_1)\\). Since also the above problem is well-posed, we get a new solution \\(\\boldsymbol{\\phi}_2(t)\\) up to \\(t_2=t_1+\\delta_2\\), for some \\(\\delta_2&gt;0\\).\nSo, we can define a new solution on the interval \\([t_0,t_0 + \\delta_1+\\delta_2]\\) by gluing the solutions around \\(t_1\\): \\[\n\\boldsymbol{\\phi}(t) = \\begin{cases}\n\\boldsymbol{\\phi}_1(t), & t \\in [t_0,t_0+\\delta_1], \\\\\n\\boldsymbol{\\phi}_2(t), & t \\in [t_0+\\delta_1,t_0+\\delta_1+\\delta_2].\n\\end{cases}\n\\]\nPlease note that \\(\\boldsymbol{\\phi}_1(t_1)=\\boldsymbol{\\phi}_2(t_1)\\) by construction, and the same applies to the derivative, so the full solution is still \\(\\mathcal{C}^1([t_0, t_0+\\delta_1+\\delta_2])\\).\nBy iterating the process, we obtain a sequence \\(\\{ \\delta_n \\}\\) of extensions. The hope is that \\(\\sum_n \\delta_n \\to \\infty\\), that is we can extend the solution indefinitely. However, it is certainly possible that \\(t_0 + \\sum_n \\delta_n \\to T_\\text{max} &lt; \\infty\\).\n\nDefinition A.6 (Maximal interval) The maximal right interval \\([t_0,T_\\text{max})\\) is simply the maximum time \\(T_\\text{max}\\) up to which we can extend the solution to the right. In general the interval is open. Similarly, the left maximal interval is \\((T_\\text{min},t_0]\\). Finally, the maximal interval is just \\((T_\\text{min},T_\\text{max})\\). We denote the maximal interval of the solution with \\(J_\\phi\\).\n\n\n\n\n\n\n\nExample (blow-up of solutions)\n\n\n\nConsider the problem \\[\n\\begin{cases} y' = y^2, \\\\ y(0) = 1.\\end{cases}\n\\]\nWe solve it by separation of variables. So, \\[\n\\begin{aligned}\n& \\int_1^y \\frac{\\mathrm{d}\\tilde{y}}{\\tilde{y}^2} = \\int_0^t \\mathrm{d}\\tilde{t} \\\\\n\\Rightarrow\\; & \\left[ -\\frac{1}{\\tilde{y}} \\right]_1^y = t \\\\\n\\Rightarrow\\; & -\\frac{1}{y} + 1 = t \\\\\n\\Rightarrow\\; & y(t) = \\frac{1}{1-t}.\n\\end{aligned}\n\\]\nWe know that the problem is locally well-posed, because \\(f(t,y) = y^2\\) is continuous and locally Lipschitz. Thus, the above solution is locally unique. However, the interval of integration cannot be arbitrarily large: starting at \\(t=0\\), we can go forward in time only up to \\(t=1-\\varepsilon\\), with \\(\\varepsilon &gt; 0\\). That is, we cannot reach \\(t=1\\). The reason is clear: the solution exhibit a vertical asymptote at \\(t=1\\), thus the solution blows up. In mathematics, this event is indeed called blow-up in finite time of the solution.\nConversely, backward integration exhibit no problem. Therefore, the maximal interval of well-posedness is \\(J_\\phi = (-\\infty,1)\\).\n\n\nComputing the maximal interval for a solution by hand is clearly not practical. We would like to estimate the interval without having to solve the problem analytically.\n\nTheorem A.3 Let \\(\\|\\boldsymbol{\\phi}(t)\\|\\le M\\) for all \\(t\\in(a,b)\\). Then \\(J_\\phi=(a,b)\\).\n\nIn the theorem, we can also take \\(a=-\\infty\\) and \\(b=+\\infty\\). In this case the maximal interval would be \\(\\mathbb{R}\\).\nIf the right hand side of the IVP grows almost linearly, then we have global existence as well. Indeed, the following theorem holds.\n\nTheorem A.4 Let \\(\\boldsymbol{f}\\colon (a,b)\\times\\mathbb{R}^n\\to\\mathbb{R}^n\\). Suppose we are under the assumptions of the Cauchy-Lipschitz theorem. If there exist 2 non-negative constants \\(k_1\\), \\(k_2\\) such that\n\\[\n\\| \\boldsymbol{f}(t,\\boldsymbol{y}) \\| \\le k_1 + k_2 \\| \\boldsymbol{y} \\|\n\\]\nfor every \\((t,\\boldsymbol{y})\\in [a,b] \\times \\mathbb{R}^n\\), then for all \\((t_0,\\boldsymbol{y}_0)\\) the unique solution \\(\\boldsymbol{\\phi}(t)\\) is defined in \\([a,b]\\).\n\nIn general, we use a priori information on the solution to apply the above Theorems. For that, we may use energy estimates (common for problems in physics), or we can study the qualitative behavior of the solutions for various initial conditions, e.g., in the phase space. Let us try that with a simple example.\n\n\n\n\n\n\nExample (logistic equation)\n\n\n\nConsider the ODE \\[\ny' = y(1-y),\n\\]\nwhere \\(y(t)\\) may represent, for instance, the density of a population. We would like to show that \\(J = \\mathbb{R}\\) for every choice of the initial data \\(y(0)=y_0\\). Note that the right hand side \\(f(t,y)=y(1-y)\\) is quadratic: we cannot exclude a blow up as in the above example!\nWe have the following cases:\n\nif \\(y_0 = 0\\) or \\(y_0 = 1\\), the solution is constant in time, \\(y(t)=y_0\\). In fact, \\(f(t,y_0) = 0\\), thus \\(y'(t)=0\\) and we have a constant solution. We are going to call this type of solutions fixed points or equilibria (see the next section.)\nif \\(y_0 \\in (0,1)\\), then \\(y(t) \\to 1^-\\) as \\(t\\to+\\infty\\) and \\(y(t) \\to 0^+\\) as \\(t\\to-\\infty\\). Thus, \\(y(t)\\in(0,1)\\) for all \\(t\\). The limit follows from an analysis of the sign of \\(f(t,y)\\). For \\(y\\in(0,1)\\), \\(f(t,y) &gt; 0\\), thus \\(y' &gt; 0\\) for all \\(t\\) and \\(y(t)\\) is monotonically increasing. But \\(y = 1\\) is an equilibrium that cannot be crossed, and there are no other equilibria in \\((0,1)\\). Thus, it must be that \\(y\\to 1\\) from below as \\(t\\to\\infty\\).\nif \\(y_0 &gt; 1\\), then \\(y(t)\\to 1^+\\) as \\(t\\to+\\infty\\). Again, the statement follows from \\(f(t,y)&lt;0\\) for \\(y &gt; 1\\), thus \\(y'&lt;0\\) and \\(y(t)\\) is monotonically decreasing. Since it is bounded from below by \\(y(t)=1\\), we have \\(y(t)\\to 1\\). However, for \\(t\\to-\\infty\\) we have a blow up in finite time, specifically for \\(t = \\bar{t} &lt; 0\\) where (show this by explicit integration of the IVP!) \\[\n\\bar{t} = \\ln(1-y_0) - \\ln(y_0).\n\\]\n\nWe conclude that the solution \\(y(t)\\) is bounded in \\([y_0,0]\\) for any choice of the initial condition \\(y_0 \\ge 0\\) and \\(t\\ge 0\\). (Why we exclude \\(y_0 &lt;0?\\)) Thus, we can apply the above theorem with \\(a=0\\) and \\(b=\\infty\\), concluding that \\(J = (0,\\infty)\\). For \\(t &lt; 0\\), the solution is bounded for \\(y_0 \\in [0,1]\\), thus \\(J = \\mathbb{R}\\). However, for \\(y_0 &gt; 1\\), we have \\(J = (\\bar{t},\\infty)\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Introduction to ODEs</span>"
    ]
  },
  {
    "objectID": "IntroODE.html#linear-odes",
    "href": "IntroODE.html#linear-odes",
    "title": "Appendix A ‚Äî Introduction to ODEs",
    "section": "A.4 Linear ODEs",
    "text": "A.4 Linear ODEs\nA general linear ODE is an equation of the form: \\[\n\\mathbf{y}' = \\mathbf{A}(t)\\mathbf{y} + \\mathbf{b}(t)\n\\]\nwith \\[\n\\mathbf{y} = \\begin{pmatrix}\ny_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix},\\quad\n\\mathbf{A}(t) = \\begin{pmatrix}\n0 & 1 & 0 & \\cdots & 0 \\\\\n0 & 0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 0 & 1 \\\\\na_n(t) & a_{n-1}(t) & \\cdots & a_2(t) & a_1(t)\n\\end{pmatrix},\\quad\n\\mathbf{b}(t) = \\begin{pmatrix}\n0 \\\\ 0 \\\\ \\vdots \\\\ b(t)\n\\end{pmatrix}.\n\\]\nNote that \\(\\mathbf{f}(t,\\mathbf{y}) = \\mathbf{A}(t)\\mathbf{y} + \\mathbf{b}(t)\\) is only linear in \\(\\mathbf{y}\\), that is \\[\n\\mathbf{f}(t,\\alpha\\mathbf{y}+\\mathbf{z}) = \\alpha \\mathbf{f}(t,\\mathbf{y}) + \\mathbf{f}(t,\\mathbf{z}),\n\\]\nfor all choices of \\(\\alpha\\in\\mathbb{R}\\) and \\(\\mathbf{y}, \\mathbf{z}\\in\\mathbb{R}^n\\).\n\nA.4.1 Well-posedness\n\nTheorem A.5 Assuming that \\(\\mathbf{A} \\in \\mathcal{C}^0([\\alpha,\\beta];\\mathbb{R}^{n\\times n})\\) and \\(\\mathbf{b}\\in \\mathcal{C}^0([\\alpha,\\beta];\\mathbb{R}^n)\\), that is all coefficients are continuous function of \\(t\\in[\\alpha,\\beta]\\), then we have a unique global solution \\(\\boldsymbol{\\phi}\\in\\mathcal{C}^1((\\alpha,\\beta);\\mathbb{R}^n)\\) for each choice of the initial data.\n\nThe proof of the local well-posedness follows for the Cauchy-Lipschitz theorem, because \\(\\mathbf{f}(t,\\mathbf{y})\\) is locally Lipschitz uniformly in \\(t\\). In fact, since \\(\\mathbf{A}(t)\\) is continous on a closed interval, it is also bounded. Thus: \\[\n\\| \\mathbf{f}(t,\\mathbf{y}) - \\mathbf{f}(t,\\mathbf{z}) \\| =\n\\| \\mathbf{A}(t) (\\mathbf{y}-\\mathbf{z}) \\|\n\\le \\| \\mathbf{A}(t) \\| \\| \\mathbf{y}-\\mathbf{z} \\|\n\\le \\underbrace{\\max_{t\\in[\\alpha,\\beta]}\\| \\mathbf{A}(t) \\|}_{L} \\| \\mathbf{y}-\\mathbf{z} \\|.\n\\]\nThe global existence follows from Theorem¬†A.4. \\[\n\\| \\mathbf{f}(t,\\mathbf{y}) \\| = \\| \\mathbf{A}(t)\\mathbf{y} + \\mathbf{b}(t) \\|\n\\le \\| \\mathbf{A}(t)\\| \\|\\mathbf{y}\\| + \\| \\mathbf{b}(t) \\|\n\\le \\underbrace{\\max_{t\\in[\\alpha,\\beta]}\\| \\mathbf{A}(t) \\|}_{k_1} \\|\\mathbf{y}\\| + \\underbrace{\\max_{t\\in[\\alpha,\\beta]}\\| \\mathbf{b}(t) \\|}_{k_2}.\n\\]\n\n\nA.4.2 Solution of the homogeneous problem\nAn interesting aspect of linear ODEs is that we can represent explicitly all the solutions of the ODE. Obviously, a linear IVP has a unique solution. The ODE, on the other hand, can have many solutions. We start with the homogeneous problem: \\[\n\\mathbf{y}' = \\mathbf{A}(t)\\mathbf{y}.\n\\]\n\nTheorem A.6 Let \\(\\mathcal{U} = \\bigl\\{ \\boldsymbol{\\phi} \\in \\mathcal{C}^1((\\alpha,\\beta);\\mathbb{R}^n) : \\boldsymbol{\\phi}'(t) = \\mathbf{A}(t)\\boldsymbol{\\phi}(t) \\: \\forall t\\in(\\alpha,\\beta) \\bigr\\}\\) be the set of solutions of the linear ODE. Then \\(\\dim\\mathcal{U} = n\\).\n\nTherefore, there exists a basis \\(\\{\\boldsymbol{\\phi}_1(t),\\ldots,\\boldsymbol{\\phi}_n(t)\\}\\) of \\(\\mathcal{U}\\) such that each solution \\(\\boldsymbol{\\phi}(t) \\in \\mathcal{U}\\) reads as follows: \\[\n\\boldsymbol{\\phi}(t) = \\sum_{i=1}^n c_i \\boldsymbol{\\phi}_i(t),\n\\]\nfor some choice of \\(\\mathbf{c} = [c_1, \\ldots, c_n]^T \\in \\mathbb{R}^n\\).\n\nDefinition A.7 (Wronskian matrix) We define the Wronskian matrix \\(\\mathbf{W}(t)\\) as the column-matrix of the basis of \\(\\mathcal{U}\\):\n\\[\n\\mathbf{W}(t)=\\Bigl[\\begin{matrix}\n& \\boldsymbol\\phi_1(t) &|& \\boldsymbol\\phi_2(t) &|& \\cdots &|& \\boldsymbol\\phi_n(t) &\n\\end{matrix}\\Bigr].\n\\]\n\nNote that from its definition it follows that: \\[\n\\mathbf{W}(t)' = \\mathbf{A}(t)\\mathbf{W}(t).\n\\]\nThe general solution of the linear ODE is: \\[\n\\boldsymbol{\\phi}(t) = \\mathbf{W}(t)\\mathbf{c},\n\\]\nfor some \\(\\mathbf{c}\\in\\mathbb{R}^n\\). For an IVP with initial condition \\(\\mathbf{y}(t_0)=\\mathbf{y}_0\\), we have that: \\[\n\\boldsymbol{\\phi}(t_0) = \\mathbf{W}(t_0)\\mathbf{c} = \\mathbf{y}_0,\n\\quad\\Rightarrow\\quad\n\\mathbf{c} = \\mathbf{W}^{-1}(t_0)\\mathbf{y}_0,\n\\]\nthus the unique solution is: \\[\n\\boldsymbol{\\phi}(t) = \\mathbf{W}(t)\\mathbf{c} = \\mathbf{W}(t)\\mathbf{W}^{-1}(t_0)\\mathbf{y}_0\n= \\mathbf{W}(t,t_0)\\mathbf{y}_0,\n\\]\nwhere \\(\\mathbf{W}(t,t_0)\\) is called transition matrix. The transition matrix bears this name because it transfers the initial condition \\(\\mathbf y_0\\) at time \\(t_0\\) to the solution at time \\(t\\). It is a particular case of flow of an ODE. We have the following very useful properties, all straightforward to prove using the definition above:\n\n\\(\\mathbf W(t_0,t_0) = \\mathbf I\\),\n\\(\\mathbf W(t,s)\\mathbf W(s,t_0) = \\mathbf W(t,t_0)\\),\n\\((\\mathbf W(t,t_0))^{-1}=\\mathbf W(t_0,t)\\).\n\nNote that we haven‚Äôt proved the invertibility of \\(\\mathbf{W}(t_0)\\). We are going to do it later for the case of \\(\\mathbf{A}\\) with constant coefficients.\n\n\n\n\n\n\nExercise\n\n\n\nProve the properties of the transfer matrix.\n\n\n\n\nA.4.3 Solution of the general problem\nThe general solution is useful for building a particular solution for the non-homogeneous problem \\[\n\\mathbf{y}' = \\mathbf{A}(t)\\mathbf{y} + \\mathbf{b}(t).\n\\tag{A.3}\\]\nStarting from the solution \\(\\boldsymbol{\\phi}(t) = \\mathbf{W}(t)\\mathbf{c}\\), we can apply the method of variation of constants to find a solution of the form \\(\\boldsymbol{\\phi}(t) = \\mathbf{W}(t)\\mathbf{c}(t)\\), for some choice of \\(\\mathbf{c}(t)\\). It turns out that the particular solution is: \\[\n\\boldsymbol\\phi(t) = \\mathbf{W}(t) \\int^t \\mathbf{W}^{-1}(s)\\mathbf b(s) \\mathrm{d}s\n= \\int^t \\mathbf{W}(t,s)\\mathbf b(s) \\mathrm{d}s.\n\\]\nSupplemented with an initial condition \\(\\mathbf{y}(t_0)=\\mathbf{y}_0\\), the solution of the IVP is \\[\n\\boldsymbol\\phi(t) = \\mathbf{W}(t,t_0)\\mathbf{y}_0 + \\int_{t_0}^t \\mathbf{W}(t,s)\\mathbf{b}(s) \\mathrm{d}s.\n\\tag{A.4}\\]\n\n\n\n\n\n\nExercise\n\n\n\nShow that (Equation¬†A.4) is the solution of the IVP (Equation¬†A.3).\n\n\n\n\nA.4.4 Matrix exponential\nWe focus now on the following case: \\[\\begin{cases}\n\\mathbf y'=\\mathbf A\\mathbf y + \\mathbf b(t), & \\\\\n\\mathbf y(t_0)=\\mathbf y_0. &\n\\end{cases}\\]\nWe need the Wronskian matrix. For computing it, we use the fact that \\(\\mathbf{W}' = \\mathbf{A}\\mathbf{W}\\) and \\(\\mathbf{W}(0)=\\mathbf{I}\\) (identity matrix). Formally, notice that \\[\n\\begin{aligned}\n\\mathbf{W}(0) &= \\mathbf{I}, \\\\\n\\mathbf{W}'(0) &= \\mathbf{A}\\mathbf{W} = \\mathbf{A}, \\\\\n\\mathbf{W}''(0) &= \\mathbf{A}\\mathbf{W}' = \\mathbf{A}^2, \\\\\n&\\cdots \\\\\n\\mathbf{W}^{(k)}(0) &= \\mathbf{A}^n.\n\\end{aligned}\n\\]\nThus we can construct the solution as a Taylor expansion: \\[\n\\mathbf{W}(t) = \\sum_{n=0}^\\infty \\frac{\\mathbf{W}^{(n)}(0)}{n!} t^k\n= \\sum_{n=0}^\\infty \\frac{\\mathbf{A}^n}{n!} t^k\n= \\mathbf{I} + t\\mathbf{A} + \\frac{t^2}{2} \\mathbf{A}^2 + \\ldots\n\\]\n\nDefinition A.8 (Matrix exponential) We define the matrix exponential as: \\[\ne^{\\mathbf{A}} := \\sum_{n=0}^\\infty \\frac{\\mathbf{A}^n(t)}{n!}.\n\\]\n\nThus, the Wronskian is: \\[\n\\mathbf{W}(t) = e^{t\\mathbf{A}}.\n\\]\nThe series converges for each \\(\\mathbf{A}\\in\\mathbb{R}^{n\\times n}\\). Below some useful properties:\n\nIf \\(\\mathbf A = \\operatorname{diag}\\{ a_1, \\dots, a_n \\}\\), then \\(e^{\\mathbf A} = \\operatorname{diag}\\{ e^{a_1}, \\dots, e^{a_n} \\}\\). The matrix exponential of diagonal matrices is then straightforward to compute. The proof is simple: just plug the diagonal matrix in the definition and note that \\(\\mathbf A^k = \\operatorname{diag}\\{ a_1^k, \\dots, a_n^k \\}\\). Hence: \\[\\sum_{k=0}^\\infty \\frac{\\mathbf{A}^k}{k!} = \\operatorname{diag}\\biggl\\{ \\sum_{k=0}^\\infty \\frac{a_1^k}{k!}, \\ldots, \\sum_{k=0}^\\infty \\frac{a_n^k}{k!} \\biggr\\} = \\operatorname{diag}\\{ e^{a_1}, \\dots, e^{a_n} \\}.\\]\n\\(e^{\\mathbf 0} = \\mathbf{I}\\). If \\(\\mathbf A\\) is the matrix with all zero entries \\(\\mathbf{0}\\), then the matrix exponential is the identity matrix \\(\\mathbf{I}\\). This is trivial because all the entries of the sum are zero except for \\(k=0\\).\n\\(\\det(e^{\\mathbf A})=e^{\\operatorname{tr}\\mathbf A}\\). This is reminescent of the Wronskian (determinant of Wronskian matrix), and the proof is similar, based on the Taylor expansion \\(|\\mathbf I + \\varepsilon\\mathbf A| = 1+\\varepsilon\\operatorname{tr}\\mathbf A + \\mathcal{O}(\\varepsilon^2)\\). We use the alternative definition of matrix exponential, and the fact that the determinant is a continuous function with respect to the coefficients of the matrix: \\[\\begin{aligned}\n|e^{\\mathbf A}| &= \\biggl| \\lim_{n\\to\\infty} \\Bigl(\\mathbf I + \\tfrac{1}{n}\\mathbf A \\Bigr)^n \\biggr| \\\\\n\\text{\\small(continuity)} &= \\lim_{n\\to\\infty} \\biggl| \\Bigl( \\mathbf I + \\tfrac{1}{n}\\mathbf A \\Bigr)^n \\biggr| \\\\\n\\text{\\small(det of product)} &= \\lim_{n\\to\\infty} \\Bigl| \\mathbf I + \\tfrac{1}{n}\\mathbf A \\Bigr|^n \\\\\n\\text{\\small(det expansion)} &= \\lim_{n\\to\\infty} \\Bigl( 1 + \\tfrac{1}{n}\\operatorname{tr}\\mathbf A + \\mathcal{O}(\\tfrac{1}{n^2}) \\Bigr)^n = e^{\\operatorname{tr}\\mathbf A}.\n\\end{aligned}\\]\n\\(e^{\\mathbf A}\\) is invertible. This is a consequence of the previous property, since the determinant is always strictly positive.\nIf \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) commute, that is \\(\\mathbf{A}\\mathbf{B} = \\mathbf{B}\\mathbf{A}\\), then \\(e^{\\mathbf{A}+\\mathbf{B}}=e^{\\mathbf{A}} e^{\\mathbf{B}}\\). We are not proving this fact.\n\\(e^{0\\mathbf A}=\\mathbf{I}\\). This follows immediately from property 2. of the matrix exponential.\n\\(e^{t \\mathbf A} e^{s \\mathbf A} = e^{(t+s)\\mathbf A}\\). In this case we could use property 5., but instead we use the formula for the product of two series and Newton‚Äôs binomial formula: \\[\\begin{aligned}\ne^{t \\mathbf A} e^{s \\mathbf A}\n&= \\biggl(\\sum_{k=0}^\\infty \\frac{t^k \\mathbf A^k}{k!}\\biggr)\n\\cdot\\biggl(\\sum_{j=0}^\\infty \\frac{s^j \\mathbf A^j}{j!}\\biggr) \\\\\n&= \\sum_{k=0}^\\infty \\sum_{l=0}^k \\frac{t^l \\mathbf A^l}{l!} \\frac{s^{k-l} \\mathbf A^{k-l}}{(k-l)!} \\\\\n&= \\sum_{k=0}^\\infty \\frac{\\mathbf A^k}{k!} \\sum_{l=0}^k \\frac{k!}{l!(k-l)!} t^l s^{k-l} \\\\\n&= \\sum_{k=0}^\\infty \\frac{(t+s)^k\\mathbf A^k}{k!}\n= e^{(t+s)\\mathbf A}.\n\\end{aligned}\\]\n\\(\\frac{\\mathrm d}{\\mathrm d t} e^{t\\mathbf A} = \\mathbf A e^{t\\mathbf A}\\). It follows from the definition: \\[\\tfrac{\\mathrm d}{\\mathrm d t} e^{t\\mathbf A}\n%= \\frac{\\mathrm d}{\\mathrm d t} \\sum_{k=0}^\\infty \\frac{t^k \\mathbf A^k}{k!}\n= \\sum_{k=0}^\\infty \\frac{\\mathrm d}{\\mathrm d t} \\frac{t^k \\mathbf A^k}{k!}\n= \\sum_{k=1}^\\infty \\frac{kt^{k-1} \\mathbf A^k}{k!}\n= \\mathbf A \\sum_{k=1}^\\infty \\frac{t^{k-1} \\mathbf A^{k-1}}{(k-1)!}\n= \\mathbf A e^{t\\mathbf A}.\\]\n\nThis last property in particular shows rigorously that the matrix exponential is the Wronskian matrix of the ODE \\(\\mathbf y' = \\mathbf A\\mathbf y\\), while with the other properties we define the transition matrix.\n\n\nA.4.5 Solution of the case with constant coefficients\nThe solution of \\[\\begin{cases}\n\\mathbf y'=\\mathbf A\\mathbf y + \\mathbf b(t), & \\\\\n\\mathbf y(t_0)=\\mathbf y_0, &\n\\end{cases}\\]\nfollows now trivially from the matrix exponential. We have: \\[\n\\boldsymbol\\phi(t) = e^{(t-t_0)\\mathbf{A}}\\mathbf{y}_0 + \\int_{t_0}^t e^{(t-s)\\mathbf{A}}\\mathbf{b}(s) \\mathrm{d}s.\n\\]\n\n\n\n\n\n\nExample (scalar case)\n\n\n\nWhen \\(n = 1\\), with \\(\\mathbf{A}=a\\in\\mathbb{R}\\), the matrix exponential is the usual exponential function. The solution is \\[\n\\phi(t) = e^{a(t-t_0)}y_0 + \\int_{t_0}^t e^{a(t-s)}b(s) \\mathrm{d}s.\n\\]\nFor instance, with \\(b = e^{\\omega t}\\), we have: \\[\n\\int_{t_0}^t e^{a(t-s)}b(s) \\mathrm{d}s =\n\\int_{t_0}^t e^{a(t-s)}e^{\\omega s} \\mathrm{d}s\n= \\frac{e^{a t} - e^{\\omega t}}{a-\\omega}.\n\\]\nSo the solution to the problem: \\[\n\\begin{cases}\ny' = ay + e^{\\omega t}, & \\\\\ny(t_0) = y_0, &\n\\end{cases}\n\\]\nis as follows: \\[\n\\phi(t) = e^{a(t-s)} + \\frac{e^{a t} - e^{\\omega t}}{a-\\omega}.\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Introduction to ODEs</span>"
    ]
  },
  {
    "objectID": "IntroODE.html#sec-dyn",
    "href": "IntroODE.html#sec-dyn",
    "title": "Appendix A ‚Äî Introduction to ODEs",
    "section": "A.5 Dynamical systems",
    "text": "A.5 Dynamical systems\nIntuitively, a dynamical system is a state or a set of variables evolving over time according to some rule. The concept is very general. Markov chains, cellular automata and large language models are examples of dynamical systems. Roughly speaking, a dynamical system is composed by\n\nA state space, the set of all admissible states of the system, and\nA rule for moving from the current state to the next one.\n\nWe can associate to a system of ODEs a dynamical system. The state space (called phase space) is the subset of \\(\\mathbb{R}^n\\) of all possible states. The update rule is the solution of the ODE (called flow). For ODEs, one is not interested in studying a specific instance (or trajectory) of the system, but rather the system as a whole, for all possible initial states.\n\nDefinition A.9 A dynamical system is well-posed autonomous ODE (autonomous means that \\(\\mathbf{f}\\) is not an explicit function of \\(t\\)). Specifically, given \\(\\mathbf{f}\\in\\mathcal{C}^1(\\Omega;\\mathbb{R}^n)\\), with \\(\\Omega\\neq\\emptyset\\) an open set of \\(\\mathbb{R}^n\\), we suppose that for every initial condition \\(\\mathbf{y}_0\\in\\mathbb{R}^n\\), the IVP \\[\\begin{cases}\n\\mathbf{y}' = \\mathbf{f}(\\mathbf{y}), \\\\\n\\mathbf{y}(0) = \\mathbf{y}_0,\n\\end{cases}\\]\nhas a unique solution \\(\\boldsymbol{\\phi}(t)\\) defined for \\(t\\in[0,\\infty)\\), and such that \\(\\boldsymbol{\\phi}(t)\\in\\Omega\\) for all \\(t\\ge0\\). We call \\(\\mathbf{y}' = \\mathbf{f}(\\mathbf{y})\\) dynamical system on the phase space \\(\\Omega\\). Succintly, the dynamical system is fully qualified by the pair \\((\\mathbf{f},\\Omega)\\).\n\n\nDefinition A.10 The flow of an ODE is a function \\(\\boldsymbol{\\Phi}\\colon\\mathbb{R}\\times\\Omega\\to \\Omega\\) such that \\[\n\\boldsymbol{\\Phi}(t,\\mathbf{y}_0) = \\boldsymbol{\\phi}(t),\n\\]\nwhere \\(\\boldsymbol{\\phi}(t)\\) is the solution of the ODE with initial condition \\(\\mathbf{y}_0\\).\n\nFor example, for a linear dynamical system \\(\\mathbf{y}' = \\mathbf{A}\\mathbf{y}\\) the flow is given by the transition matrix: \\[\n\\boldsymbol{\\Phi}(t,\\mathbf{y}_0) = \\mathbf{W}(t,0)\\mathbf{y}_0 = e^{t\\mathbf{A}}\\mathbf{y}_0.\n\\]\nNote. It is worth to mention that also the general, non-autonomous ODEs \\(\\mathbf{y}' = \\mathbf{f}(t,\\mathbf{y})\\) in \\(\\mathbb{R}^n\\) can be recast to the autonomous form \\(\\mathbf{z}' = \\mathbf{g}(\\mathbf{z})\\) in \\(\\mathbb{R}^{n+1}\\) by setting \\[\\mathbf{z}(t) = \\begin{bmatrix} \\tau(t) \\\\ \\mathbf{y}(t) \\end{bmatrix},\\quad \\mathbf{g}(\\mathbf{z}) = \\begin{bmatrix} 1 \\\\ \\mathbf{f}(\\tau,\\mathbf{y}) \\end{bmatrix},\\]\nin fact, \\(\\tau(t)=t\\).\n\nA.5.1 Orbits and trajectories\n\nDefinition A.11 For a given solution (or trajectory) \\(\\boldsymbol{\\phi}(t)\\), we consider the associated orbit \\(\\mathcal{O}(\\boldsymbol{\\phi})\\) defined as follows \\[\\mathcal{O}(\\boldsymbol{\\phi}) = \\bigcup_{t\\ge 0} \\boldsymbol{\\phi}(t),\\]\nthat is, the orbit is the ‚Äúshadow‚Äù of a trajectory onto the phase space \\(\\Omega\\).\n\nIn practice, the study of a dynamical system is not limited to a particular solution (defined by some initial condition), but rather it covers the entire domain. In some sense, there is a strong geometrical interpretation: the r.h.s. of the ODE determines a vectorial field in the phase space \\(\\Omega\\), so that a given orbit is tangent at every point to such field.\n\nProposition A.1 If \\(\\boldsymbol{\\phi}(t)\\) is a trajectory of the dynamical system for all \\(t\\ge 0\\), also \\(\\boldsymbol{\\psi}(t) = \\boldsymbol{\\phi}(t+c)\\) is a solution for all \\(t\\ge -c\\), with \\(c\\in\\mathbb{R}\\).\n\nThe proposition is a simple consequence of the fact that a dynamical system is an autonomous ODE, that is it \\(\\mathbf{f}\\) is not a function of time.\n\nProposition A.2 Two distinct orbits of a dynamical system either coincide or they never intersect.\n\nThis proposition is extremely important when analyzing the phase portrait of a system, because some orbits may act as ‚Äúbarriers‚Äù for other orbits. Please note that the statement refers to orbits, not simply to the trajectories.\nTo prove the proposition, suppose there are two distinct orbits associated to the trajectories \\(\\boldsymbol{\\phi}\\) and \\(\\boldsymbol{\\psi}\\), and such that \\(\\boldsymbol{\\phi}(t_1) = \\boldsymbol{\\psi}(t_2)\\) for some \\(t_1,t_2\\ge 0\\). That is, the orbits intersect. Now consider the function \\(\\tilde{\\boldsymbol{\\psi}}(t) = \\boldsymbol{\\psi}(t+t_2-t_1)\\). This function is still a solution, and \\(\\tilde{\\boldsymbol{\\psi}}(t_1) = \\boldsymbol{\\psi}(t_2) = \\boldsymbol{\\phi}(t_1)\\). Thus, \\(\\tilde{\\boldsymbol{\\psi}}\\) and \\(\\boldsymbol{\\phi}\\) have the same initial condition, and they must coincide for all \\(t\\), that is \\(\\tilde{\\boldsymbol{\\psi}}(t) = \\boldsymbol{\\phi}(t)\\). In other words, \\(\\tilde{\\boldsymbol{\\psi}}\\) and \\(\\boldsymbol{\\phi}\\) have the same orbit. But also \\(\\tilde{\\boldsymbol{\\psi}}\\) and \\(\\boldsymbol{\\psi}\\) have the same orbit, because the system is autonomous. In conclusion, if two orbits intersect at a least one point, they must concide at all points.\n\n\n\n\n\n\nExample (Van der Pol)\n\n\n\nBelow an example of the phase space for the equation \\(y'' - \\mu(1-y^2)y' + y = 0\\), which models a stiff oscillating system.\n\n\n\nTrajectory\n\n\nIn the figure, in blue we have the orbit associated to the trajectory in orange.\n\n\n\nPhase space\n\n\nThis second figure represents the phase portrait of the system, with the orbit and the limit cycle.\n\n\n\n\n\n\n\n\nExample (positive systems)\n\n\n\nAn example are dynamical systems of the form: \\[\\begin{cases}\ny_1' = y_1 \\cdot f(y_1,y_2), \\\\\ny_2' = y_2 \\cdot g(y_1,y_2).\n\\end{cases}\\]\nIn fact, the line \\((y_1,y_2) = (0,y_2)\\) (the ordinate) is an orbit of the system, because if we start from \\((0,y_2)\\) we never leave the line, since \\(y_1'=0\\). Similarly, the line \\((y_1,y_2) = (y_1,0)\\) (the abscissa) is also an orbit. Therefore, any other orbit starting in the positive quadrant will stay in the positive quadrant for all times. That is, the two orbits are barriers that ensures the positivity of the system for all times.\n\n\n\n\nA.5.2 Types of orbits\nSome orbits of a dynamical system are special. Here, we only mention the two most important ones: equilibria and periodic orbits.\n\nDefinition A.12 We say that \\(\\mathbf{y}_0\\in\\Omega\\) is an equilibrium of the system if \\(\\mathbf{f}(\\mathbf{y}_0)=0\\). In particular, the trajectory \\(\\boldsymbol{\\phi}(t)=\\mathbf{y}_0\\) for all \\(t\\in\\mathbb{R}\\) is called solution of the equilibrium. The orbit is simply the point \\(\\{ y_0 \\}\\).\n\nWhat if an orbit self-intersect at some finite time? We have a periodic orbit.\n\nProposition A.3 If there exist \\(\\tau_1,\\tau_2\\ge 0\\) such that \\(\\boldsymbol{\\phi}(\\tau_1) = \\boldsymbol{\\phi}(\\tau_2)\\), then \\(\\boldsymbol{\\phi}(t)\\) is a periodic solution.\n\nIn fact, as above, consider the trajectory \\(\\boldsymbol{\\psi}(t) = \\boldsymbol{\\phi}(t-\\tau_1+\\tau_2)\\). Then \\(\\boldsymbol{\\psi}(\\tau_1)=\\boldsymbol{\\phi}(\\tau_2)=\\boldsymbol{\\phi}(\\tau_1)\\). So we conclude that \\(\\boldsymbol{\\psi}(t)\\) and \\(\\boldsymbol{\\phi}(t)\\) coincide for all time. In particular, \\(\\boldsymbol{\\phi}(t)=\\boldsymbol{\\phi}(t+\\tau_2-\\tau_1)\\), that is, \\(\\boldsymbol{\\phi}(t)\\) is periodic with period \\(\\tau_2-\\tau_1\\).\nThe kind of orbits that we may find in a dynamical system is actually limited to\n\nOrbits consisting of a single point (equilibria);\nOrbits corresponding to closed regular curves (periodic solutions);\nOrbits corresponding to open regular curves, with no self-intersections.\n\nThe dimension of the phase space plays a key role in the type of attractor. For \\(\\Omega \\subset \\mathbb{R}\\), we can only have equilibria. In fact:\n\nProposition A.4 For \\(n=1\\) a dynamical system cannot have periodic solutions.\n\nFor the proof, suppose that \\(\\phi(t)\\) is periodic with period \\(T&gt;0\\). Then, we multiply by \\(\\phi'\\) both sides of the ODE and integrate in \\([t,t+T]\\): \\[\n\\int_t^{t+T} \\bigl( \\phi'(s)\\bigr)^2 \\:\\mathrm{d}s = \\int_t^{t+T} f(\\phi(s))\\phi'(s)\\:\\mathrm{d}s\n= \\int_{\\phi(t)}^{\\phi(t+T)} f(u)\\:\\mathrm{d}u = 0,\n\\]\nwhere we applied the change of variables \\(u = \\phi(s)\\). The last integral is zero because \\(\\phi(t)=\\phi(t+T)\\). But the first integral is strictly positive, so we have a contradiction.\nNote. For \\(n=1\\), a more general non-autonomuous ODE \\(y'=f(t,y)\\) can have periodic solutions. But this is not a dynamical system, unless we recast it as a system, thus \\(n=2\\) and periodic solutions are possible.\n\n\n\n\n\n\nExample (test problem)\n\n\n\nThe dynamical system \\(y'=\\lambda y\\) has only one equilibrium, \\(y^*=0\\).\n\n\n\n\n\n\n\n\nExample (logistic equation)\n\n\n\nThe dynamical system \\(y'=y(1-y)\\) has two equilibria, \\(y_1=0\\) and \\(y_2=1\\).\n\n\n\n\n\n\n\n\nExample (linear system)\n\n\n\nGiven \\(\\mathbf{A}\\in\\mathbb{R}^{n\\times n}\\), the dynamical system \\(\\mathbf{y}'=\\mathbf{A}\\mathbf{y}\\) has equilibria such that \\(\\mathbf{A}\\mathbf{y}^*=0\\), that is equilibria belong to the kernel of \\(\\mathbf{A}\\), \\(\\mathbf{y}^*\\in\\ker\\mathbf{A}\\). If \\(\\mathbf{A}\\) is invertible, then we only have \\(\\mathbf{y}^*=0\\).\n\n\n\n\n\n\n\n\nExample (FitzHugh-Nagumo model)\n\n\n\nWe consider the dynamical system defined by the ODE \\[\\begin{cases}\nu' = f(u,r) = u(1-u)(u-\\alpha) - r, \\\\\nr' = g(u,r) = \\beta(u-\\gamma r),\n\\end{cases}\\]\nwhere \\(\\alpha,\\beta,\\gamma\\in\\mathbb{R}\\) are parameters of the model. This system is an important phenomenological model of excitability of cells and for modeling the action potential.\nWe define as nullclines associated to \\(u'\\) (resp. \\(r'\\)) the implicit curve defined by the equation \\(f(u,r)=0\\) (resp. \\(g(u,r)=0\\)). Equilibria are found as intersections between nullclines, because at those points all right hand sides simultaneously cancel. For the FitzHugh-Nagumo system, nullclines are: \\[\\begin{cases}\nf(u,r) = 0, \\Leftrightarrow r = u(1-u)(u-\\alpha), \\\\\ng(u,r) = 0, \\Leftrightarrow r = \\gamma^{-1} u.\n\\end{cases}\\]\nThe first nullcline is a cubic function with zeros at \\(0\\), \\(\\alpha\\), and \\(1\\), and such that \\(r\\to-\\infty\\) for \\(u\\to\\infty\\). The second nullcline is a line through the origin and with slope \\(\\gamma^{-1}\\). One equilibrium is certainly the point \\((u_1,r_1)=(0,0)\\), for all choices of the parameters. The other equilibria depend on the choice of the parameters, specifically on whether the cubic function and the line intersect outside the origin. Those are such that \\[u(1-u)(u-\\alpha) = \\gamma^{-1} u,\\]\nso, besides \\(u=0\\), the number of zeros depends on the sign of the discriminant: \\[\\Delta = (\\alpha+1)^2 - 4 \\gamma^{-1}.\\]\nIn conclusion, we have three equilibria for \\(\\Delta &gt; 0\\), two equilibria for \\(\\Delta = 0\\) and only one for \\(\\Delta &lt; 0\\).\n\n\n\n\nA.5.3 Lyapunov stability and attractors\nConsider a dynamical system \\((\\mathbf{f},\\Omega)\\) with \\(\\Omega\\subseteq \\mathbb{R}^n\\), with an equilibrium \\(\\mathbf{y}_0 \\in\\Omega\\), that is \\(\\mathbf{f}(\\mathbf{y}_0)=0\\). How does the system behave in a neighborhood of the equilibrium? Say, if we start close to \\(\\mathbf{y}_0\\), do the stay close to it for long? Please note that the interest is purely qualitative: we are not interested in the specific form of the trajectory, but rather its behavior for \\(t\\to\\infty\\).\n\nDefinition A.13 (Lyapunov stability) We say that \\(\\mathbf{y}_0\\) is a locally stable equilibrium if for each \\(\\varepsilon&gt;0\\) there exists \\(\\delta = \\delta(\\varepsilon,\\mathbf{y}_0)\\) such that for all \\(\\mathbf{y}_1\\in\\Omega\\) with \\(\\| \\mathbf{y}_1 - \\mathbf{y}_0 \\| &lt; \\delta\\) we have that \\(\\| \\boldsymbol{\\Phi}(t,\\mathbf{y}_1) - \\mathbf{y}_0 \\| &lt; \\varepsilon\\) for all \\(t\\ge 0\\), where \\(\\boldsymbol{\\Phi}(t,\\mathbf{y})\\) is the flow of the system.\nMoreover, we say that \\(\\mathbf{y}_0\\) is asymptotically stable if in the above definition we have \\(\\| \\boldsymbol{\\phi}(t;\\mathbf{y}_1) - \\mathbf{y}_0 \\| \\to 0\\) for \\(t\\to\\infty\\).\n\nThe difference between simple stability and asymptotic stability is that in the former case orbits stay close to the equilibrium without necessarily approaching it for \\(t\\to\\infty\\). For instance, the vertical downward position of the frictionless pendulum is only stable, because the orbits of the systems (oscillations at given amplitude) stay close to it with distance equal to the amplitude but never approach it. On the other hand, in the presence of friction, the equilibrium becomes asymptotically stable. (Thermodynamically speaking, asymptotically stable equilibria are quite boring: it often means extinction, death.)\nFinally, an unstable equilibrium is defined by (logically) negating the above definition. That is, \\(\\mathbf{y}_0\\in\\Omega\\) is an unstable equilibrium if there exists \\(\\varepsilon&gt;0\\) such that for all \\(\\delta &gt; 0\\) there exists \\(\\mathbf{y}_1\\in\\Omega\\) with the property that \\(\\|\\mathbf{y}_1 - \\mathbf{y}_0\\| &lt; \\delta\\) but \\(\\|\\boldsymbol{\\phi}(t_n,\\mathbf{y}_1) - \\mathbf{y}_0 \\| \\ge \\varepsilon\\) for a sequence \\(\\{ t_n \\}_n\\to\\infty\\) and all \\(n\\in\\mathbb{N}\\). In other words, there exists at least one initial condition that brings the associated trajectory arbitrarily far from the equilibrium at some time points \\(t_n\\) approaching infinity. (It would be too much to ask the same for all \\(t\\ge M\\), because we could have a diverging trajectory that periodically comes very close to the equilibrium, without really approaching it. Take for instance the function \\(\\phi(t)=t\\sin^2 t + \\varepsilon/2\\), where for \\(t=t_n=n\\pi\\) is equal to \\(\\varepsilon/2\\), but for \\(t_n=n\\pi/2\\) is diverging: it is clearly unstable.)\nAn interesting concept associated with equilibria is that of basin of attraction.\n\nDefinition A.14 (Basin of attraction) If an equilibrium is only locally asymptotically stable, then we have the basin of attraction defined as \\[\\mathcal{B}(\\mathbf{y}_0) := \\Bigl\\{ \\tilde{\\mathbf{y}}\\in\\Omega\\colon \\lim_{t\\to+\\infty} \\boldsymbol{\\Phi}(t,\\mathbf{y}_0) = \\mathbf{y}_0 \\Bigr\\}.\\]\n\nAn equilibrium is globally asymptotically stable when \\(\\mathcal{B}(\\mathbf{y}_0)=\\Omega\\).\nRemark. Lyapunov stability is different from the concept of stability to perturbation, or zero stability (see Corollary¬†A.1). In the latter, the aim is check whether we are able to recover the original solution (equilibrium or not) as the perturbation in the dynamical system goes to zero. The original and perturbed solutions may diverge from each other for long time, but we can always reduce the gap between them by reducing the initial error (the perturbation): when that‚Äôs not possible, the system is not (zero) stable. Lyapunov stability, on the other hand, concerns the structural stability properties of the system, that is we study the long-term behavior of solutions without controlling the initial perturbation. Actually, all initial conditions in the basin of attraction of an equilibrium yield solutions that converge to the same equilibrium, irrespective of the gap between them.\n\n\n\n\n\n\nExample (stability of the test problem)\n\n\n\nThe stability of equilibrium \\(y=0\\) of \\(y'=\\lambda y\\) depends clearly on \\(\\lambda\\). Suppose that \\(\\lambda\\in\\mathbb{C}\\). Then, the full solution is of the form \\[\\phi(t) = e^{\\lambda t} y_0.\\]\nWe can expand the exponential to get more insights: \\[\\phi(t) = e^{\\lambda t} y_0 = e^{t \\operatorname{Re}\\lambda}e^{t \\operatorname{Im}\\lambda} y_0 =\ne^{t \\operatorname{Re}\\lambda} \\bigl(\\cos(t\\operatorname{Im}\\lambda) + i \\sin(t\\operatorname{Im}\\lambda)\\bigr) y_0.\\]\nSince the equilibrium is 0, we just need to check whether this trajectory, in modulus, stays close (or even approaches) zero over time. That is, \\[|\\phi(t)|^2 = \\phi^*(t)\\phi(t) = e^{2 t \\operatorname{Re}\\lambda} \\bigl(\\cos(t\\operatorname{Im}\\lambda)^2 + \\sin(t\\operatorname{Im}\\lambda)\\bigr)^2 |y_0|^2,\\]\nso we have: \\[|\\phi(t)| = e^{t \\operatorname{Re}\\lambda}|y_0|.\\]\nFrom this expression, we easily deduce that\n\nIf \\(\\operatorname{Re}\\lambda &lt; 0\\), then \\(|\\phi(t)|\\to 0\\) for all \\(y_0\\in\\mathbb{C}\\). The equilibrium is therefore globally asymptotically stable.\nIf \\(\\operatorname{Re}\\lambda &gt; 0\\), then \\(|\\phi(t)|\\to\\infty\\) for at least one non trivial \\(y_0\\in\\mathbb{C}\\). The equilibrium is therefore unstable.\nIf \\(\\operatorname{Re}\\lambda = 0\\), then \\(|\\phi(t)| = |y_0|\\). The equilibrium is stable.\n\n\n\n\n\n\n\n\n\nExample (stability of logistic equation)\n\n\n\nThe logistic equation \\(y'=y(1-y)\\) has two equilibria, \\(y_1=0\\) and \\(y_2=1\\). We could study their stability by taking advantage of the analytical solution, available in this case, but we will not. We will proceed more generally. The dynamics is determined by the sign of \\(f(y)=y(1-y)\\).\n\nIf \\(y_0 = y_1\\) or \\(y_0 = y_2\\), there is no dynamics over time.\nIf \\(y_0\\in(0,1)\\) (boundaries excluded), then the solution \\(\\phi(t;y_0)\\) will never leave the interval \\((0,1)\\), because \\(0\\) and \\(1\\) are barriers (equilibria are also orbits, and orbits cannot intersect). Moreover, in this region \\(f(y)&gt;0\\). Therefore \\(y'&gt;0\\), that the solution \\(\\phi(t;y_0)\\) increases over time. Since the equilibrium \\(y_2=1\\) cannot be crossed, \\(\\phi(t;y_0)\\) indefinitely approaches \\(y_2\\) (from the left) without crossing it.\nIf \\(y_0 &gt; 1\\), with the same reasoning as above we conclude that \\(\\phi(t;y_0)&gt;1\\) indefinitely. But here \\(f(y)&lt;0\\), so \\(y'&lt;0\\) and again \\(\\phi(t;y_0)\\) approaches \\(y_2\\) (from the right).\nIf \\(y_0 &lt; 0\\), \\(y' = f(y)&lt;0\\) and \\(\\phi(t;y_0)\\to-\\infty\\).\n\nWith the above analysis, we can easily conclude that\n\n\\(y_1=0\\) is unstable, because orbits diverge from it.\n\\(y_2=0\\) is asymptotically stable, but only locally, with \\(\\mathcal{B}(y_2) = \\{ y &gt; 0 \\}\\).\n\nWhen restricting the dynamical system to \\(\\Omega=\\mathbb{R}^+\\), as in the case of population dynamics, the equilibrium \\(y_2\\) is almost globally attractive, except when we start from \\(y_1=0\\).\n\n\nPlease note that the above argument is very general, as it can be straightforwardly applied to any 1-D dynamical system: it is enough to study the sign and the zeros of \\(f(y)\\).\n\n\nA.5.4 Stability of linear ODEs\n\nA.5.4.1 Computation of the matrix exponential\nThe computation of the matrix exponential is not a trivial task, in general. In some cases, however, it is practical. When \\(\\mathbf A\\) is diagonal we have seen that the matrix exponential is trivially the element-wise exponential.\nWhen \\(\\mathbf A\\) is diagonalizable, that is there exists a matrix \\(\\mathbf S\\) such that \\[\\mathbf S^{-1}\\mathbf A\\mathbf S = \\boldsymbol\\Lambda,\\]\nwith \\(\\boldsymbol\\Lambda\\) diagonal matrix, the exponential matrix follows immediately from the following observation: \\[(\\mathbf S\\boldsymbol\\Lambda\\mathbf S^{-1})^k = (\\mathbf S\\boldsymbol\\Lambda\\mathbf S^{-1})(\\mathbf S\\boldsymbol\\Lambda\\mathbf S^{-1})\\cdots(\\mathbf S\\boldsymbol\\Lambda\\mathbf S^{-1})=\\mathbf S\\boldsymbol\\Lambda^k \\mathbf S^{-1}.\\]\nIn fact, we have \\[e^{\\mathbf A} = \\sum_{k=0}^\\infty \\frac{\\mathbf A^k}{k!}\n= \\sum_{k=0}^\\infty \\frac{(\\mathbf S\\boldsymbol\\Lambda\\mathbf S^{-1})^k}{k!}\n= \\mathbf S \\sum_{k=0}^\\infty \\frac{\\boldsymbol\\Lambda^k}{k!} \\mathbf S^{-1}\n= \\mathbf S e^{\\boldsymbol\\Lambda} \\mathbf S^{-1}.\\]\nThe matrix \\(e^{\\boldsymbol{\\Lambda}}\\) is easy to compute, using the above formula (at least when it is easy to compute eigenvalues and eigenvectors.)\nIn the general case, when \\(\\mathbf A\\) is not diagonalizable, the computation is not straightforward. It is based on the so-called Jordan canonical form. In practice, it is always possible to find a matrix \\(\\mathbf S\\) such that \\(\\mathbf S^{-1}\\mathbf A\\mathbf S = \\mathbf J\\) is in the Jordan canonical form, that is \\(\\mathbf J\\) is a block diagonal matrix, each block with a specific structure. The matrix exponential of the Jordan canonical form is again a block diagonal matrix. The matrix exponential of each block can be computed explicitly.\nThe Jordan canonical form is as follows: \\[\\mathbf J = \\begin{pmatrix} \\mathbf{J}_1 & & \\\\ & \\ddots & \\\\ & & \\mathbf{J}_r \\end{pmatrix}\\]\nwhere \\(J_i\\) is a matrix of the form: \\[\\mathbf{J}_i = \\begin{pmatrix} \\lambda_i & 1 & & \\\\ & \\lambda_i & \\ddots & \\\\ & & \\ddots & 1 \\\\ & & & \\lambda_i \\end{pmatrix}\\]\nThe eigenvalues \\(\\lambda_i\\) of the matrix \\(\\mathbf A\\) appears on the diagonal of the Jordan block \\(\\mathbf{J}_i\\). If the matrix is diagonalizable, then there are exactly \\(n\\) Jordan blocks each of dimension 1. In fact, the Jordan canonical form is diagonal. If some eigenvalues have geometric multiplicity strictly less than algebraic multiplicity, then the matrix is not diagonalizable. The Jordan blocks compensate for the difference in multiplicity. For instance, consider the matrix \\[\\mathbf A = \\begin{pmatrix} 0 & 1 \\\\ -1 & 2 \\end{pmatrix}.\\]\nThe eigenvalues are \\(\\lambda_1 = \\lambda_2 = 1\\). The algebraic multiplicity is 2, but the geometric multiplicity, that is the dimension of the eigenspace \\(E_\\lambda = \\operatorname{ker}(\\mathbf A - \\lambda\\mathbf I)\\) associated to \\(\\lambda=1\\), is only 1. The Jordan canonical form is \\[\\mathbf J = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}.\\]\nThe matrix exponential of the Jordan block follows from the property (for a \\(2\\times 2\\) block): \\[\\begin{pmatrix} \\lambda_i & 1 \\\\ 0 & \\lambda_i \\end{pmatrix}^k\n= \\begin{pmatrix} \\lambda_i^k & k \\lambda_i^{k-1} \\\\ 0 & \\lambda_i^k \\end{pmatrix}.\\]\nSo putting everything together we have \\[e^{t\\mathbf J} = \\begin{pmatrix} e^{\\lambda_i t} & t e^{\\lambda_i t} \\\\ 0 & e^{\\lambda_i t} \\end{pmatrix}.\\]\nPlease note that the appearance of the term \\(t e^{\\lambda_i t}\\).\nFor the sake of completeness, we just recall that the algebraic multiplicity is associated with the characteristic polynomial \\(\\mathcal{P}(\\lambda) = \\det(\\lambda\\mathbf{I}-\\mathbf{A})\\), and corresponds to the number of times a zero of \\(\\mathcal{P}(\\lambda)\\) appears. More precisely, we can always write \\(\\mathcal{P}(\\lambda) = (\\lambda-\\lambda_1)^{\\mu_1}\\cdot (\\lambda-\\lambda_2)^{\\mu_2}\\cdots (\\lambda-\\lambda_r)^{\\mu_r}\\), \\(\\{ \\lambda_i \\}_{i=1}^r\\), \\(r\\le n\\), are the eigenvalues, and \\(1\\le \\mu_i\\le n\\) the algebraic multiplicity of \\(\\lambda_i\\). The geometrical multiplicity \\(\\mu_i\\) is the dimension of the eigenspace \\(V_i = \\ker(\\lambda_i\\mathbf{I}-\\mathbf{A}) = \\{ \\mathbf{v}\\in\\mathbb{R}^n\\colon \\lambda_i\\mathbf{v}=\\mathbf{A}\\mathbf{v} \\}\\). Since \\(\\nu_i \\le \\mu_i\\), when at least one eigenvalue has \\(\\nu_i &lt; \\mu_i\\), the direct sum of all \\(V_i\\)s does not fill the whole \\(\\mathbb{R}^n\\), and additional (generalized) eigenvectors are required. These are taken from \\(\\mathbf{A}^k\\), for some \\(k=2,3,\\ldots\\), and leads to terms of the form \\(t^{k-1}e^{t\\lambda_i}\\) in the matrix exponential.\n\n\nA.5.4.2 Stability\nFor the linear ODEs \\(\\mathbf{y}' = \\mathbf{A}\\mathbf{y}\\), with \\(\\mathbf{A}\\in\\mathbb{R}^{n\\times n}\\) invertible, we only have the equilibrium \\(\\mathbf{y}^* = \\mathbf{0}\\). In the more general case \\(\\mathbf{y}' = \\mathbf{A}\\mathbf{y} + \\mathbf{b}\\), the equilibrium is \\(\\mathbf{y}^* = \\mathbf{A}^{-1}\\mathbf{b}\\). Note that in this case we can define \\(\\mathbf{z}(t) = \\mathbf{y}(t) - \\mathbf{y}^*\\) that satisfies the ODE \\(\\mathbf{z}' = \\mathbf{A}\\mathbf{z}\\) with equilibrium \\(\\mathbf{z}^* = \\mathbf{0}\\). Thus, we can focus on the homogeneous case with no loss of generality.\nWe now try to characterize the stability of the equilibrium \\(\\mathbf{y}^* = \\mathbf{A}\\) for \\(\\mathbf{y}' = \\mathbf{A}\\mathbf{y}\\).\nLet‚Äôs first consider the case of \\(\\mathbf{A}\\) diagonalizible. Here, there exists an invertible matrix \\(\\mathbf{V}\\in\\mathbb{R}^{n\\times n}\\) such that \\(\\mathbf{V}^{-1}\\mathbf{A}\\mathbf{V} = \\boldsymbol{\\Lambda}\\) is a diagonal matrix. The entries of this matrix are the eigenvalues of \\(\\mathbf{A}\\). The general solution of the ODE reads: \\[\\boldsymbol{\\phi}(t) = e^{t\\mathbf{A}}\\mathbf{y}_0 = \\mathbf{V} e^{t\\boldsymbol{\\Lambda}}\\mathbf{V}^{-1}\\mathbf{y}_0.\\]\nWe need to compute \\[\n\\lim_{t\\to\\infty} \\| \\boldsymbol{\\phi}(t) - \\mathbf{y}^* \\|,\n\\]\nfor an arbitrary initial condition \\(\\mathbf{y}_0\\).\nSince the matrix exponential of a diagonal matrix is just the component-wise exponentiation, we have that the components of \\(\\mathbf{V}^{-1}\\boldsymbol{\\phi}\\) are a linear combination of terms of the form \\(e^{\\lambda_i t}\\), being \\(\\lambda_i\\) the \\(i\\)-th eigenvalue of \\(\\mathbf{A}\\). Thus, we have that\n\nIf \\(\\operatorname{Re}\\lambda_i &lt; 0\\) for all \\(i=1,\\ldots,n\\), then \\(0\\) is the only globally attractive equilibrium.\nIf there exists at least one eigenvalue such that \\(\\operatorname{Re}\\lambda_i &gt; 0\\), the equilibrium \\(0\\) is unstable.\nIf \\(\\operatorname{Re}\\lambda_i \\le 0\\) for all \\(i=1,\\ldots,n\\), then \\(0\\) is stable.\n\nFor a generic \\(\\mathbf{A}\\in\\mathbb{R}^{n\\times n}\\) (diagonalizable or not), given the set of eigenvalues \\(\\lambda_i\\), the solution is some linear combination of terms of the form: \\[e^{\\lambda t}, t e^{\\lambda t}, \\ldots, t^m e^{\\lambda t},\\]\ndepending on the geometric multiplicity of \\(\\lambda\\). So we have that 1. and 2. above still applies, because the exponential is stronger than any polynomial. The non-trivial case is when \\(\\operatorname{Re}\\lambda_i = 0\\) for some \\(\\lambda_i\\). If \\(\\lambda_i\\) is such that we need extra terms of the form \\(t^j e^{\\lambda_i t}\\), \\(j\\ge 1\\), to complete the solution space, then the equilibrium is clearly unstable, because: \\[t^j e^{\\lambda_i t} = t^j \\bigl( \\cos(t\\operatorname{Im}\\lambda_i) + i \\sin(t\\operatorname{Im}\\lambda_i) \\bigr) \\to \\infty\\]\nas \\(t\\to\\infty\\). Otherwise, the equilibrium is stable (not asymptotically). The extra polynomial terms are required when the geometrical multiplicity of \\(\\lambda_i\\) is strictly lower than its algebraic multiplicity. So, in the general case we replace 3. above with\n\nIf \\(\\operatorname{Re}\\lambda_i \\le 0\\) for all \\(i=1,\\ldots,n\\), and for those with \\(\\operatorname{Re}\\lambda_i = 0\\) the algebraic and geometrical multiplicity coincide, then the equilibrium is stable.\n\nConcerning asymptotic stability, it is possible to find algebraic conditions on the coefficients of \\(\\mathbf{A}\\) such that \\(\\operatorname{Re}\\lambda_i &lt; 0\\) for all eigenvalues, without actually computing them.\n\n\nA.5.4.3 The \\(n=2\\) case\nThe characteristic polynomial of the matrix \\[\n\\mathbf{A} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\n\\]\nis as follows \\[\n\\mathcal{P}(\\lambda) = \\lambda^2 - (a+b)\\lambda + (ad-bc) = \\lambda^2 - \\operatorname{tr}(\\mathbf{A})\\lambda + \\det(\\mathbf{A}).\n\\]\nwhere we introduced the trace \\(\\operatorname{tr}(\\mathbf{A}) = a + b\\) and the determinant \\(\\det(\\mathbf{A}) = ad - bc\\).\n\nProposition A.5 The condition \\(\\operatorname{Re}\\lambda_i &lt; 0\\) for all \\(i=1,\\ldots,n\\) is equivalent to \\[\n\\operatorname{tr}(\\mathbf{A}) &lt; 0, \\quad\\text{and}\\quad\n\\det(\\mathbf{A}) &gt; 0.\n\\]\n\nThe proof is simple. We also know that: \\[\n\\begin{aligned}\n\\operatorname{tr}(\\mathbf{A}) &= \\lambda_1 + \\lambda_2, \\\\\n\\det(\\mathbf{A}) &= \\lambda_1 \\lambda_2,\n\\end{aligned}\n\\]\nbecause the above polynomial has always 2 roots on \\(\\mathbb{C}\\), so it admits a factorization \\(\\mathcal{P}(\\lambda) = (\\lambda - \\lambda_1)(\\lambda - \\lambda_2)\\) that expanded gives the equality.\nProposition¬†A.5 is one of the most important results for the course, and we will use it very often.\nThe case \\(n=2\\) is also very common in applications, thus it is worth studying in depth the equilibria in the phase space. We suppose that the equilibrium is the origin, that is we study the equation \\(\\mathbf{y}'=\\mathbf{A}\\mathbf{y}\\).\n\n\n\n\n\n\nExample (real and distinct eigenvalues)\n\n\n\nSuppose that the eigenvalues \\(\\lambda_1\\) and \\(\\lambda_2\\) of the matrix \\(\\mathbf{A}\\) are real, with eigenvectors \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\). Then the solution is: \\[\n\\mathbf{y}(t) = c_1 e^{t\\lambda_1}\\mathbf{v}_1 + c_2 e^{t\\lambda_2}\\mathbf{v}_2,\n\\] where \\(\\mathbf{c} = \\begin{bmatrix} c_1 \\\\ c_2 \\end{bmatrix} = \\mathbf{V}^{-1}\\mathbf{y}_0\\) and \\(\\mathbf{V}\\) is matrix with the eigenvectors. Thus, the solution is a linear combination of the eigenvectors.\nWhen \\(\\mathbf{y}_0 = \\alpha \\mathbf{v}_1\\), then \\(c_1=\\alpha\\) and \\(c_2=0\\), so we stay along the line with direction \\(\\mathbf{v}_1\\).Along this line, the solution is \\[\n\\mathbf{y}(t) = \\alpha e^{t\\lambda_1} \\mathbf{v}_1,\n\\]\nthus \\(\\mathbf{y}(t)\\) stays along the line and, when \\(\\lambda_1 &lt;0\\), approaches the equilibrium for \\(t\\to \\infty\\). In this case, we call the space generated by \\(\\mathbf{v}_1\\), the stable manifold of the equilibrium. Viceversa, for \\(\\lambda_1 &gt; 0\\), the trajectory diverges from the equilibrium; in this case, \\(\\mathbf{v}_1\\), is the unstable manifold.\nThe same applies to \\(\\mathbf{v}_2\\). So, when \\(\\lambda_1&lt;0\\) and \\(\\lambda_2 &lt; 0\\), the equilibrium is globally stable, with stable manifold is simply \\(\\mathbf{R}^2\\). The equilibrium is a stable node. When the eigenvalues are both positive, we have the opposite behavior, and the equilibrium is an unstable node. Finally, when they have opposite sign, the equilibrium is a saddle.\n\n\n\n\nA.5.4.4 The \\(n &gt; 2\\) case: Routh-Hurwitz criteria\nIn general, for \\(n&gt;2\\), we can find a set of algebraic conditions ensuring the asymptotic stability, without computing the eigenvalues.\nWe have the following\n\nTheorem A.7 Given the polynomial \\[\n\\mathcal{P}(\\lambda) = \\lambda^n + a_1 \\lambda^{n-1} + \\cdots + a_{n-1}\\lambda + a_n,\n\\]\nwhere the coefficients \\(a_i\\) are real constants, \\(i=1,\\ldots,n\\), define the \\(n\\) Hurwitz matrices using the coefficients \\(a_i\\) of the characteristic polynomial: \\[\n\\mathbf{H}_1 = \\begin{pmatrix} a_1 \\end{pmatrix},\n\\quad\n\\mathbf{H}_2 = \\begin{pmatrix} a_1 & 1 \\\\ a_3 & a_2 \\end{pmatrix},\n\\quad\n\\mathbf{H}_3 = \\begin{pmatrix} a_1 & 1 & 0 \\\\ a_3 & a_2 & a_1 \\\\ a_5 & a_4 & a_3 \\end{pmatrix},\n\\]\nand in general \\[\n\\mathbf{H}_n = \\begin{pmatrix}\na_1 &   1 &   0 &   0 & \\cdots & 0 \\\\\na_3 & a_2 & a_1 &   1 & \\cdots & 0 \\\\\na_5 & a_4 & a_3 & a_2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  0 &   0 &   0 &   0 & \\cdots & a_n\n\\end{pmatrix},\n\\]\nwhere \\(a_j = 0\\) if \\(j&gt;n\\). All the roots of the polynomial \\(\\mathcal{P}(\\lambda)\\) are negative or have negative real part if and only if the determinants of all Hurwitz matrices are positive: \\[\n\\det\\mathbf{H}_j &gt; 0, \\quad j=1,2,\\ldots,n.\n\\]\n\nWe can specialize the Theorem for low \\(n\\). We have\n\n\\(n=2\\): \\(\\det\\mathbf{H}_1 = a_1 &gt; 0\\) and \\(\\det\\mathbf{H}_2 = a_1a_2 &gt; 0\\). This is equivalent to \\(a_1&gt;0\\) and \\(a_2&gt;0\\). Note that \\(a_1 = -\\operatorname{tr}\\mathbf{A}\\) and \\(a_2 = \\det\\mathbf{A}\\).\n\\(n=3\\): as above, \\(a_1&gt;0\\), \\(a_1 a_2 - a_3 &gt; 0\\), and \\(\\det\\mathbf{H}_3 = a_3\\det\\mathbf{H}_2 &gt; 0\\). So, this is equivalent to \\(a_i &gt; 0\\) and \\(a_1a_2 - a_3 &gt; 0\\).\n\\(n=4\\): it is possible to show that we need \\(a_1&gt;0\\), \\(a_3 &gt; 0\\), \\(a_4 &gt; 0\\) and \\(a_1 a_2 a_3 &gt; a_3^2 + a_1^2 a_4\\).\n\nWe will make use of the case \\(n=3\\).\n\n\nA.5.4.5 Other useful cases\nSometimes we are going to deal with \\(4\\times 4\\) or larger matrices. Very often, though, they have a block structure. For instance: \\[\n\\mathbf{A} = \\begin{pmatrix}\n\\mathbf{A}_1 & \\mathbf{C} \\\\\n\\mathbf{0}   & \\mathbf{A}_2\n\\end{pmatrix},\n\\]\nwhere \\(\\mathbf{A}_1\\) and \\(\\mathbf{A}_2\\) can differ in dimension. Then, the eigenvalues of \\(\\mathbf{A}\\) are the union of the eigenvalues of \\(\\mathbf{A}_1\\) and \\(\\mathbf{A}_2\\). A specific case is the triangular matrix, where eigenvalues are on the diagonal.\n\n\n\nA.5.5 Linearization method\nThe analysis of stability quickly becomes impractical for complex non-linearities. The study of local stability properties, however, can be carried out fairly easily. Suppose that \\(\\mathbf{y}^*\\in\\mathbb{R}^n\\) is an equilibrium of the dynamical system \\((\\mathbf{f},\\Omega)\\), with \\(\\mathbf{f}\\in\\mathcal{C}^1(\\Omega)\\). Then \\[\\mathbf{f}(\\mathbf{y}) = \\mathbf{f}(\\mathbf{y}^*) + D\\mathbf{f}(\\mathbf{y}^*)(\\mathbf{y}-\\mathbf{y}^*) + \\ldots,\\] where \\(D\\mathbf{f}(\\mathbf{y}^*)\\) is the Jacobian of \\(\\mathbf{f}\\). Since \\(\\mathbf{f}(\\mathbf{y}^*) = 0\\), we have the following linear approximation of the dynamical system: \\[\\mathbf{y}' = D\\mathbf{f}(\\mathbf{y}^*)(\\mathbf{y}-\\mathbf{y}^*) + \\ldots\\] We now consider \\(\\mathbf{z}(t) = \\mathbf{y}(t) - \\mathbf{y}^*\\), and notice that \\(\\mathbf{z}' = \\mathbf{y}'\\), to obtain: \\[\\mathbf{z}' = \\mathbf{A}\\mathbf{z},\\] where we set \\(\\mathbf{A}=D\\mathbf{f}(\\mathbf{y}^*)\\). The dynamical system in \\(\\mathbf{z}\\) is linear, and we know how to analyze the stability of the equilibrium \\(\\mathbf{z}=0\\). (This corresponds to the equilibrium \\(\\mathbf{y}^*\\) in the original variables.) In fact, let \\(\\{\\lambda_i \\}\\) be the eigenvalues of \\(D\\mathbf{f}(\\mathbf{y}^*)\\). Then:\n\nIf \\(\\operatorname{Re}\\lambda_i &lt; 0\\) for all \\(i=1,\\ldots,r\\), then \\(\\mathbf{y}^*\\) is locally asymptotically stable.\nIf there exists \\(i\\in\\{1,\\ldots,r\\}\\) such that \\(\\operatorname{Re}\\lambda_i &gt; 0\\), the equilibrium is locally unstable.\n\nWe cannot conclude anything regarding the case \\(\\operatorname{Re}\\lambda_i = 0\\), because in this case higher order terms in the Taylor expansion dictates the local dynamics of the system.\n\n\n\n\n\n\nExample (logistic equation, alternative analysis)\n\n\n\nWe consider again the ODE \\(y'=y(1-y)\\). Given \\(f(y)=y(1-y)\\), we have \\(f'(y)=1-2y\\). At the first equilibrium, \\(f'(0)=1 &gt; 0\\), so it is locally unstable. For the second one, \\(f'(1)=-1&lt;0\\), so it is locally asympotically stable.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Introduction to ODEs</span>"
    ]
  },
  {
    "objectID": "IntroODE.html#periodic-orbits",
    "href": "IntroODE.html#periodic-orbits",
    "title": "Appendix A ‚Äî Introduction to ODEs",
    "section": "A.6 Periodic orbits",
    "text": "A.6 Periodic orbits\nSo far, we went is more great detail in the study of equilibria of a dynamical system. Equilibria as ‚Äúsimple‚Äù, in the sense that we can find all of them as solution of the nonlinear system \\(\\mathbf{f}(\\mathbf{y}) = \\mathbf{0}\\). Their local stability follows from the linearization of the ODE around the equilibrium.\nPeriodic orbits are more difficult to characterize, but they are extremely important in applications.\n\nA.6.1 Hamiltonian systems\nConsider a system of 2 ODEs, written in general as \\[\n\\left\\{\\begin{aligned}\ny_1' &= f_1(y_1,y_2), \\\\\ny_2' &= f_2(y_1,y_2).\n\\end{aligned}\\right.\n\\]\nThe phase space is planar. Let us introduce the vector field \\[\n\\mathbf{f}(y_1,y_2) = \\begin{pmatrix} f_1(y_1,y_2) \\\\ f_2(y_1,y_2) \\end{pmatrix},\n\\]\nand suppose that given \\(\\Omega\\subset\\mathbb{R}^2\\) we have a dynamical system \\((\\mathbf{f},\\Omega)\\). We introduce another vector field: \\[\n\\mathbf{G}(y_1,y_2) = \\begin{pmatrix} -f_2(y_1,y_2) \\\\ f_1(y_1,y_2) \\\\ 0 \\end{pmatrix},\n\\]\nwith the following associated differential form \\[\n\\omega(y_1,y_2) = -f_2(y_1,y_2)\\mathrm{d}y_1 + f_1(y_1,y_2)\\mathrm{d}y_2.\n\\]\nRecall the following interpretation of a differential form: given an infinitesimal displacement \\(\\mathrm{d}\\mathbf{y}\\) in \\(\\mathbb{R}^3\\), the quantity \\(\\omega = \\langle\\mathbf{G},\\mathrm{d}\\mathbf{y}\\rangle\\) is the infinitesimal work done by \\(\\mathbf{G}\\) along \\(\\mathrm{d}\\mathbf{y}\\). A \\(\\mathcal{C}^1(\\Omega)\\) differential form is exact when there exists a \\(\\mathcal{C}^2(\\Omega)\\) function \\(H: \\Omega\\to\\mathbb{R}\\) such that \\(\\mathrm{d}H = \\omega\\) in \\(\\Omega\\). In particular, \\[\n\\mathrm{d}H = \\langle\\nabla H, \\mathrm{d}\\mathbf{y}\\rangle = \\langle\\mathbf{G},\\mathrm{d}\\mathbf{y}\\rangle = \\omega,\n\\]\nso we conclude that \\(\\nabla H = \\mathbf{G}\\). We call \\(H\\) potential or Hamiltonian function.\nA simple necessary condition for exactness is that the curl of \\(\\mathbf{G}\\) cancels, since \\(\\nabla\\times\\mathbf{G} = \\nabla\\times\\nabla H = \\mathbf{0}\\), so \\[\n\\nabla\\times \\mathbf{G}(y_1,y_2) = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\frac{\\partial f_1(y_1,y_2)}{\\partial y_1} + \\frac{\\partial f_2(y_1,y_2)}{\\partial y_2} \\end{pmatrix} = \\mathbf{0}.\n\\]\nThe condition \\[\n\\frac{\\partial f_1(y_1,y_2)}{\\partial y_1} + \\frac{\\partial f_2(y_1,y_2)}{\\partial y_2} = 0,\n\\]\nis also sufficient for \\(\\omega\\) being exact when \\(\\Omega\\) is simply connected. In conclusion, if \\(\\nabla\\times\\mathbf{G}=\\mathbf{0}\\) and \\(\\Omega\\) is simply connected, then there exists a Hamiltonian function \\(H(y_1,y_2)\\) such that: \\[\n\\left\\{\\begin{aligned}\ny_1' &= \\frac{\\partial H}{\\partial y_2}(y_1,y_2), \\\\\ny_2' &= -\\frac{\\partial H}{\\partial y_1}(y_1,y_2).\n\\end{aligned}\\right.\n\\]\nODEs of the above form are called Hamiltonian systems. They are very common in mechanics.\nFor a Hamiltonian system, orbits are level sets of the function \\(H\\). Indeed, given a trajectory \\(\\mathbf{y}(t)\\) of the ODE, we have that the Hamiltonian function along the trajectory is constant: \\[\n\\frac{\\mathrm{d}}{\\mathrm{d}t} H\\bigl(y_1(t),y_2(t)\\bigr) =\n\\frac{\\partial H}{\\partial y_1} y_1'(t) + \\frac{\\partial H}{\\partial y_2} y_2'(t) =\n\\frac{\\partial H}{\\partial y_1} \\frac{\\partial H}{\\partial y_2} - \\frac{\\partial H}{\\partial y_2} \\frac{\\partial H}{\\partial y_1} = 0.\n\\]\nThus, \\(H\\bigl(y_1(t),y_2(t)\\bigr) = \\text{constant}\\) for \\(t\\ge 0\\). Given \\(H(y_1,y_2)\\), we can easily draw the phase portrait.\n\n\n\n\n\n\nExample (conservative systems)\n\n\n\nConsider the ODE \\[\nm x'' = F(x),\n\\]\nwhere \\(m\\) is the mass of a particle and \\(F(x)\\) is a force. We can recast this into a system: \\[\n\\left\\{\\begin{aligned}\ny_1' &= \\tfrac{1}{m} y_2, \\\\\ny_2' &= F(y_1),\n\\end{aligned}\\right.\n\\]\nwhere \\(y_1 = x\\) is the position and \\(y_2 = m x'\\) is the momentum.\nWhen the force is conservative, there exists a potential energy \\(U(x)\\) such that \\(F = -U'\\). This implies the existence of a Hamiltonian function \\[\nH(y_1,y_2) = \\frac{1}{2m} y_2^2 + \\frac{1}{m} U(y_1),\n\\]\nwhich is exactly the total energy, the sum of kinetic and potential energy. This quantity is conserved and the orbits are level sets of \\(H = H_0\\), where \\(H_0 = H(y_1(0),y_2(0))\\) is the initial energy of the particle.\nTake for instance the Lennard-Jones potential below:\n\n\n\nLennard-Jones potential\n\n\nWe observe that if the initial energy is between \\(-1\\) and \\(0\\), then we have closed orbits. These corresponds to periodic solutions. When the initial energy is positive, then orbits are open.\n\n\n\n\n\n\n\n\nExercise (Double-well potential)\n\n\n\nFind a Hamiltonian for the ODE \\[\nx'' = x - x^3,\n\\]\nand show how orbits vary as \\(H_0\\) increases.\n\n\n\n\n\n\n\n\nExample (Lotka-Volterra system)\n\n\n\nConsider the system: \\[\n\\left\\{\\begin{aligned}\ny_1' &= y_1(a-by_2), \\\\\ny_2' &= y_2(-c + dy_1).\n\\end{aligned}\\right.\n\\]\nwhere \\(y_1(t)\\) and \\(y_2(t)\\) are respectively the prey and the predator density. We will study this system more in depth. We can find the Hamiltonian in the positive quadrant as follows. Note that \\[\n\\frac{\\mathrm{d}y_1}{\\mathrm{d}y_2} = \\frac{y_1(a-by_2)}{y_2(-c + dy_1)},\n\\]\nso we can separate the variables: \\[\n\\frac{(-c + dy_1)\\mathrm{d}y_1}{y_1} = \\frac{(a-by_2)\\mathrm{d}y_2}{y_2},\n\\]\nand integrate: \\[\n\\int_{y_1(0)}^{y_1(t)} \\Bigl(-\\frac{c}{y} + d\\Bigr)\\mathrm{d}y = \\int_{y_2(0)}^{y_2(t)} \\Bigl( \\frac{a}{y_2} - b \\Bigr) \\mathrm{d}y_2,\n\\]\nwe obtain that \\(H(y_1(t),y_2(t)) = H(y_1(0),y_2(0))\\) with \\[\nH(y_1,y_2) = -c \\ln y_1 + dy_1 - a\\ln y_2 + by_2.\n\\]\nThis function has a minimum at \\((y_1^*,y_2^*) = (\\frac{a}{b}, \\frac{c}{d})\\), since \\(\\nabla H(y_1^*,y_2^*) = 0\\) and the Hessian is positive definite. For \\(y_1\\) or \\(y_2\\to 0\\) we have \\(H\\to+\\infty\\), as well for \\(y_{1,2}\\to+\\infty\\). So, the function has level set that are closed curves, and those are associated with periodic orbits.\n\n\n\n\nA.6.2 Isolated periodic orbits\nWe have seen that periodic orbits are possible in planar systems. In all the above cases, however, periodic orbits are packed, that is if we have an orbit for an energy level \\(H_0\\), then we can have another periodic orbit infinitesimally close to the original one. Is it possible to have isolated periodic orbits? In other words, is it possible that there exists an open set \\(\\Omega'\\subseteq\\Omega\\) such that \\(\\Omega'\\) contains only one periodic orbit? If so, we call this orbit a limit cycle.\n\n\n\n\n\n\nExample\n\n\n\nThe ODE \\[\n\\left\\{\\begin{aligned}\ny_1' &= y_2, \\\\\ny_2' &= y_2(1-y_1^2+y_2^2) - y_1.\n\\end{aligned}\\right.\n\\]\nhas an isolated limit cycle for \\(y_1(t)^2 + y_2(t)^2 = 1\\). To see this, first note that \\((y_1(t),y_2(t)) = (\\cos t,\\sin t)\\) is a (periodic) solution. Now we check the stability.\nTake the function \\[\nE(y_1,y_2) = \\frac{1}{2}(y_1^2 + y_2^2),\n\\]\nand note that \\[\n\\frac{\\mathrm{d}}{\\mathrm{d}t} E\\bigl(y_1(t),y_2(t)\\bigr) =\n\\bigl(1- y_1(t)^2 - y_2(t)^2\\bigr)y_2(t)^2.\n\\]\nThus, if \\(y_1^2 + y_2^2 \\le 1\\) we have \\(E'(t) \\ge 0\\), whereas if \\(y_1^2 + y_2^2 \\ge 1\\) we have \\(E'(t)\\le 0\\). Hence, the limit cycle is attracting orbits.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Introduction to ODEs</span>"
    ]
  },
  {
    "objectID": "IntroODE.html#limit-cycles",
    "href": "IntroODE.html#limit-cycles",
    "title": "Appendix A ‚Äî Introduction to ODEs",
    "section": "A.7 Limit cycles",
    "text": "A.7 Limit cycles\nWe have seen that periodic orbits are possible in planar systems. In all the above cases, however, periodic orbits are packed, that is if we have an orbit for an energy level \\(H_0\\), then we can have another periodic orbit infinitesimally close to the original one. Is it possible to have isolated periodic orbits? In other words, is it possible that there exists an open set \\(\\Omega'\\subseteq\\Omega\\) such that \\(\\Omega'\\) contains only one periodic orbit? If so, we call this orbit a limit cycle.\n\nA.7.1 Dulac‚Äôs criterium\nWe have the following result of non-existence of limit cycles:\n\nTheorem A.8 (Dulac‚Äôs criterium) Let \\(\\mathbf{y}' = \\mathbf{f}(\\mathbf{y})\\) a planar dynamical system with \\(\\mathbf{f}\\in\\mathcal{C}^1(\\Omega)\\), \\(\\Omega\\subset\\mathbb{R}^2\\) open. If there exists a function \\(h\\in\\mathcal{C}^1(\\Omega)\\) such that \\(\\operatorname{div}(h\\mathbf{f})\\) does not change sign in \\(\\Omega'\\subseteq\\Omega\\) simply connected, then there exists no limit cycle in \\(\\Omega'\\).\n\nNote that the theorem applies also with \\(h\\equiv 1\\). The proof is simple. Take a closed orbit entirely lying in the region \\(\\Omega'\\). We denote by \\(C\\subset\\Omega'\\) the interior of the orbit whose boundary \\(\\partial C\\) is the orbit itself. Then \\[\n\\int_{C} \\operatorname{div}(h\\mathbf{f})\\mathrm{d}\\mathbb{y} = \\int_{\\partial C} h\\langle \\mathbf{f},\\mathbf{n}\\rangle \\mathrm{d}s.\n\\]\nSince \\(\\partial C\\) is an orbit, \\(\\langle \\mathbf{f},\\mathbf{n}\\rangle = 0\\), so the second integral is zero, no matter of the choice of \\(h\\). But the first integral cannot be zero, because \\(\\operatorname{div}(h\\mathbf{f})\\) has a constant sign in \\(C\\). The contraction yields to the non-existence of such a periodic orbit.\nUnfortunately there is no algorithm for finding the function \\(h\\). Usually is of the form \\((y_1y_2)^{-1}\\), \\(e^{y_1}\\), \\(e^{y_2}\\), ‚Ä¶\n\n\n\n\n\n\nExample\n\n\n\nConsider the ODE: \\[\n\\left\\{\\begin{aligned}\ny_1' &= y_1(2-y_1-y_2), \\\\\ny_2' &= y_2(4y_1-y_1^2-3).\n\\end{aligned}\\right.\n\\]\nWe can show that the system has no closed orbits in the positive quadrant. We take \\(h(y_1,y_2) = 1/(y_1 y_2)\\). Then: \\[\n\\nabla\\cdot(h\\mathbf{f}) = \\frac{\\partial}{\\partial y_1}\\left(\\frac{2-y_1-y_2}{y_2}\\right) + \\frac{\\partial}{\\partial y_2}\\left( \\frac{4y_1-y_1^2-3}{y_1}\\right) = -\\frac{1}{y_2} &lt; 0.\n\\]\nThe region \\(\\mathrm{R}^2_+\\) is simply connected, and the functions are smooth, so there are no limit cycles in it.\n\n\n\n\nA.7.2 Poincar√©-Bendixson existence theorems\n\nTheorem A.9 (Poincar√©-Bendixson) Suppose that \\((\\mathbf{f},\\Omega)\\) is a planar dynamical system, \\(\\Omega\\subset\\mathbb{R}^2\\), with \\(\\mathbf{f}\\in\\mathcal{C}^1(\\Omega)\\). If there exists a closed and bounded region \\(R\\subseteq\\Omega\\) that does not contain any equilibrium and there exists a trajectory \\(C\\) that is ‚Äúconfined‚Äù in \\(D\\), in the sense that it starts in \\(D\\) and stays in \\(D\\) for all future time, then either \\(C\\) is a closed orbit, or it spirals toward a closed orbit at \\(t\\to\\infty\\). In either case, \\(D\\) contains a closed orbit.\n\nIn order to apply the theorem, we need to find a ‚Äútrapped‚Äù orbit in \\(D\\). How to proceed? The idea is the following: we can construct a region \\(D\\) that is closed and connected, and such that its boundary is inflow for the dynamical system. That is, the vector field \\(\\mathbf{f}\\) only enters into \\(D\\). So, orbits cannot leave.\n\n\n\nTrapping region\n\n\nSome comments:\n\nThe requirement for no equilibria is important. Very often we do have an equilibrium inside (as we are going to see in a moment, a limit cycle always has an equilibrium inside of it), but we can remove it by ‚Äúdigging‚Äù a hole around it.\nEquilibria cannot be on the border of the region. Consider a homoclinic orbit as in the figure below. We see that the ‚Äúpunctured‚Äù region traps an orbit \\(C\\), but there are no limit cycles inside. The orbit tends to the homoclinic.\n\n\n\n\nHomoclinic orbit\n\n\nWe can be more precise. Let us define\n\nDefinition A.15 (\\(\\alpha\\)-limit and \\(\\omega\\)-limit sets) We define the \\(\\alpha\\)-limit and the \\(\\omega\\)-limit of an initial condition \\(\\mathbf{y}_0\\) as the set of \\(\\Omega\\) that the trajectory approches as \\(t\\to-\\infty\\) and \\(t\\to\\infty\\), respectively: \\[\n\\begin{aligned}\n\\alpha(\\mathbf{y}_0) &= \\lim_{t\\to-\\infty} \\boldsymbol{\\Phi}(t,\\mathbf{y}_0), \\\\\n\\omega(\\mathbf{y}_0) &= \\lim_{t\\to+\\infty} \\boldsymbol{\\Phi}(t,\\mathbf{y}_0).\n\\end{aligned}\n\\]\n\nThen, we have the following\n\nTheorem A.10 (Poincar√©-Bendixson Trichotomy) Suppose that \\((\\mathbf{f},\\Omega)\\) is a planar dynamical system, \\(\\Omega\\subset\\mathbb{R}^2\\), with \\(\\mathbf{f}\\in\\mathcal{C}^1(\\Omega)\\). Let \\(C^+(\\mathbf{y}_0) = \\{ \\boldsymbol{\\Phi}(t,\\mathbf{y}_0), t\\ge 0\\}\\) a positive orbit that remains in a closed, bounded region \\(R\\subseteq\\Omega\\) that contains only a finite number of equilibria. Then the \\(\\omega\\)-limit set takes one of the following three forms:\n\n\\(\\omega(\\mathbf{y}_0)\\) is an equilibrium,\n\\(\\omega(\\mathbf{y}_0)\\) is a periodic orbits,\n\\(\\omega(\\mathbf{y}_0)\\) is a singular cycle, that is \\(\\omega(\\mathbf{y}_0)\\) contains a finite number of equilibria and a set of orbits whose \\(\\alpha\\)- and \\(\\omega\\)-limit sets consist of one these equilibria for each orbit.\n\n\nThe point 3. is for instance a heteroclinic, which contains two saddle equilibria connected by the stable and unstable manifolds, or a homoclinic orbit, which is a single saddle equilibrium where the stable and unstable manifolds correspond (for \\(t\\ge 0\\)).\nThe proofs of these theorems heavily rely on concepts from topology. Here, we only recall index theory for a planar dynamical system.\n\n\n\n\n\n\nExample (glycolysis)\n\n\n\nWe consider a model for glycolysis as proposed by Sel‚Äôkov (1968). In yeast cells glycolysis can proceed in a oscillatory fashion. Here is a model: \\[\n\\left\\{\\begin{aligned}\nx' &= -x + ay + x^2 y, \\\\\ny' &= b - ay - x^2 y,\n\\end{aligned}\\right.\n\\]\nwhere \\(x(t)\\) is the concentration of ADP (adenosine diphosphate) and \\(y(t)\\) is the concentration of F6P (fructose-6-phosphate). The parameters \\(a&gt;0\\) and \\(b&gt;0\\) comes from the kinetic of the reaction.\nThe phase portrait is as follows:\n\n\n\nTrapping region\n\n\nBy looking at the arrows, it looks like we could have a limit cycle or some stable equilibrium. We construct the trapping region as follows:\n\n\n\nTrapping region\n\n\nNow we check. Graphically, we note that\n\nAbove the nullcline \\(x'=0\\) we have \\(x'&gt;0\\), below \\(x'&lt;0\\).\nOn the left of the nullcline \\(y'=0\\) we have \\(y'&gt;0\\), on the right \\(y'&lt;0\\).\n\nThus, the horizontal and vertical parts in of the region are easy to check, just by looking at the arrows. The diagonal part is more subtle. Intuitively, when \\(x\\) and \\(y\\) are large, the ODE is approximately \\(x' \\approx x^2 y\\) and \\(y' \\approx -x^2 y\\). So, \\(y'/x' \\approx -1\\), which is tangent to the trajectories. This suggests to take: \\[\nx' - (-y') = -x +ay + x^2y + (b - ay -x^2y) = b - x.\n\\]\nHence, for \\(x&gt;b\\) we have \\(-y' &gt; x'\\), and trajectories are entering into the diagonal part, because their slope is more negative than \\(-1\\).\nWe conclude that \\(R\\) is a trapping region. However, we cannot apply the Theorem, because we still need to rule out equilibria. We have only one, at the intersection between nullclines. We can do so just by digging a hole around it, obtaining a ‚Äúpunctured‚Äù region.\n\n\n\nPunctured region\n\n\nFor a sufficiently small hole, we can ensure that orbits are leaving or entering the region just by looking at the stability of the equilibrium. If it is unstable, then orbits are diverging, and thus entering into \\(R\\). That is, \\(R\\) traps orbits and it does not contain equilibria. Therefore, there must exist a closed orbit in it!\nThe stability of the equilibrium is easy to check. First, the equilibrium is \\[\n(x^*,y^*) =  \\Bigl( b, \\frac{b}{a+b^2} \\Bigr).\n\\]\nThe Jacobian is: \\[\nJ_\\mathbf{f}(x,y) = \\begin{pmatrix} -1 + 2xy & a + x^2 \\\\ -2xy & -a-x^2 \\end{pmatrix}.\n\\]\nThe trace and the determinant at \\((x,y)=(x^*,y^*)\\): \\[\n\\det J=a+b^2&gt;0, \\quad\n\\operatorname{tr}J = -\\frac{b^4 + (2a-1)b^2 + (a+a^2)}{a+b^2}.\n\\]\nDepending on \\(a\\) and \\(b\\), the trace can be either positive or negative. Solving the numerator for \\(b^2\\) we have that the trace is zero for \\[\nb^2 = \\frac{1}{2}\\Bigl( 1 - 2a \\pm \\sqrt{1-8a} \\Bigr).\n\\]\nFor choice of the parameters such that the trace is positive, the equilibrium is unstable and the region contains a closed orbit.\n\n\n\n\nA.7.3 Index theory\n\nDefinition A.16 (index of a closed curve) We define the index of a closed curve \\(C\\), denoted by \\(i_C\\), as the net number of counterclockwise revolutions made by the vector field \\(\\mathbf{f}\\) around \\(C\\). More precisely, take the angle \\(\\phi(t)\\) between the tangent to the curve \\(C\\) and the vector field \\(\\mathbf{f}\\). Then, \\(\\phi(t)\\) has changed by an integer multiple of \\(2\\pi\\). Such integer is the index.\n\n\n\n\nIndex of a curve\n\n\nThe index closely resembles residue theory in complex analysis. For instance, we have that\n\nIf \\(C\\) can be continuously deformed into \\(C'\\) without passing through an equilibrium, then \\(i_C = i_{C'}\\). (Note that \\(i_C\\) is an integer-valued function of \\(C\\). The only way \\(i_C\\) can be a continuous function of \\(C\\) is being constant!)\nIf \\(C\\) does not enclose any equilibrium, then \\(i_C = 0\\). (Since \\(C\\) does not contain any zero of \\(\\mathbf{f}\\), we can shrink \\(C\\) to a tiny circle where \\(\\mathbf{f}\\) is almost constant, so the index is zero.)\nIf we reverse the arrows of the vector field, that is \\(t\\mapsto -t\\), the index is unchanged.\nSuppose that \\(C\\) is a closed orbit. Then \\(i_C = +1\\).\n\nMathematically, given the vector field \\(\\mathbf{f}(x,y) = (f(x,t), g(x,y))\\) in \\(\\mathbb{R}^2\\), the angle given by \\[\n\\phi = \\tan^{-1}(g/f),\n\\]\nso in terms of differentials we have: \\[\n\\mathrm{d}\\phi = \\mathrm{d}\\bigl(\\tan^{-1}(g/f)\\bigr) = \\frac{f\\mathrm{d}g - g\\mathrm{d}f}{f^2+g^2}.\n\\]\nThe index is, therefore, \\[\ni_C = \\frac{1}{2\\pi}\\int_C \\mathrm{d}\\phi = \\frac{1}{2\\pi}\\int_C \\frac{f\\mathrm{d}g - g\\mathrm{d}f}{f^2+g^2}.\n\\]\n\nDefinition A.17 (Index of an equilibrium) The index of an equilibrium \\(\\mathbf{y}^*\\), denoted by \\(i_{\\mathbf{y}^*}\\) is the index of a curve containing the equilibrium.\n\nNote that thanks to 1. the definition of \\(i_{\\mathbf{y}^*}\\) does not depend on the choice of \\(C\\).\n\nTheorem A.11 All hyperbolic equilibria, that is the Jacobian of \\(\\mathbf{f}\\) is non-singular at the equilibrium, have \\(i_{\\mathbf{y}^*} = \\pm 1\\). In particular, only the saddle has \\(i_{\\mathbf{y}^*} = -1\\).\n\nA non-hyperbolic equilibrium can also have index 0, for instance a saddle-node.\n\nTheorem A.12 If a closed curve \\(C\\) surrounds \\(n\\) isolated equilibria \\(\\mathbf{y}_1^*, \\mathbf{y}_2^*, \\ldots,\\mathbf{y}_n^*\\), then \\[\ni_C = \\sum_{k=1}^n i_{\\mathbf{y}_n^*}.\n\\]\n\nThe proof is familiar from multivariate calculus. We shrink the curve \\(C\\) so to surround the equilibria with tiny circles and corridors connecting them. The index does not change. Then we show that the contribution of corridors cancels out, because of symmetry reasons. So, we are only left with circles around the equilibria, which is the sum of the indices of each equilibrium.\n\nTheorem A.13 (Poincar√©) Any closed periodic orbit in the phase plane must enclose equilibria whose indices sum to \\(+1\\). If all equilibria are hyperbolic, then the limit cycle contains exactly \\(2s + 1\\) equilibria, \\(s\\) saddles and \\(s+1\\) non saddles, with \\(s\\ge 0\\) integer.\n\n\n\n\n\n\n\nExample\n\n\n\nConsider the system: \\[\n\\left\\{\\begin{aligned}\ny_1' &= -y_1 - y_1^2 y_2^2, \\\\\ny_2' &= y_1 + y_2.\n\\end{aligned}\\right.\n\\]\nWe show now that there exists no limit cycle around the origin.\nFirst, notice that the divergence of \\(\\mathbf{f}\\) is: \\[\n\\operatorname{div}\\mathbf{f} = -2y_1 y_2^2,\n\\]\nwhich changes sign in \\(\\mathbb{R}^2\\), so we cannot exclude the existence of a limit cycle.\nWe have 2 equilibria, \\(\\mathbf{y}_1^* = (0,0)\\) and \\(\\mathbf{y}_2^* = (-1,1)\\). In particular, \\(\\mathbf{y}_1^*\\) is a saddle. A limit cycle around the origin must contain the saddle, but the index of the cycle is \\(+1\\), whereas the index of the saddle is \\(-1\\). It cannot be. Even the case of a limit cycle surrounding both equilibria is not possible, because the sum of the indices is \\(0\\). We conclude that we cnnot have a limit cycle around the origin.\n\n\n\n\nA.7.4 Poincar√© maps\nWe still need to discuss the stability of limit cycles. Since they are isolated by definition, they can attract or repulse nearby orbits.\nConsider a periodic orbit \\(\\Gamma\\subset\\mathbb{R}^n\\), and take any point \\(\\mathbf{y}_0\\in\\Gamma\\). We introduce a cross-section \\(\\Sigma\\) to the cycle at this point as a smooth hypersurface of dimension \\(n-1\\) that crosses \\(\\Gamma\\) at non-zero angle. A non-zero angle means that the normal of the hypersurface at \\(\\mathbf{y}_0\\) is not orthogonal to the tangent of \\(\\Gamma\\) at \\(\\mathbf{y}_0\\).\nNote that the limit cycle reduces to a point on the cross-section \\(\\Sigma\\). Other closeby orbits will also intersect the cross-section \\(\\Sigma\\) at some angle (by continuity the angle remains non-zero in a neighborhood of \\(\\mathbf{y}_0\\)). A point \\(\\mathbf{z}\\in\\Sigma\\), close to \\(\\mathbf{y}_0\\), will give some orbit through the flow of the ODE, and eventually return to \\(\\Sigma\\) after some time at a point \\(\\tilde{\\mathbf{z}}\\). This procedure is encoded in a map \\(\\mathbf{z}\\mapsto \\tilde{\\mathbf{z}} = \\mathcal{P}(\\mathbf{z})\\).\nDefinition (Poincar√© map). We call \\(\\mathcal{P}:\\Sigma\\to \\Sigma\\) the Poincar√© map of the periodic orbit \\(\\Gamma\\).\n\n\n\nPoincar√© maps\n\n\nA fixed point \\(\\mathbf{z}^*\\) of the map \\(\\mathcal{P}\\) corresponds to a limit cycle for the original system. Specifically, if \\(\\mathbf{z}^*\\) is a stable equilibrium for the discrete map \\[\n\\mathbf{z}^{(k+1)} = \\mathcal{P}(\\mathbf{z}^{(k)}),\n\\]\nthat is, \\(\\mathbf{z}^* = \\mathcal{P}(\\mathbf{z}^*)\\), then the limit cycle is stable. If \\(\\mathbf{z}^*\\) is asympotically stable, then the limit cycle is asymptotically stable. Otherwise, it is unstable.\n\n\n\n\n\n\nExample\n\n\n\nConsider the following ODE is polar coordinates: \\[\n\\left\\{\\begin{aligned}\nr'     &= r(1-r^2), \\\\\n\\theta' &= 1.\n\\end{aligned}\\right.\n\\]\nSince \\(\\theta' = 1\\), we make a turn after a period of \\(2\\pi\\). Thus, if we start from \\(r_0\\), we return at \\(r_1\\) after \\(\\Delta t = 2\\pi\\). Hence: \\[\n\\int_{r_0}^{r_1} \\frac{\\mathrm{d}r}{r(1-r^2)} = \\int_0^{2\\pi}\\mathrm{d}t = 2\\pi.\n\\]\nIntegrating we get: \\[\n\\mathcal{P}(r) = \\frac{1}{\\sqrt{1+e^{-4\\pi}(r^{-2}-1)}}.\n\\]\nFor \\(r^* = 1\\) we have a fixed point. We can show (graphically with the cobweb plot) that the equilibrium is attractive.\n\n\nGiven a parametrization of \\(\\mathcal{P}\\) in some local coordinate system \\(\\boldsymbol{\\xi} = (\\xi_1, \\ldots, \\xi_{n-1})\\) such that \\(\\boldsymbol{\\xi}=\\mathbf{0}\\) corresponds to \\(\\mathbf{z}^*\\), the origin is a fixed point of the map: \\(\\mathcal{P}(\\mathbf{0}) = \\mathbf{0}\\). Thanks to the contraction Theorem, the fixed-point is asymptotically stable if \\(\\mathcal{P}\\) is a contraction, which means that \\[\n\\mathbf{B} = \\left.\\frac{\\mathrm{d}\\mathcal{P}}{\\mathrm{d}\\boldsymbol{\\xi}}\\right|_{\\boldsymbol{\\xi}=\\mathbf{0}}\n\\]\nhas eigenvalues (multipliers) \\(\\mu_1, \\ldots, \\mu_{n-1}\\) with modulus strictly less than 1, that is \\(|\\mu_i|&lt;1\\). It is possible to show that the multipliers are independent on the choice of the cross-section, the local parametrization, and the point \\(\\mathbf{y}_0\\in \\Gamma\\). In other words, the multipliers are a characteristic of the limit cycle, from which we can deduce the stability.\nWe can now formalize the stability for a limit cycle. Consider a periodic solution \\(\\boldsymbol{\\phi}(t)\\) of the dynamical system \\[\n\\mathbf{y}' = \\mathbf{f}(\\mathbf{y}),\n\\]\nthat is \\(\\boldsymbol{\\phi}(t)\\) is a solution and \\(\\boldsymbol{\\phi}(t+T_0) = \\boldsymbol{\\phi}(t)\\) for some \\(T_0&gt;0\\), called the period of the limit cycle. Now we perturb the periodic orbit: \\[\n\\tilde{\\boldsymbol{\\phi}}(t) = \\boldsymbol{\\phi}(t) + \\mathbf{u}(t),\n\\]\nfor some perturbation \\(\\mathbf{u}(t)\\). Now notice that: \\[\n\\mathbf{u}'(t) = \\tilde{\\boldsymbol{\\phi}}(t) - \\boldsymbol{\\phi}(t) = \\mathbf{f}(\\boldsymbol{\\phi}(t) + \\mathbf{u}(t)) - \\mathbf{f}(\\boldsymbol{\\phi}(t)) = \\mathbf{A}(t)\\mathbf{u}(t) + \\mathcal{O}(\\| \\mathbf{u}(t) \\|^2 ),\n\\]\nwhere \\(\\mathbf{A} = J_\\mathbf{f}(\\boldsymbol{\\phi}(t))\\), and \\(\\mathbf{A}(t+T_0) = \\mathbf{A}(t)\\). The system \\(\\mathbf{u}' = \\mathbf{A}(t)\\mathbf{u}\\) is non-autonomous and linear. The set of solutions form the Wronskian matrix \\(\\mathbf{W}(t)\\) and are such that \\[\n\\mathbf{W}' = \\mathbf{A}(t)\\mathbf{W},\n\\]\nwith initial condition \\(\\mathbf{W}(0) = \\mathbf{I}\\). Any solution of \\(\\mathbf{u}' = \\mathbf{A}(t)\\mathbf{u}\\) is of the form \\(\\mathbf{u}(t) = \\mathbf{W}(t)\\mathbf{u}(0)\\). In particular, \\[\n\\mathbf{u}(T_0) = \\mathbf{W}(T_0)\\mathbf{u}(0).\n\\]\nWe call \\(\\mathbf{W}(T_0)\\) monodromy matrix for the limit cycle \\(\\Gamma\\).\n\nTheorem A.14 The monodromy matrix has eigenvalues \\(1, \\mu_1, \\ldots, \\mu_{n-1}\\), where \\(\\mu_i\\) are the multipliers of the Poincar√© map associated to the limit cycle \\(\\Gamma\\).\nThus, the size of the perturbation \\(\\mathbf{u}(t)\\), measured with \\(\\| \\mathbf{u}(t) \\|\\), will tend to zero if \\(|\\mu_i| &lt; 1\\). The unitary eigenvalue corresponds to the tangent direction to the limit cycle.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Introduction to ODEs</span>"
    ]
  },
  {
    "objectID": "bifurcations.html",
    "href": "bifurcations.html",
    "title": "Appendix B ‚Äî Introduction to bifurcations",
    "section": "",
    "text": "B.1 Background\nAs we have seen, dynamical system often depend on parameters. For instance, the spruce budworm model \\[\nu' = \\rho\\Bigl( 1 - \\frac{u}{q} \\Bigr) - \\frac{u^2}{1+u^2},\n\\]\ndepends on two parameters, \\(\\rho&gt;0\\) and \\(q&gt;0\\). The number of equilibria and their stability will depend on the parameters. However, most of the time a change of a parameter has little effect on the phase portrait: equilibria barely move, their stability is unchanged. In this case we say that the system is topologically stable. Otherwise, like for \\(\\rho=\\rho_1\\) or \\(\\rho = \\rho_2\\), we have a bifurcation.\nAnother example we have seen is the Gause-type prey-predator model: \\[\n\\left\\{\\begin{aligned}\nu' &= \\rho u(1-u) - \\frac{\\alpha\\delta u v}{1+\\delta u}, \\\\\nv' &= - v + \\frac{\\alpha\\delta u v}{1+\\delta u},\n\\end{aligned}\\right.\n\\]\nwhere we have 3 parameters. For \\(\\delta = \\frac{1}{\\alpha-1}\\) or \\(\\delta = \\frac{\\alpha+1}{\\alpha-1}\\) we also have bifurcations.\nIn general, a parametric dynamical system is an equation of the form \\[\n\\mathbf{y}' = \\mathbf{f}(\\mathbf{y},\\mathbf{p}),\n\\]\nwhere \\(\\mathbf{p}\\in\\mathbb{R}^m\\) is the set of parameters. For the spruce budworm equation, \\(\\mathbf{p} = (\\rho,q)\\). Here, we are only focusing on the case \\(m = 1\\), when \\(\\mathbf{p}\\) is a scalar. Bifurcations occuring in this case are of co-dimension 1.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Introduction to bifurcations</span>"
    ]
  },
  {
    "objectID": "bifurcations.html#structural-stability",
    "href": "bifurcations.html#structural-stability",
    "title": "Appendix B ‚Äî Introduction to bifurcations",
    "section": "B.2 Structural stability",
    "text": "B.2 Structural stability\nConsider the one-parameter ODE \\[\n\\mathbf{y}' = \\mathbf{f}(\\mathbf{y},p),\n\\]\nand suppose that the problem is well-posed for \\(p\\in\\mathcal{P}\\subseteq\\mathbb{R}\\) and \\(\\mathbf{f}\\).\nGiven \\(p=\\bar{p}\\), the system will have some phase portrait. Is it stable to small perturbations of the parameter \\(p\\)? In other words, if we consider a new system: \\[\n\\tilde{\\mathbf{y}}' = \\mathbf{f}(\\tilde{\\mathbf{y}},p),\n\\]\nwith \\(p \\in (\\bar{p}-\\varepsilon, \\bar{p}+\\varepsilon)\\), \\(\\varepsilon\\ll 1\\), is the ‚Äúerror‚Äù between \\(\\tilde{\\mathbf{y}}(t)\\) and \\(\\mathbf{y}(t)\\) going to zero as \\(\\varepsilon\\to 0\\) for any choice of the initial condition? That is, is the phase portrait of the perturbed system ‚Äúconverging‚Äù to the phase portrait of the original ODE? This is typically the case.\n\nTheorem B.1 (stability to perturbations) Suppose that \\(\\mathbf{f}(\\mathbf{y},p)\\) is continuous and locally Lipschitz in \\(\\mathbf{y}\\), uniformly in \\(p\\), and that the ODE is well-posed for any choice of \\(p\\) in the interval. Then the solution of the ODE \\(\\boldsymbol{\\phi}(t,\\mathbf{y}_0,p)\\to\\boldsymbol{\\phi}(t,\\mathbf{y}_0,\\bar{p})\\) uniformly as \\(p\\to \\bar{p}\\).\n\nThis theorem gives a zero-stability result, in the limit \\(p\\to \\bar{p}\\). How about the case of \\(p\\in(\\bar{p}-\\varepsilon, \\bar{p}+\\varepsilon)\\) but \\(\\varepsilon &gt; 0\\)? Is the perturbed ODE still ‚Äúclose‚Äù to the original one? We have the following definition\n\nDefinition B.1 (structural stability) The original ODE is structurally stable in some (closed) region of the phase space if and only if there exists \\(\\varepsilon &gt; 0\\) such that the perturbed system is topologically equivalent to the original ODE for all \\(p\\in (\\bar{p}-\\varepsilon, \\bar{p}+\\varepsilon)\\).\n\n\nDefinition B.2 (topological equivalence) Two ODEs are topologically equivalent if there exists a diffeomorphism that smoothly maps the phase portrait of one system into the other, preserving the direction of time.\n\nFor instance, the two phase portraits below are topologically equivalent, because we can smoothly deform one into the other. (Put a fork at the center, then twist like you would do with spaghetti.)\n\n\n\nTopological equivalence",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Introduction to bifurcations</span>"
    ]
  },
  {
    "objectID": "bifurcations.html#bifurcations",
    "href": "bifurcations.html#bifurcations",
    "title": "Appendix B ‚Äî Introduction to bifurcations",
    "section": "B.3 Bifurcations",
    "text": "B.3 Bifurcations\nOn the other hand, these below are not topologically equivalent, because if we shrink the limit cycle down to a point, we collide with another equilibrium at the center, making the map non-invertible.\n\n\n\nPhase portraits that are not topologically equivalent\n\n\nWhen we are a situation like the one above, we have a bifurcation.\n\nDefinition B.3 (bifurcation) If for some value \\(p=\\bar{p}\\) a system is not structurally stable, then for \\(p=\\bar{p}\\) we have a bifurcation.\n\nBifurcations result from the collision of invariant sets of the dynamical system, for instance 2 equilibria or 2 limit cycles colliding one into the other, for some value of the parameter. We can visualize this in a bifurcation plot, like the one below:\n\n\n\nbifurcation in 1D\n\n\nwhere we have on the abscissa the parameter, and on the ordinate the phase space. The curves are equilibria, and for \\(\\bar{p}\\) we have a bifurcation.\nThe bifurcation plot is just a ‚Äústack‚Äù of phase portraits for various values of \\(p\\). In some sense, a smooth transition between one layer to the other implies topological equivalence, otherwise we have bifurcations. For instance, for planar system we could have:\n\n\n\nbifurcation in 2D\n\n\nThe ‚Äúcone‚Äù is formed by limit cycles. Here, we have 3 bifurcations.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Introduction to bifurcations</span>"
    ]
  },
  {
    "objectID": "bifurcations.html#continuation-of-equilibria",
    "href": "bifurcations.html#continuation-of-equilibria",
    "title": "Appendix B ‚Äî Introduction to bifurcations",
    "section": "B.4 Continuation of equilibria",
    "text": "B.4 Continuation of equilibria\nConsider the dynamical system in 1D: \\[\ny' = f(y,p),\n\\]\nwith \\(p\\in\\mathbb{R}\\). An equilibrium \\(\\bar{y}\\) of the ODE for a given parameter \\(\\bar{p}\\) is a solution to the equation: \\[\n0 = f(\\bar{y},\\bar{p}).\n\\]\nSuppose that \\(f\\) is a smooth function in \\((y,p)\\). Thanks to the implicit function theorem, if \\(\\partial_p f(\\bar{y}, \\bar{p})\\neq 0\\) we can locally define a represent the curve \\(f(\\bar{y}, \\bar{p})=0\\) in the space \\((y,p)\\in\\mathbb{R}^2\\) with a function \\(y=\\phi(p)\\) such that \\(f(\\phi(p),p)=0\\) for \\(p\\in \\mathcal{B}_\\varepsilon(\\bar{p})\\), some neighborhood of \\(\\bar{p}\\).\nThe curve \\((\\phi(p),p)\\) represents the continuation of the equilibrium \\(\\bar{y}\\) as we change the parameter \\(p=\\bar{p}\\). All points along \\((\\phi(p),p)\\) are equilibria, by construction, for different values of the parameter \\(p\\).\nWe can extend the same construction in several dimensions, say \\(\\mathbf{y}\\in\\mathbb{R}^n\\) and \\(p\\in\\mathbb{R}\\). Let us by \\(\\mathbf{f}_\\mathbf{y}(\\mathbf{y},p)\\) the Jacobian of \\(\\mathbf{f}\\) with respect to \\(\\mathbf{y}\\). Suppose that \\(\\bar{\\mathbf{y}}\\) is an equilibrium for the ODE \\[\n\\mathbf{y}' = \\mathbf{f}(\\mathbf{y},\\bar{p}),\n\\]\nthat is, \\(\\mathbf{f}(\\bar{\\mathbf{y}},\\bar{p})=\\mathbf{0}\\), and that \\[\n\\det \\mathbf{f}_\\mathbf{y}(\\bar{\\mathbf{y}},\\bar{p}) \\neq 0,\n\\]\nthen there exists a function \\(\\mathbf{y}=\\boldsymbol{\\phi}(p)\\) such that \\(\\mathbf{f}(\\boldsymbol{\\phi}(p),p)=\\mathbf{0}\\) for \\(p\\in\\mathcal{B}_\\varepsilon(\\bar{p})\\).\nThe stability of the equilibria along the curve is also preserved: the eigenvalues of \\(\\mathbf{f}_\\mathbf{y}(\\mathbf{y},p)\\) depends continuously on \\(p\\) so the branch of equilibria \\(\\mathbf{y}=\\boldsymbol{\\phi}(p)\\) will inherit the same stability properties of \\(\\bar{\\mathbf{y}}\\).\n\n\n\nCurve of equilibria\n\n\nWhat if an eigenvalue of \\(\\mathbf{f}_\\mathbf{y}(\\boldsymbol{\\phi}(p),p)\\) will end up with zero real part along the curve?\nThen we have a bifurcation. We still denote this point with \\(\\bar{p}\\) and the corresponding equilibrium with \\(\\bar{\\mathbf{y}}\\). We have different options:\n\nTangent bifurcation. \\(\\mathbf{f}_\\mathbf{y}(\\bar{\\mathbf{y}},\\bar{p})\\) as eigenvalue 0 and \\(\\mathbf{f}_p(\\bar{\\mathbf{y}},\\bar{p}) \\neq 0\\).\nTranscritical bifurcation. \\(\\mathbf{f}_\\mathbf{y}(\\bar{\\mathbf{y}},\\bar{p})\\) as eigenvalue 0 and \\(\\mathbf{f}_p(\\bar{\\mathbf{y}},\\bar{p}) = 0\\).\nHopf bifurcation. \\(\\mathbf{f}_\\mathbf{y}(\\bar{\\mathbf{y}},\\bar{p})\\) as eigenvalue \\(\\pm i\\omega\\) (only \\(n\\ge 2\\)).\n\nNote that if one real eigenvalue is zero, then we cannot apply the implicit function theorem, and the curve \\(\\boldsymbol{\\phi}\\) cannot be defined at \\(p=\\bar{p}\\) (this is the case of the first 2 bifurcations.)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Introduction to bifurcations</span>"
    ]
  },
  {
    "objectID": "bifurcations.html#tangent-bifurcation",
    "href": "bifurcations.html#tangent-bifurcation",
    "title": "Appendix B ‚Äî Introduction to bifurcations",
    "section": "B.5 Tangent bifurcation",
    "text": "B.5 Tangent bifurcation\nThe simplest bifurcation is perhaps the tangent bifurcation. Consider the following ODE: \\[\ny' = f(y,p) = p - y^2,\n\\]\nfor \\(p\\in\\mathbb{R}\\). We have the following situation:\n\nIf \\(p&gt;0\\) there are 2 equilibria, \\(\\bar{y}^{\\pm} = \\pm\\sqrt{p}\\). Since \\(\\partial_y f = -2y\\), we have that \\(\\bar{y}^-\\) is unstable and \\(\\bar{y}^+\\) is asympotically stable.\nIf \\(p&lt;0\\) there is no real equilibrium.\nIf \\(p=\\bar{p}=0\\) we have a single equilibrium \\(\\bar{y}=0\\). The stability cannot be deduced from the linearization (why?), but from the sign of \\(f\\) we see that \\(\\bar{y}\\) is attractive for \\(y&gt;0\\) and repulsive for \\(y&lt;0\\). It is called saddle-node.\n\nSince \\(\\partial_y f(\\bar{y},\\bar{p})=0\\) but \\(\\partial_p f(\\bar{y},\\bar{p}) = 1 \\neq 0\\), for \\(p=\\bar{p}=0\\) we have a tangent bifurcation. Tangent bifurcations are also called limit points (in MatCont denoted by LP). The reason for the name should be clear from the bifurcation plot.\n\n\n\ntangent bifurcation\n\n\nTangent bifurcations are catastrophic, in the sense that a point at equilibrium right before the bifurcation will diverge from its state after the bifurcation (imagine to move a point along the stable branch from right to left.) Note that the curve is smooth, because \\(\\partial_p f(\\bar{y},\\bar{p})\\neq 0\\). Check the next figure, where we visualize \\(f(y,p)\\) around the bifurcation (in black the zero levelset, that is the curve of equilibria):\n\n\n\nimage",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Introduction to bifurcations</span>"
    ]
  },
  {
    "objectID": "bifurcations.html#transcritical-bifurcation",
    "href": "bifurcations.html#transcritical-bifurcation",
    "title": "Appendix B ‚Äî Introduction to bifurcations",
    "section": "B.6 Transcritical bifurcation",
    "text": "B.6 Transcritical bifurcation\nThe transcritical bifurcation occurs when an equilibrium has 0 real part and also \\(\\partial_p f(\\bar{y},\\bar{p})=0\\). The normal form is: \\[\ny' = py - y^2.\n\\]\nThe derivative of \\(f\\) is \\(f_y(y,p) = p-2y\\). We have 2 equilibria,\n\n\\(y_0=0\\), stable for \\(p&gt;0\\) and unstable for \\(p&lt;0\\).\n\\(\\bar{y}=p\\), unstable for \\(p&gt;0\\) and stable for \\(p&lt;0\\).\n\nFor \\(p=0\\) we only have one equilibrium \\(\\bar{y}=0\\), which is a saddle-node because it is attractive for \\(y&lt;0\\) and repulsive for \\(y&gt;0\\).\n\n\n\nimage\n\n\nTranscritical bifurcations are very common in biological modeling, because some equilibria (like the origin) do not depend on the parameters and are always present. Therefore, in the bifurcation plot their curve do not change, and can be intersected by other curves of equilibria. In fact, if \\(f(0,p)=0\\) for all \\(p\\), then also \\(f_p(0,p)=0\\). Note that the above model is the logistic equation.\nFrom the 3d visualization of \\(f\\):\n\n\n\nimage\n\n\nwe see a critical point at the bifurcation point. The eigenvectors give the tangents to the 2 curves of equilibria. In this way, it is possible to switch curve at the bifurcation point.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Introduction to bifurcations</span>"
    ]
  },
  {
    "objectID": "bifurcations.html#hopf-bifurcation",
    "href": "bifurcations.html#hopf-bifurcation",
    "title": "Appendix B ‚Äî Introduction to bifurcations",
    "section": "B.7 Hopf bifurcation",
    "text": "B.7 Hopf bifurcation\nThe Hopf bifurcation occurs when an equilibrium changes stability and a limit cycle appears. The normal form is: \\[\n\\left\\{ \\begin{aligned}\nx' &= \\sigma x + \\omega y + c x (x^2 + y^2), \\\\\ny' &= \\sigma y - \\omega x + c y (x^2 + y^2), \\\\\n\\end{aligned} \\right.\n\\]\nSuppose that \\(\\sigma\\), \\(\\mu\\), and \\(c\\) depend on some parameter \\(p\\in\\mathbb{R}\\).\nWe have that \\((x,y)=(0,0)\\) is an equilibrium for all choices of \\(p\\). The Jacobian at \\((0,0)\\) has eigenvalues \\[\n\\lambda^\\pm = \\sigma \\pm i \\omega,\n\\]\nWhen \\(\\sigma(p) = 0\\), the real part of the complex conjugate eigenvalues becomes zero: we have a Hopf bifurcation. To check the existence of a limit cycle, we consider the system in polar coordinates: \\[\n\\left\\{ \\begin{aligned}\nx &= r \\cos\\theta, \\\\\ny &= r \\sin\\theta,\n\\end{aligned} \\right.\n\\quad\\Rightarrow\\quad\n\\left\\{ \\begin{aligned}\nr &= \\sqrt{x^2 + y^2}, \\\\\n\\theta &= \\arctan (y/x).\n\\end{aligned} \\right.\n\\]\nTo change the variables, simply note that: \\[\n\\begin{split}\nr\\mathrm{d}r &= \\tfrac{1}{2}\\mathrm{d}(r^2) = x\\mathrm{d}x + y\\mathrm{d}y \\\\\n&= \\bigl(\\sigma (x^2 + y^2) + c (x^2 + y^2)(x^2 + y^2) \\bigr)\\mathrm{d}t = \\\\\n&= \\bigl(\\sigma r^2 + c r^4 \\bigr)\\mathrm{d}t,\n\\end{split}\n\\]\nand \\[\n\\begin{split}\n\\mathrm{d}\\theta &= \\frac{\\mathrm{d}y/x - y\\mathrm{d}x/x^2}{1 + (y/x)^2} = \\frac{x\\mathrm{d}y - y\\mathrm{d}x}{x^2 + y^2} \\\\\n&= \\frac{x(\\sigma y - \\omega x + cy(x^2+y^2) - y(\\sigma x + \\omega y + cx(x^2+y^2)}{x^2 + y^2}\\mathrm{d}t \\\\\n&= -\\frac{\\omega (x^2 + y^2)}{x^2 + y^2}\\mathrm{d}t = -\\omega\\mathrm{d}t.\n\\end{split}\n\\]\nThe system in polar coordinates is: \\[\n\\left\\{ \\begin{aligned}\nr' &= \\sigma r + c r^3, \\\\\n\\theta' &= -\\omega. \\\\\n\\end{aligned} \\right.\n\\]\nAssuming \\(\\omega(p)\\neq 0\\), we have two equilibria:\n\n\\(r=0\\), which corresponds to the origin and we know it is stable for \\(\\sigma(p)&lt;0\\) and unstable for \\(\\sigma(p)&gt;0\\).\n\\(r=\\bar{r}=\\sqrt{-\\sigma/c}\\), which exists only for \\(c(p)\\neq 0\\) and \\(\\sigma(p)c(p)&lt;0\\). When there exists, it is stable if \\(\\sigma(p)&gt;0\\) and unstable if \\(\\sigma(p)&lt;0\\). This corresponds to a limit cycle of radius \\(\\bar{r}\\).\n\nIn conclusion, we have 2 types of Hopf bifurcations.\n\nSupercritical Hopf bifurcation \\(H^S\\): \\(\\sigma'(\\bar{p})&gt;0\\) and \\(c(\\bar{p})&lt;0\\). The stable equilibrium at the origin becomes unstable and a stable limit cycle is born. The bifurcation is not catastrophical.\n\n\n\n\nimage\n\n\n\nSubrcritical Hopf bifurcation \\(H_S\\): \\(\\sigma'(\\bar{p})&lt;0\\) and \\(c(\\bar{p})&gt;0\\). The unstable equilibrium at the origin becomes stable and an unstable limit cycle is born. The bifurcation is catastrophical.\n\n\n\n\nimage\n\n\nNote that for \\(c=0\\) at the bifurcation point we have an infinite number of limit cycles (non-isolated periodic orbit), like the classical oscillator. This is a degenerate Hopf bifurcation so the condition \\(c(\\bar{p})\\neq 0\\) is called non-degeneracy condition. The condition \\(\\sigma'(\\bar{p})\\neq 0\\) is called trasversality condition.\nFor a general system of ODEs: \\[\n\\left\\{ \\begin{aligned}\nx' &= f(x,y,p), \\\\\ny' &= g(x,y,p), \\\\\n\\end{aligned} \\right.\n\\]\nsuch that \\((\\bar{x},\\bar{y})=(0,0)\\) is an equilibrium (with no loss of generality we can take the equilibrium at the origin), thanks to linearization we can always write the system around the equilibrium in the form: \\[\n\\left\\{ \\begin{aligned}\nx' &= \\sigma(p) x + \\omega(p) y + \\tilde{f}(x,y,p), \\\\\ny' &= \\sigma(p) y - \\omega(p) x + \\tilde{g}(x,y,p), \\\\\n\\end{aligned} \\right.\n\\]\nwith \\(\\tilde{f}(0,0,p)=\\tilde{g}(0,0,p)=0\\). It is possible to further expand \\(\\tilde{f}\\) and \\(\\tilde{g}\\) so to find a term of the form \\(cr^3\\) as above. The coefficient \\(c\\) is called first Lyapunov exponent and it reads: \\[\n\\begin{split}\n16 c &= f_{xxx} + f_{xyy} + g_{xxy} + g_{yyy} \\\\\n&+ \\frac{1}{\\omega} \\Bigl( f_{xy}(f_{xx}+f_{yy}) - g_{xy}(g_{xx}+g_{yy}) - f_{xx}g_{xx} - f_{yy}g_{yy} \\Bigr).\n\\end{split}\n\\]\nThen we have the following:\n\nTheorem B.2 (Hopf) Suppose that \\(\\sigma'(p)\\neq 0\\) (trasversality) and \\(c\\neq 0\\) (non-degeneracy). Then there exists a branch \\(\\Gamma_p\\) of periodic solutions with period \\(T(p)\\) for \\(p\\) such that \\(|p-\\bar{p}|\\) is small and \\(p &gt; \\bar{p}\\) if \\(\\sigma'(\\bar{p})c &lt; 0\\) (resp. \\(p &lt; \\bar{p}\\) if \\(\\sigma'(\\bar{p})c &gt; 0\\)). Furthermore, \\(\\Gamma_p \\to \\bar{y}\\) for \\(p\\to\\bar{p}\\) and \\(T(p)\\to \\frac{2\\pi}{\\omega(\\bar{p})}\\). If \\(c&lt;0\\) \\(\\Gamma_p\\) are attracting; if \\(c&gt;0\\) \\(\\Gamma_p\\) are repelling.\n\nWe observed the Hopf bifurcation in Gause-type prey-predator models and in negative feedback networks.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Introduction to bifurcations</span>"
    ]
  },
  {
    "objectID": "bifurcations.html#limit-cycle-tangent-bifurcation",
    "href": "bifurcations.html#limit-cycle-tangent-bifurcation",
    "title": "Appendix B ‚Äî Introduction to bifurcations",
    "section": "B.8 Limit cycle tangent bifurcation",
    "text": "B.8 Limit cycle tangent bifurcation\nConsider the following model: \\[\n\\left\\{ \\begin{aligned}\nr' &= r\\sigma\\bigl(1 + c r^2 - r^4 \\bigr), \\\\\n\\theta' &= -\\omega. \\\\\n\\end{aligned} \\right.\n\\]\nWe have 3 equilibria (note that necessarily \\(r\\ge0\\)): the origin \\(r=0\\) and: \\[\n\\bar{r} = \\sqrt{\\frac{c \\pm \\sqrt{c^2+4\\sigma}}{2}}.\n\\]\nIf we study the existence of limit cycles (LCs) in the \\((\\sigma,c)\\) plane we have:\n\n\n\nimage\n\n\nwhere:\n\n\\(H^S\\): supercritical Hopf bifurcation,\n\\(H_S\\): subcritical Hopf bifurcation,\n\\(T\\): tangent bifurcation between limit cycles.\n\nIn terms of stability we have:\n\n\n\nimage\n\n\nAs we cross \\(T\\) we have 2 limit cycles colliding and disappearing, exactly like the tangent bifurcation between equilibria. (In fact, this is what happens in polar coordinates.)\nThe tangent bifurcation can be visualized as follows:\n\n\n\nimage\n\n\nWe have seen this type of bifurcation in a negative feedback loop. It is catastrophic.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Introduction to bifurcations</span>"
    ]
  },
  {
    "objectID": "Laboratories/00 Intro Matlab/introMATLAB.html",
    "href": "Laboratories/00 Intro Matlab/introMATLAB.html",
    "title": "Appendix C ‚Äî Introduction to MATLAB",
    "section": "",
    "text": "C.1 Overview\nMATLAB is a software for scientific computing. It is a self-contained environment, with thousands of optimized functions for many tasks. MATLAB is a proprietary software, so you need a license to use it. Most universities offer an educational license (see this website for UniTrento.)\nThere are free alternatives too:\nYou are free to use any Matlab alternative, and rely on it only for MatCont.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>Introduction to MATLAB</span>"
    ]
  },
  {
    "objectID": "Laboratories/00 Intro Matlab/introMATLAB.html#overview",
    "href": "Laboratories/00 Intro Matlab/introMATLAB.html#overview",
    "title": "Appendix C ‚Äî Introduction to MATLAB",
    "section": "",
    "text": "GNU Octave, an open-source software aiming at 1:1 compatibility with Matlab. You can use it, but beware that some MATLAB packages may not work (e.g., MatCont).\nPython, probably the most popular programming language, has many libraries for scientific computing, such as numpy, scipy, matplotlib, jax, and many others. It would be the best alternative to Matlab, if it weren‚Äôt for MatCont, that we will extensively use.\nJulia, it is a python competitor, with a similar syntax but much better performance. Many libraries have a Julia wrapper, but again not MatCont.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>Introduction to MATLAB</span>"
    ]
  },
  {
    "objectID": "Laboratories/00 Intro Matlab/introMATLAB.html#installation",
    "href": "Laboratories/00 Intro Matlab/introMATLAB.html#installation",
    "title": "Appendix C ‚Äî Introduction to MATLAB",
    "section": "C.2 Installation",
    "text": "C.2 Installation\nMatlab installation is straightforward on most OSes. Please follow the instruction on the website. We do not need special toolboxes. In case, you can install them later.\nYou can also use the online version of MATLAB. It comes with an online storage called MATLAB Drive that you can use to sync your files.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>Introduction to MATLAB</span>"
    ]
  },
  {
    "objectID": "Laboratories/00 Intro Matlab/introMATLAB.html#quick-tour",
    "href": "Laboratories/00 Intro Matlab/introMATLAB.html#quick-tour",
    "title": "Appendix C ‚Äî Introduction to MATLAB",
    "section": "C.3 Quick tour",
    "text": "C.3 Quick tour\nOnce open, Matlab will look like this:\n\n\n\nThe MATLAB interface\n\n\nYou can notice different panels:\n\nCommand Window is where we type commands.\nCurrent Folder is the current working directory.\nWorkspace is a summary of all variables.\n\nThe system is waiting for an input &gt;&gt;.\n\nC.3.1 Basics\nMATLAB stands for MAtrix LABoratory, so its philosophy is to store everything numeric as a matrix. (This is not entirely true anymore, recent versions of Matlab introduced many new types.) A scalar is a \\(1\\times 1\\) matrix. Matlab a weakly typed (and interpreted) programming language. A variable is a generic, and no type is needed like in C:\n&gt;&gt; a = 2.45\na =\n    2.4500\nAs we inout a = 2.45, the system responds with its representation. Note that this is not the internal precision! All numbers with a period are stored as double. To suppress the output, just put a semicolon:\n&gt;&gt; a = 2.45;\nWe can execute more commands per line (note the comma):\n&gt;&gt; a = 2.45 , b = 3.1; A = 1.2;\n\n\n\n\n\n\nWarning\n\n\n\nMatlab is case-sensitive: the variable a and A are different. This is the standard behavior also for file names in POSIX systems like UNIX, but not for Windows. Also on macOS you need to be careful: by default the file system is case-insensitive like Windows.\n\n\nIf a number is assigned to nothing, it will go to the variable ans:\n&gt;&gt; 2.45;\n&gt;&gt; whos\n  Name      Size            Bytes  Class     Attributes\n  ans       1x1                 8  double              \n&gt;&gt; ans\nans =\n    2.4500\nSome variables are predefined, like pi, \\(\\pi\\), and 1i or 1j, the imaginary unit \\(i\\). Note that also i and j are valid for Matlab, but dangerous: i is often an iteration variable in a for-loop:\n&gt;&gt; a = 5 + 2*i\na =\n   5.0000 + 2.0000i\n\n&gt;&gt; i = 2;\n&gt;&gt; b = 5 + 2*i\nb =\n     9\n\n&gt;&gt; c = 5 + 2*1i\nc =\n   5.0000 + 2.0000i\nYou can clear a variable by using clear myvar, or all variables with clear. The command clc clears the screen.\nIf you are not sure of a command, use tab-completion: type the beginning of a command, then type ‚ÄúTAB‚Äù. You can also use help and doc.\n\n\nC.3.2 Output format\nBy default, numbers are visualized in short format. The command format can change the behavior:\n&gt;&gt; format long\n&gt;&gt; 1/7\nans =\n   0.142857142857143\nBelow in the table different options:\n\n\n\nFormat\nans\n\n\n\n\nformat rat\n1/7\n\n\nformat short\n0.1429\n\n\nformat short e\n1.4286e-01\n\n\nformat short g\n0.14286\n\n\nformat long\n0.142857142857143\n\n\nformat long e\n1.428571428571428e-01\n\n\nformat long g\n0.142857142857143\n\n\n\n\n\nC.3.3 Vector and matrices\nWe have many ways to generate a vector in Matlab. The most direct is:\n&gt;&gt; b = [1 5 6 7];\n&gt;&gt; c = [1, 5, 6, 7];\nThe two vectors are identical: the comma is optional. The dimension is \\(1\\times 4\\), that is 1 row and 4 columns. For column vector, we need to use the semi-colon (now mandatory):\n&gt;&gt; d = [1; 5; 6; 7];\n&gt;&gt; whos\n  Name      Size            Bytes  Class     Attributes\n  b         1x4                32  double              \n  c         1x4                32  double              \n  d         4x1                32  double              \nNote that the memory requirement is 32 bytes, that it \\(4\\cdot 8\\) bytes, where 8 bytes is a 64-bit floating-point number (double).\nWe can generate a vector also as follows:\n&lt;start&gt; : &lt;increment&gt; : &lt;end&gt;\nIf we omit the increment, it is assumed one:\n&gt;&gt; 1:8\nans =\n     1     2     3     4     5     6     7     8\n\n&gt;&gt; 1:2:8\nans =\n     1     3     5     7\n\n&gt;&gt; 0.5:-0.1:0\nans =\n    0.5000    0.4000    0.3000    0.2000    0.1000         0\nThe command above is quite powerful: it also compensates for rounding off errors. Alternatively, we can use linspace:\n&gt;&gt; linspace(0,1,5)\nans =\n         0    0.2500    0.5000    0.7500    1.0000\nMatrices easily follows:\n&gt;&gt; A = [ 1 2 3 4; 5 6 7 8 ; 9 10 11 12; 13 14 15 16 ]\nA =\n     1     2     3     4\n     5     6     7     8\n     9    10    11    12\n    13    14    15    16\nwhich is returning a \\(4\\times 4\\) matrix. More complex matrices can be created with ad-hoc functions like eye, ones, zeros, reshape, and so on.\n&gt;&gt; reshape(A, 2, 8)\nans =\n\n     1     9     2    10     3    11     4    12\n     5    13     6    14     7    15     8    16\n\n&gt;&gt; B = [ 1:3 0; 4:-1:1 ]\nB =\n     1     2     3     0\n     4     3     2     1\n\n&gt;&gt; eye(2,3)\nans =\n     1     0     0\n     0     1     0\n\n&gt;&gt; ones(2,2)\nans =\n     1     1\n     1     1\n\n\n\n\n\n\nExercise\n\n\n\nTry to create the following matrix:\n\n\n\nimage\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n&gt;&gt; n = 3;\n&gt;&gt; A = [ones(n), zeros(n); ...\n         eye(n), [1:n; n*ones(n-1,n) ];\n\n\n\n\n\n\n\nC.3.4 Matrix manipulation\nWe can access and modify matrices quite naturally:\n&gt;&gt; v = linspace(0, 1, 5);\n&gt;&gt; v(2)\nans =\n    0.2500\nThe operator ( ) is much more poweful: we can use slices:\n&gt;&gt; v([1 3 5])\nans =\n         0    0.5000    1.0000\n\n&gt;&gt; v(1:3)\nans =\n         0    0.2500    0.5000\n\n&gt;&gt; v(2:end)\nans =\n    0.2500    0.5000    0.7500    1.0000\n\n&gt;&gt; v([1 2; 3 2])\nans =\n         0    0.2500\n    0.5000    0.2500\nWe also can use vectors or matrices of indices. For instance, if indices are [1 3 5], the output is [v(1) v(3) v(5)]. With matrices is similar:\n&gt;&gt; A = [ 1 2 3 ; 4 5 6 ; 7 8 9]\nA =\n     1     2     3\n     4     5     6\n     7     8     9\n\n&gt;&gt; A(1,1)\nans =\n     1\n\n&gt;&gt; A(1:3,1:2)\nans =\n     1     2\n     4     5\n     7     8\n\n&gt;&gt; A(1:end-1,2:end)\nans =\n     2     3\n     5     6\n\n&gt;&gt; A([1 2],[2 3])\nans =\n     2     3\n     5     6\nIn the last case we extract the first and the second row, and then crossed with the second and third columns.\n&gt;&gt; A(1:end)\nans =\n     1     4     7     2     5     8     3     6     9\n\n&gt;&gt; A([2 3; 9 6])\nans =\n     4     7\n     9     8\nThe operation A(1:end) is equivalent to A(:), except for returning a column vector instead of a row vector.\n\n\nC.3.5 Operations on Matrices\nLet‚Äôs now discuss algebraic operations on matrices. The basic rules are as follows, for two matrices \\(\\mathbf{A} \\in \\mathbb{C}^{n\\times n}\\) and \\(\\mathbf{B}\\in\\mathbb{C}^{p\\times q}\\):\n\nAddition is defined if and only if \\(n = p\\) and \\(m = q\\), and \\(\\mathbf{A}+\\mathbf{B} \\in \\mathbb{C}^{n \\times m}\\).\nMultiplication is defined if and only if \\(m = p\\), and \\(\\mathbf{A}\\mathbf{B} \\in \\mathbb{C}^{n \\times q}\\).\nTranspose \\(\\mathbf{A}^T \\in \\mathbb{C}^{m \\times n}\\), with \\((\\mathbf{A}^T)_{ij} = (\\mathbf{A})_{ji}\\).\nConjugate transpose \\(\\mathbf{A}^H \\in \\mathbb{C}^{m \\times n}\\), with \\((\\mathbf{A}^H)_{ij} = \\overline{(\\mathbf{A})_{ji}}\\).\n\nThe last two operations can be performed as follows:\n&gt;&gt; A = [ 1+1i, 3; 1, 1-2i ];\n&gt;&gt; A.'\nans =\n   1.0000 + 1.0000i   1.0000          \n   3.0000             1.0000 - 2.0000i\n&gt;&gt; A'\nans =\n   1.0000 - 1.0000i   1.0000          \n   3.0000             1.0000 + 2.0000i\nIf the matrix is real, these two operations are the same. In fact, it is common to use ' as the transpose of a matrix:\n&gt;&gt; a = [ 1 2 5 ];\n&gt;&gt; b = [ 4 6 4 ];\n&gt;&gt; H = [ 1 2 3 ; 2 4 7 ; 1 4 3 ];\n&gt;&gt; G = [ 4 6 2 ; 8 4 1 ; 3 2 9 ];\n&gt;&gt; a + b\nans =\n     5     8     9\n&gt;&gt; a - b\nans =\n    -3    -4     1\n&gt;&gt; H + G\nans =\n     5     8     5\n    10     8     8\n     4     6    12\n&gt;&gt; H * G\nans =\n    29    20    31\n    61    42    71\n    45    28    33\n&gt;&gt; G * a\nError using *\nInner matrix dimensions must agree.\nThis is the most common error. In this case, the vector a is \\(1 \\times 3\\), while H is \\(3 \\times 3\\), which is not valid.\n&gt;&gt; G * a'\nans =\n    26\n    21\n    52\n&gt;&gt; a * G\nans =\n    35    24    49\n&gt;&gt; 2 * a\nans =\n     2     4    10\n&gt;&gt; a' * b\nans =\n     4     6     4\n     8    12     8\n    20    30    20\n&gt;&gt; a * b'\nans =\n    36\n&gt;&gt; A^2\nans =\n     8    22    26\n    17    48    55\n    12    30    40\n&gt;&gt; a^2\nError using ^\nInputs must be a scalar and a square matrix.\nTo compute elementwise POWER, use POWER (.^) instead.\nThe last message tells us that exponentiation is intended as matrix multiplication and is only defined for square matrices. It suggests using the dot version instead. In fact, scalar binary operations like multiplication, division, and exponentiation become element-wise operations for vectors and matrices when you put a dot in front of the operation symbol:\n&gt;&gt; a .* b\nans =\n     4    12    20\n&gt;&gt; a ./ b\nans =\n    0.2500    0.3333    1.2500\n&gt;&gt; a .^ 2\nans =\n     1     4    25\n&gt;&gt; a .^ b\nans =\n     1    64   625\nThe only thing to keep in mind is that the two operands should have the same dimensions, or at least one of them should be a scalar (in which case it is repeated).\n\n\n\n\n\n\nExercise\n\n\n\nTry to create the following vector, for arbitrary \\(n\\): \\[\n\\Bigl[\\:\n\\overbrace{\n\\underbrace{1, 1, \\ldots, 1}_n,\n\\underbrace{2, 2, \\ldots, 2}_n,\n\\ldots,\n\\underbrace{n, n, \\ldots, n}_n}^n\n\\:\\Bigr].\n\\]\n\n\n\n\n\n\nSolution\n\n\n\n\n\n&gt;&gt; n = 5;\n&gt;&gt; v = reshape(ones(n,1)*(1:n),1,[])\n\n\n\n\n\n\n\nC.3.6 Elementary Mathematical Functions\nOne of the strengths of MATLAB is the vast number of mathematical functions (both elementary and advanced) it offers. Unless specified otherwise, almost all of these functions operate element-wise. For example, given a matrix A, exp(A) is not the matrix exponential (which is calculated with expm(A)), but rather a matrix whose elements are the exponentials of the original elements. Here are some elementary mathematical functions:\n\nabs(A): Absolute value of the elements of A\nsqrt(A): Square root of the elements of A\nexp(A): Exponential function applied to the elements of A\nlog(A): Natural logarithm of the elements of A\nlog10(A): Base 10 logarithm of the elements of A\nlog2(A): Base 2 logarithm of the elements of A\nsin(A): Sine of the elements of A\ncos(A): Cosine of the elements of A\ntan(A): Tangent of the elements of A\nasin(A): Arcsine of the elements of A (in radians)\nacos(A): Arccosine of the elements of A (in radians)\natan(A): Arctangent of the elements of A (in radians)\nsinh(A): Hyperbolic sine of the elements of A\ncosh(A): Hyperbolic cosine of the elements of A\ntanh(A): Hyperbolic tangent of the elements of A\n\n\n\n\n\n\n\nExercise\n\n\n\n\nCompute \\(\\dfrac{e^5 + \\sin(\\pi)}{\\sqrt{\\log_2 30}-10}\\) and \\(e^{\\log_{10}50} + e^{\\log 30} + e^{\\log_2 40}\\).\nDefine \\(\\mathbf{x} = [1, 3, 4]\\) and \\(\\mathbf{y} = [1, 1, 2]\\). Compute \\(2x_i \\log_2(|y_i|+1) - y_i\\log_{10}(x_i+2)\\) and \\(\\arctan\\left( \\dfrac{x_i}{y_i}\\right) - \\sin^2\\left( x_i\\sqrt[3]{|y_i|^2} \\right)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n&gt;&gt; % Point 1\n&gt;&gt; (exp(5) + sin(pi))/(sqrt(log2(30))-10)\n&gt;&gt; exp(log10(50)) + exp(log(30)) + exp(log2(40))\n&gt;&gt; % Point 2\n&gt;&gt; x = [ 1 3 4 ];\n&gt;&gt; y = [ 1 1 2 ];\n&gt;&gt; 2*x.*log2(abs(y)+1) - y.*log10(x+2)\n&gt;&gt; atan(x./y) - sin(x.*abs(y).^(2/3)).^2\n\n\n\n\n\n\n\nC.3.7 Elementary Mathematical Functions (Continued)\nLet A, B be two matrices, and b be a vector.\n\nsize(A): Returns a two-element vector, where the first element is the number of rows in A, and the second element is the number of columns.\nsize(A, 1): Returns the first element of size(A), which is the number of rows.\nsize(A, 2): Returns the second element of size(A), which is the number of columns.\nlength(b): Returns the number of elements in the vector b.\nmax(b): Returns the largest element in the vector b.\nmin(b): Returns the smallest element in the vector b.\nmax(A): Returns a row vector containing the maximum element of each column of A.\nmin(A): Returns a row vector containing the minimum element of each column of A.\nmax(A, B): Returns a matrix of the same dimensions as A and B, containing the element-wise maximum.\nmin(A, B): Returns a matrix of the same dimensions as A and B, containing the element-wise minimum.\nmax(A, [], 2): Returns a column vector containing the maximum element of each row of A. If you replace 2 with 1, you get max(A).\nmin(A, [], 2): Returns a column vector containing the minimum element of each row of A. If you replace 2 with 1, you get min(A).\nsum(b): Returns a scalar equal to the sum of the elements in the vector b.\nsum(A): Returns a row vector whose elements are the column-wise sum of the elements in matrix A.\nsum(A, 2): Returns a column vector whose elements are the row-wise sum of the elements in matrix A.\ndiag(A): Returns the vector of the diagonal elements of matrix A.\ndiag(A, k): Returns the vector of the \\(k\\)-th super-diagonal of matrix A.\ndiag(A, -k): Returns the vector of the \\(k\\)-th sub-diagonal of matrix A.\ndiag(b): Returns a matrix with the elements of vector b on its diagonal.\ndiag(b, k): Returns a matrix with the elements of vector b on its \\(k\\)-th super-diagonal.\ndiag(b, -k): Returns a matrix with the elements of vector b on its \\(k\\)-th sub-diagonal.\ntril(A): Returns the lower triangular part of matrix A, making all elements strictly above the diagonal zero (even in a rectangular matrix).\ntriu(A): Returns the upper triangular part of matrix A, making all elements strictly below the diagonal zero (even in a rectangular matrix).\n\nThere are many other important functions such as det (determinant of a square matrix), trace (trace of a matrix), norm (for calculating norms), and so on. We will introduce these functions as needed.\n\n\n\n\n\n\nExercise\n\n\n\nTry to use the function magic(n). Then sum by row, column, diagonal, etc. What do you observe?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n&gt;&gt; n = 4;\n&gt;&gt; A = magic(n);\n&gt;&gt; sum(A, 2)\n&gt;&gt; sum(A, 1)\n&gt;&gt; sum(diag(A))\nWe notice that the sum is always \\(34\\), at least for \\(n=4\\). In general, the magic square contains numbers from \\(1\\) to \\(n^2\\), and the magic number is \\(\\frac{1}{2}n(n^2+1)\\). This is true even if we sum over the antidiagonal\n&gt;&gt; sum(diag(fliplr(A)))\nThe function fliplr (flip left‚Äìright) inverts the right with the left in a given matrix.\n\n\n\n\n\n\n\nC.3.8 Functions and Scripts\nMany of the commands we have introduced are defined as functions, which are procedures that take input data and provide output. In MATLAB, there are several ways to define a function.\nSuppose we want to create a function fun(x) that returns the value of \\(f(x) = x\\sin(x) + \\cos^2(x)\\) for a given \\(x\\). The ‚Äúlegacy‚Äù method involves defining a string corresponding to the function and then evaluating it using the eval command:\n&gt;&gt; fun = 'x*sin(x) + (cos(x))^2';\n&gt;&gt; x = 1.0;\n&gt;&gt; eval(fun)\nans =\n    1.1334\nHowever, there is an issue when x is a vector:\n&gt;&gt; x = [ 1 2 3 ];\n&gt;&gt; eval(fun)\nError using *\nInner matrix dimensions must agree.\nTo handle vector input correctly, we need to ‚Äúvectorize‚Äù the expression by using element-wise operations:\n&gt;&gt; fun = 'x.*sin(x) + cos(x).^2';\n&gt;&gt; x = [ 1 2 3 ];\n&gt;&gt; eval(fun)\nans =\n    1.1334    1.9918    1.4034\nAlternatively, you can use the vectorize command to transform the function into its vectorized version.\nHowever, a more convenient way is to define functions using ‚Äúanonymous functions‚Äù:\n&gt;&gt; fun = @(x) x.*sin(x) + cos(x).^2;\n&gt;&gt; x = [ 1 2 3 ];\n&gt;&gt; fun(x)\nans =\n    1.1334    1.9918    1.4034\nThe @ symbol is called a ‚Äúfunction handle.‚Äù Note that the vectorize command does not work with anonymous functions.\nThere are at least two other methods for defining functions, such as using inline and the Symbolic Math Toolbox. We won‚Äôt delve into these methods here.\nInstead, let‚Äôs explore using scripts to define functions. An ‚Äúscript‚Äù is a text file with a .m extension containing a list of commands that MATLAB will execute when the file is called. You can edit the script using any text editor or with MATLAB‚Äôs built-in editor by running the following command:\n&gt;&gt; edit\nSuppose, for example, that the file myscript.m contains the following:\nclear; clc;\n% This is a comment:\n% sum of squares of the diagonal\n% of the magic matrix\nn = 4;\nA = magic(n);\nsumma = sum(diag(A).^2)\nIf the file is in the working directory, you can run it as follows:\n&gt;&gt; myscript\nsumma =\n   414\nThe script is executed as if you had entered the commands directly in the terminal.\nYou can also have scripts that contain only a function with the following syntax:\nfunction y = myfun(x)\n  y = x.*sin(x) + (cos(x)).^2;\nend\nIn this case, you must save the file with the same name as the function (e.g., myfun.m).\nNow you can use the function:\n&gt;&gt; x = [ 1 2 3 ];\n&gt;&gt; myfun(x)\nans =\n    1.1334    1.9918    1.4034\nFunction scripts can have multiple arguments and return multiple outputs:\nfunction [A, b] = vecmat(n, m)\n  % Generate a matrix A of size m x n and\n  % a vector b of size n x 1 with random values.\n  A = rand(n, m);\n  b = rand(n, 1);\nend\nYou can call this function as follows:\n&gt;&gt; [K, f] = vecmat(2, 3)\nK =\n    0.4387    0.7655    0.1869\n    0.3816    0.7952    0.4898\nf =\n    0.4456\n    0.6463\nScript functions can contain at most one function. It is also a good practice to use lowercase names without spaces. You can use functions with multiple arguments and return values as needed.\nThere are many other advanced features and techniques for creating and using functions in MATLAB, but these are the basics to get you started.\n\n\nC.3.9 Loops and Control Structures\nMATLAB is a full-fledged interpreted programming language, and it provides many control flow operations.\n&gt;&gt; for i = 1:3, magic(i), end;\nans =\n     1\nans =\n     1     3\n     4     2\nans =\n     8     1     6\n     3     5     7\n     4     9     2\nIn the example above, a for loop iterates from 1 to 3, and for each iteration, the magic function is called.\n&gt;&gt; n = 1; while n &lt; 4, magic(n), n = n + 1; end;\nans =\n     1\nans =\n     1     3\n     4     2\nans =\n     8     1     6\n     3     5     7\n     4     9     2\nIn this example, a while loop is used. It continues to execute as long as the condition n &lt; 4 is true. Inside the loop, the magic function is called, and n is incremented.\n&gt;&gt; if rand(1) &gt; 0.5, disp('Greater than 0.5'); else, disp('Less than 0.5'); end;\nGreater than 0.5\nThe if statement is used to perform conditional execution. In this example, it checks if a random number is greater than 0.5 and displays a message accordingly.\nAs we need more details, we will add them gradually. Typically, writing these control structures directly in the command line can be cumbersome and less readable. It‚Äôs preferable to write everything in a script with proper indentation and execute the script as needed.\n\n\n\n\n\n\nExercise\n\n\n\n\nWrite a function named cubediag that takes a matrix A as input and returns the sum of the cubes of the diagonal elements.\nWrite another function named cubeantidiag that returns the sum of the cubes of the anti-diagonal elements (the diagonal that is symmetric to the main diagonal).\n\nYou can define these functions in separate script files or directly in the MATLAB environment.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nfunction s = cubediag(A)\n  s = sum(diag(A).^2);\nend\n\nfunction s = cubeantidiag(A)\n  % we could also use fliplr\n  Aflip = A(:,end:-1:1);\n  s = cubediag(Aflip);\nend\n\nfunction s = cubeantidiag(A)\n  % case of square matrices\n  n = size(A,1);\n  s = sum(A(end‚àín+1:1‚àín:n));\nend\n\n\n\n\n\n\n\nC.3.10 Plots\nTo create a plot of a real-valued function of a real variable in MATLAB, you can use the plot command. In its basic form, you provide two vectors, x and y, of the same size that represent the data points.\n&gt;&gt; x = linspace(-1, 1, 100);\n&gt;&gt; parab = @(x) x.^2;\n&gt;&gt; plot(x, parab(x));\n\n\n\nParabola Plot\n\n\nYou can customize the style, size, and color of the line using various options typically provided after the data points:\n&gt;&gt; x = linspace(-1, 1, 20);\n&gt;&gt; plot(x, parab(x), 'r*--', 'LineWidth', 2.0, 'MarkerSize', 15.0);\n&gt;&gt; grid on;\n&gt;&gt; xlabel('X-Axis');\n&gt;&gt; ylabel('Y-Axis');\n&gt;&gt; title('A Parabola', 'FontWeight', 'bold');\n\n\n\nStyled Parabola Plot\n\n\nTo see the possible combinations of line styles and markers, you can type help plot or doc plot. However, the most commonly used is the third argument, which is a string like r*--, which translates to:\n\nr: Red line\n*: Asterisk-shaped markers\n--: Dashed line\n\nThe order of styles is fixed, but each of them is optional. For example, by simply typing y, you will get a yellow line.\nTo overlay multiple plots, you can follow several approaches:\n\nDirect Overlay:\n&gt;&gt; x = linspace(-1, 1, 20);\n&gt;&gt; parab = @(x) x.^2;\n&gt;&gt; cubic = @(x) x.^3;\n&gt;&gt; plot(x, parab(x), x, cubic(x));\nCreating a Data Matrix:\n&gt;&gt; plot(x, [parab(x); cubic(x)]);\nUsing the hold Command:\n&gt;&gt; plot(x, parab(x));\n&gt;&gt; hold all;\n&gt;&gt; plot(x, cubic(x));\n&gt;&gt; hold off;\n\nYou can also add a legend to distinguish the plots:\n&gt;&gt; legend('Parabola', 'Cubic');\nYou can add multiple plots to the same graph by using the hold command:\n&gt;&gt; plot(x, parab(x));\n&gt;&gt; hold on;\n&gt;&gt; plot(x, cubic(x));\n&gt;&gt; hold off;\nYou can add a legend to differentiate between the plots:\n&gt;&gt; legend('Parabola', 'Cubic');\nAnother important type of plot, especially when evaluating the behavior of errors concerning parameters, is the use of logarithmic or semilogarithmic scales. For a given function \\(f(x)\\) with the graph \\(y = f(x)\\):\n\nsemilogx represents the points after changing the variable \\(x \\mapsto \\log x\\).\nsemilogy represents the points after changing the variable \\(y \\mapsto \\log y\\).\nloglog performs both of the above changes.\n\nFor example, the graph \\(y = e^{\\alpha x}\\), in a \\(y\\)-logarithmic scale, becomes \\(\\tilde{y} := \\log y = \\alpha x\\), which is a straight line.\n&gt;&gt; x = linspace(1, 10, 100);\n&gt;&gt; semilogy(x, exp(x), x, exp(2*x), 'LineWidth', 2.0);\n&gt;&gt; grid on;\n&gt;&gt; legend('Slope 1', 'Slope 2', 'Location', 'NorthWest');\n\n\n\nLogarithmic Plot\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nPlot the graph of the function \\(f(x) = 2 + (x-3)\\sin(5(x-3))\\) for \\(0 \\leq x \\leq 6\\). Overlay dashed lines that bound this function.\nConsider the function \\(f(x) = (\\log x)^2\\) for \\(0.1 \\leq x \\leq 10\\). What do you expect from the graph in logarithmic scaling? Plot and verify.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n&gt;&gt; x = linspace(0, 6, 100);\n&gt;&gt; f = 2 + (x-3).*sin(5*(x-3));\n&gt;&gt; r = [ 2 + (x-3); 2 - (x-3) ];\n&gt;&gt; plot(x, f, 'k-', x, r, 'k--', 'LineWidth', 2.0);\n\n\n\nimage\n\n\nGiven \\(\\hat{x} = \\log x\\) and \\(\\hat{y}=\\log y\\), the plot in \\(x\\)-log scale is \\(y = (\\log 2)^2 = \\hat{x}^2\\). We expect a parabola.\n&gt;&gt; x = 10.^linspace(-2,2,100);\n&gt;&gt; y = log(x).^2;\n&gt;&gt; semilogx(x, y, 'LineWidth', 2.0);\n\n\n\nimage",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>Introduction to MATLAB</span>"
    ]
  },
  {
    "objectID": "Laboratories/00 Intro Matlab/introMATLAB.html#approximation-error",
    "href": "Laboratories/00 Intro Matlab/introMATLAB.html#approximation-error",
    "title": "Appendix C ‚Äî Introduction to MATLAB",
    "section": "C.4 Approximation error",
    "text": "C.4 Approximation error\nWhen working with approximations, it‚Äôs essential to measure the accuracy of an approximation \\(\\hat{x}\\) to a number \\(x \\in \\mathbb{R}\\). Two common metrics are:\n\nAbsolute Error: \\(E_{\\text{abs}}(\\hat{x}) = |x - \\hat{x}|\\)\nRelative Error: \\(E_{\\text{rel}}(\\hat{x}) = \\frac{|x - \\hat{x}|}{|x|}\\) for \\(x \\neq 0\\)\n\nSignificant figures are a way to represent the accuracy of an approximation. Given an approximation \\(\\hat{x}\\) to \\(x\\), it has \\(p\\) significant figures if:\n\\[\nE_{\\text{abs}}(\\hat{x}) \\leq \\frac{1}{2} \\times 10^{s-p+1}\n\\]\nWhere \\(s\\) is the largest integer such that \\(|x| \\geq 10^s\\).\nWhen \\(x\\) is not known (common in many practical applications), significant figures can be determined by considering successive approximations. For a sequence \\(\\{x_i\\}_{i=0}^\\infty\\) that converges to \\(x\\), you can calculate the absolute error at each step \\(|x_i - x_{i-1}|\\).\n\n\n\n\n\n\nExercise\n\n\n\nComplete the following table (use format short for relative error):\n\n\n\n\n\n\n\n\n\n\n\\(x\\)\n\\(\\hat{x}\\)\nRelative Error\nAbsolute Error\nSignificant Figures\n\n\n\n\n1.6925\n1.69285\n‚Äì\n‚Äì\n‚Äì\n\n\n23.130\n23.129\n‚Äì\n‚Äì\n‚Äì\n\n\n23.130\n23.1299\n‚Äì\n‚Äì\n‚Äì\n\n\n23.130\n23.129999\n‚Äì\n‚Äì\n‚Äì\n\n\n0.00345\n0.00343\n‚Äì\n‚Äì\n‚Äì\n\n\n0.01008\n0.01012\n‚Äì\n‚Äì\n‚Äì\n\n\n0.01008\n0.01002\n‚Äì\n‚Äì\n‚Äì\n\n\n0.01008\n0.0102\n‚Äì\n‚Äì\n‚Äì\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(x\\)\n\\(\\hat{x}\\)\nRelative Error\nAbsolute Error\nSignificant Figures\n\n\n\n\n1.6925\n1.69285\n\\(0.21\\cdot 10^{-3}\\)\n\\(0.4\\cdot 10^{-3}\\)\n4\n\n\n23.130\n23.129\n\\(0.43\\cdot 10^{-4}\\)\n\\(0.1\\cdot 10^{-2}\\)\n4\n\n\n23.130\n23.1299\n\\(0.43\\cdot 10^{-5}\\)\n\\(0.1\\cdot 10^{-3}\\)\n5\n\n\n23.130\n23.129999\n\\(0.43\\cdot 10^{-7}\\)\n\\(0.1\\cdot 10^{-5}\\)\n7\n\n\n0.00345\n0.00343\n\\(0.58\\cdot 10^{-2}\\)\n\\(0.1\\cdot 10^{-4}\\)\n2\n\n\n0.01008\n0.01012\n\\(0.40\\cdot 10^{-2}\\)\n\\(0.4\\cdot 10^{-4}\\)\n3\n\n\n0.01008\n0.01002\n\\(0.60\\cdot 10^{-2}\\)\n\\(0.6\\cdot 10^{-4}\\)\n2\n\n\n0.01008\n0.0102\n\\(0.12\\cdot 10^{-1}\\)\n\\(0.1\\cdot 10^{-3}\\)\n1\n\n\n\n\n\n\n\n\n\nC.4.1 Floating-Point Arithmetic\nNatural numbers (\\(\\mathbb{N}\\)), although infinite, can be represented exactly on a computer within a predefined range of values. For example, a 32-bit integer can represent numbers from 0 to \\(2^{32}-1\\). Similarly, it can represent signed integers (\\(\\mathbb{Z}\\)) from \\(-2^{31}\\) to \\(2^{31}-1\\).\nReal numbers (\\(\\mathbb{R}\\)), on the other hand, are too numerous to be represented exactly within any chosen range. There are at least two possibilities: the first involves fixing the number of digits after the decimal point (fixed-point). The second considers the proper subset \\(\\mathbb{F} \\subset \\mathbb{R}\\) of floating-point numbers:\n\\[y = \\pm m \\times \\beta^{e-t},\\]\nwhere \\(\\beta\\) is the base, \\(t\\) is the precision, and \\(e \\in [e_{\\text{min}}, e_{\\text{max}}]\\) is the exponent. For example, with \\(\\beta=2\\), \\(t=3\\), \\(e_{\\text{min}}=-1\\), and \\(e_{\\text{max}}=3\\), you can represent numbers like:\n\\[\n\\begin{split}\n0&, 0.25, 0.3125, 0.3750, 0.4375, 0.5, 0.625, 0.750, 0.875, \\\\\n1.0&, 1.25, 1.50, 1.75, 2.0, 2.5, 3.0, 4.0, 5.0, 6.0, 7.0.\n\\end{split}\n\\]\nThe following graph illustrates the spacing of these numbers:\n\n\n\nGraphical representation of floats\n\n\nThe IEEE-754 standard sets values for these parameters in two significant cases: single precision (float in C), where \\(\\beta=2\\), \\(t=24\\), \\(e_{\\text{min}}=-125\\), and \\(e_{\\text{max}}=128\\), and double precision (double in C), where \\(\\beta=2\\), \\(t=53\\), \\(e_{\\text{min}}=-1021\\), and \\(e_{\\text{max}}=1024\\).\nMachine epsilon (\\(\\varepsilon_{\\text{M}}\\)) is the smallest positive number such that \\(1 + \\varepsilon_{\\text{M}} \\neq 1\\) in the floating-point system. It represents the distance between 1 and the next representable number in the system. For single precision, \\(\\varepsilon_{\\text{M}}\\) is approximately \\(1.19 \\times 10^{-7}\\), while for double precision, it‚Äôs approximately \\(2.22 \\times 10^{-16}\\).\nIn MATLAB, you can check these values using the eps function:\n&gt;&gt; eps\nans = \n   2.2204e-16 \nThe limits of representable numbers can also be checked with realmin and realmax:\n&gt;&gt; realmax\nans =\n  1.7977e+308\n&gt;&gt; realmin\nans =\n  2.2251e-308\nAs observed from the previous graph, the spacing between numbers in \\(\\mathbb{F}\\) is not constant. MATLAB‚Äôs eps command allows you to determine these spacings:\n&gt;&gt; eps(1)\nans = \n   2.2204e-16\n&gt;&gt; eps(10)\nans = \n   1.7764e-15\n&gt;&gt; eps(100)\nans = \n   1.4211e-14\n&gt;&gt; eps(1000)\nans =\n   1.1369e-13\nThe set \\(\\mathbb{F}\\) also includes some exceptional cases:\n&gt;&gt; 1e400\nans = \n   Inf\n&gt;&gt; 1e-400\nans =\n     0\n&gt;&gt; 1/0\nans =\n   Inf\n&gt;&gt; 0/0\nans =\n   NaN\n\nThe number Inf represents infinity, meaning it‚Äôs beyond realmax. It‚Äôs not considered an error, and arithmetic operations involving infinity are still valid, although they can often lead to undesirable results.\nWhen dealing with numbers smaller than realmin, they are approximated to 0.\nThe situation NaN (Not-a-Number) indicates that the result is undefined or has no meaningful value.\n\nFloating-point arithmetic has its limitations, including a lack of associativity for some operations, which can lead to significant errors in some cases. For example, when subtracting two nearly equal numbers, you can experience loss of precision due to the finite precision of the representation:\n&gt;&gt; x = 1.0e-15;\n&gt;&gt; (1+x)-1\nans =\n   1.1102e-15\n&gt;&gt; (1-1)+x\nans =\n   1.0000e-15\nThis issue is known as cancellation error and should be avoided whenever possible in numerical computations. The problem is indeed quite serious, and it‚Äôs a common issue in numerical computations, especially when dealing with small numbers. For example, you have two functions:\n\\[f(x) = \\frac{1-\\cos x}{x^2}, \\quad g(x) = \\frac{1}{2}\\left(\\frac{\\sin(x/2)}{x/2}\\right)^2.\\]\nBoth of these functions are identical. However, when you evaluate them for \\(x = 1.2 \\times 10^{-8}\\) in MATLAB, you get different results:\n&gt;&gt; x = 1.2e-8;\n&gt;&gt; (1 - cos(x)) / x^2;\nans =\n    0.7710\n\n&gt;&gt; 0.5 * (sin(x / 2) / (x / 2))^2;\nans =\n    0.5000\nThe first result is clearly incorrect, and it violates the bounds of the function, which should be between 0 and 0.5. This is a classic example of the problem of numerical precision and the limitations of finite-precision arithmetic. Small values of x lead to significant errors due to rounding and truncation.\nTo address such issues, numerical analysts often use techniques like Taylor series expansions, higher precision arithmetic, or specialized algorithms to improve the accuracy of computations involving small numbers.\nIt is good practice to avoid cases like these as much as possible, as they are quite common. For example:\n\\[\n\\begin{aligned}\nf(x) &= \\frac{e^x-1}{x}, &&\\text{for $x \\approx 1$,} \\\\\nx_{1,2} &= \\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}, &&\\text{for $b^2 \\approx 4ac$ and $b \\approx \\sqrt{b^2-4ac}$,} \\\\\ns_n^2 &= \\frac{1}{n-1}\\left( \\sum\\nolimits_{i=1}^n x_i^2 - \\frac{1}{n}\\left(\\sum\\nolimits_{i=1}^n x_i \\right)^2 \\right) &&\n\\end{aligned}\n\\]\nIn the last case, when calculating variance, it is even possible to obtain negative results, which have no mathematical sense.\n\n\n\n\n\n\nExercise (Archimedes‚Äô Method for \\(\\pi\\) Approximation)\n\n\n\nThe strategy used by Archimedes to approximate \\(\\pi\\) involves considering regular polygons inscribed and circumscribed within the unit circle. In fact, if \\(n\\) is the number of sides, the perimeter \\(P_n\\) approaches \\(2\\pi\\) as \\(n \\to \\infty\\).\nStarting with a hexagon and successively doubling the number of sides, it can be found that as \\(i \\to \\infty\\), \\(6 \\cdot 2^i \\cdot t_i\\) approaches \\(\\pi\\), where \\(t_i\\) satisfies the following relation:\n\\[\nt_0 = \\frac{1}{\\sqrt{3}}, \\qquad\nt_{i+1} = \\frac{\\sqrt{t_i^2+1}-1}{t_i}.\n\\]\nHere are the tasks related to this approximation:\n\nImplement this algorithm in MATLAB and compare the approximation with \\(\\pi\\) provided by the pi constant. What do you observe?\nReplace the recursive formula with its equivalent:\n\n\\[\nt_0 = \\frac{1}{\\sqrt{3}}, \\qquad\nt_{i+1} = \\frac{t_i}{\\sqrt{t_i^2+1}+1}.\n\\]\nComment on any differences observed.\nNote: Use format long to display the differences.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe value given by Matlab for \\(\\pi\\) is:\n&gt;&gt; format long\n&gt;&gt; pi\nans =\n   3.141592653589793\nWe compute now \\(\\pi\\) with method 1. pia and method 2. pib:\n&gt;&gt; format long;\n&gt;&gt; n = 30;\n&gt;&gt; ta = 1/sqrt(3);\n&gt;&gt; tb = 1/sqrt(3);\n&gt;&gt; for i = 1:n\n     % first method\n     ta = (sqrt(ta^2+1)-1)/ta;\n     pia = 6*2^i*ta;\n     % second method\n     tb = tb/(sqrt(tb^2+1)+1);\n     pib = 6*2^i*tb;\n&gt;&gt; end\n&gt;&gt; fprintf('Method 1, pi = %f, err = %e\\n', pia, abs(pia-pi));\n&gt;&gt; fprintf('Method 2, pi = %f, err = %e\\n', pib, abs(pib-pi));\nWe see that the first method suffers of cancellation errors.\n\n\n\n\n\n\n\n\n\n\n\nExercise (Polynomial Expansion)\n\n\n\nConsider the polynomial \\(p(x) = (x-1)^7\\) for \\(x \\in [0.998, 1.012]\\). Expand the polynomial using the binomial formula and compare it to the original unexpanded polynomial. Comment on any differences.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nExpanding we have \\[\np(x) = x^7 - 7x^6 + 21x^5 - 35x^4 + 35x^3 - 21x^2 + 7x - 1.\n\\]\nNow we compare\n&gt;&gt; x = 0.998:0.0001:1.012;\n&gt;&gt; plot(x, x.^7-7*x.^6+21*x.^5-35*x.^4+35*x.^3-21*x.^2+7*x-1, ...\n        x, (x-1).^7, 'LineWidth', 2.0);\n&gt;&gt; grid on;\n&gt;&gt; legend('Expanded', 'Original');\n\n\n\nimage",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>Introduction to MATLAB</span>"
    ]
  }
]