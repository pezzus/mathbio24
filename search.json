[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mathematical Biology",
    "section": "",
    "text": "Welcome\nThis is the webpage of Mathematical Biology! Here you will find all the material for the course. Stay tuned for regular updates!",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Mathematical Biology",
    "section": "Schedule",
    "text": "Schedule\n\n\n\nSchedule of the course\n\n\n\nüì¢ Exam‚Äôs schedule üì¢\n\n1st call\n\nwritten, Jan 8th, 2025, 10:30 - 13:30 (A208)\noral, Jan 10th, 2025, 08:30 - 13:30 (A210)\n\n2nd call\n\nwritten, Feb 12th, 2025, 14:00 - 17:00 (A204)\noral, Feb 14th, 2025, 08:30 - 13:30 (A208)",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#office-hours",
    "href": "index.html#office-hours",
    "title": "Mathematical Biology",
    "section": "Office hours",
    "text": "Office hours\nAny time Mon-Wed, please make an appointment by email first Works best right after the lectures",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#exams",
    "href": "index.html#exams",
    "title": "Mathematical Biology",
    "section": "Exams",
    "text": "Exams\nThe exam will consist of a written test (50% of the final grade), an oral exam (20%), and a project presentation (30%). Specifically:\n\nWritten Test: Exercises similar to those covered in class and available on the course website.\nOral Exam: One question at the blackboard on the theoretical part of the course.\nProject: Conducted in groups of up to 2 students. Typically involves reading an article, or implementing or studying (presentation of the model, qualitative analysis, simulations) a model not covered in class. The presentation will last about 25 minutes (total, not per student) and will be part of a mini-workshop open to other students in the course and the public. A selection of possible projects and articles will be available on the website.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#topics",
    "href": "index.html#topics",
    "title": "Mathematical Biology",
    "section": "Topics",
    "text": "Topics\nThe course ‚ÄúMathematical Modeling‚Äù has a dual purpose: on one hand, to introduce students to some basic mathematical models in various areas of biology (demography, ecology, infectious diseases, enzyme reactions, physiology, molecular networks); on the other hand, to provide fundamental knowledge in the analysis and numerical simulation of ordinary and partial differential equations.\nSpecifically, the first part of the course is dedicated to modeling using ordinary differential equations and introduces various analytical techniques (linearization, equilibria and their stability, bifurcation, regular and singular perturbations).\n\nOverview of ordinary differential equations (ODEs): Solution of linear equations; equilibria and linearized stability; phase plane, limit cycles; numerical schemes for solving ODEs.\nOne- or two-dimensional models in demography, ecology, epidemiology, and immunology. Non-dimensionalization of variables and parameters.\nSlow-fast systems, enzyme reaction models and their simplification using perturbative methods.\nBifurcation of equilibria and application to predator-prey systems and molecular networks. Simplified models of important biological phenomena, such as the cell cycle and glucose-insulin oscillations.\nExcitable systems: Hodgkin-Huxley equations (overview) and FitzHugh-Nagumo equations.\nParameter estimation for differential models.\n\nIn the second part, partial differential equation models and some techniques for constructing or approximating solutions will be studied. Additionally, some of the most interesting phenomena of reaction-diffusion equations (traveling wave solutions, Turing mechanism) will be presented in a biological context (morphogenesis).\n\nDynamical systems on networks. Examples in epidemiology.\nIntroduction to partial differential equations (PDEs): Solutions by separation of variables. Fourier series. The heat equation and Brownian motion. Eigenfunctions of the Laplacian. Numerical approximation.\nSkellam and Fisher equations: Waveform solutions; stationary solutions of the boundary value problem.\nStability of stationary solutions of reaction-diffusion systems and Turing‚Äôs mechanism for morphogenesis. Conditions for its validity and examples. Chemotaxis: The Keller-Segel model.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "01_Bathtub.html",
    "href": "01_Bathtub.html",
    "title": "1¬† Bathtub model",
    "section": "",
    "text": "1.1 The bathtub üõÅ model\nThe models of Newtonian physics are made of differential equations built starting from the second law of the dynamics. The structure of the models discussed here is instead simpler; they are based on the ‚Äúbalance equation of the bathtub‚Äù: if \\(Q(t)\\) is the quantity of a substance in the bathtub we have \\[\n\\frac{\\mathrm{d}Q}{\\mathrm{d}t} = Q'(t) = I(t) - O(t),\n\\]\nwhere\nTo be more precise, the assumption is that, if \\(I_{(t,t+\\Delta t)}\\) is the quantity that enters in the interval \\((t,t+\\Delta t)\\), we have \\(I_{(t,t+\\Delta t)} = I(t)\\Delta t + o(\\Delta t)\\), where \\(o(\\Delta t)\\) is a higher order infinitesimal than \\(\\Delta t\\). Hence: \\[\nI(t) = \\lim_{\\Delta t\\to 0} \\frac{I_{(t,t+\\Delta t)}}{\\Delta t}.\n\\]\nThe input rate \\(I(t)\\) is like an instantaneous velocity: the quantity entered in a given time, when that time becomes very small. Hence \\(I(t)\\) is measured in \\([C][t^{‚àí1}]\\) units where \\([C]\\) represents the concentration of the quantity \\(Q\\). Similarly for the exit rate \\(O(t)\\).\nLet us start from a very simple example. Assume \\(I(t) = \\Lambda\\) constant input flux; \\(O(t) = \\gamma Q(t)\\), i.e.¬†exit flux is proportional to the quantity present at the moment; the proportionality constant \\(\\gamma\\) is often called the exit rate and has the dimension \\([t^{‚àí1}]\\), the inverse of time. From these assumptions we get: \\[\nQ'(t) = \\Lambda - \\gamma Q(t),\n\\tag{1.1}\\]\nsupplemented with some initial condition \\[\nQ(0) = Q_0.\n\\]\nThe solution is: \\[\nQ(t) = e^{-\\gamma t} Q_0 + \\frac{\\Lambda}{\\gamma}\\Bigl( 1 - e^{-\\gamma t} \\Bigr).\n\\]\nNote that if \\(\\Lambda = 0\\) (no input), the solution is simply \\[\nQ(t) = Q_0 e^{-\\gamma t}.\n\\]\nThis means that the survival time of a molecule initially present follows the exponential distribution: \\[\n\\mathbb{P}[\\text{a molecule present at time 0 is present at time $t &gt; 0$}] =\n\\frac{Q(t)}{Q_0} = e^{-\\gamma t}.\n\\]\nFrom the properties of the exponential distribution, we obtain that the mean survival time \\(\\mathbb{E}[T] = 1/\\gamma\\); hence the exit rate \\(\\gamma\\) can be interpreted as the inverse of the mean survival time.\nTo be more precise, let us define a continuous random variable \\(T\\), which measures the lifetime of a particle present in the bathtub. Then, the cumulative distribution \\(F(t)\\) of \\(T\\) is given by \\[\n\\begin{split}\nF(t) &= \\mathbb{P}[T \\le t] \\\\\n&= 1 - \\mathbb{P}[T &gt; t] \\\\\n&= 1 - \\mathbb{P}[\\text{a molecule present at time 0 is present at time $t &gt; 0$}] \\\\\n&= 1 - e^{-\\gamma t}.\n\\end{split}\n\\]\nSo, we indeed have an exponential distribution. The probability density function is: \\[\nf(t) = F'(t) = \\gamma e^{-\\gamma t},\n\\]\nand the expectation is: \\[\n\\mathbb{E}[T] = \\int_0^\\infty t f(t) \\mathrm{d}t = \\frac{1}{\\gamma}.\n\\]",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Bathtub model</span>"
    ]
  },
  {
    "objectID": "01_Bathtub.html#the-bathtub-model",
    "href": "01_Bathtub.html#the-bathtub-model",
    "title": "1¬† Bathtub model",
    "section": "",
    "text": "\\(I(t)\\) is the input rate (quantity that enters per unit time)\n\\(Q(t)\\) is the output rate (quantity that leaves per unit time).\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nSolve Equation¬†1.1 with the method you prefer.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nSolve Equation¬†1.1 with the general formula for linear ODEs, by first defining the matrix exponential (here, just a scalar function).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCompute the above integral explicitly.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Bathtub model</span>"
    ]
  },
  {
    "objectID": "01_Bathtub.html#malthus-equation",
    "href": "01_Bathtub.html#malthus-equation",
    "title": "1¬† Bathtub model",
    "section": "1.2 Malthus equation",
    "text": "1.2 Malthus equation\nThe metaphor of the bathtub can be used to model the dynamics of a population. Neglecting all differences among individuals (due to age, sex, genetic,‚Ä¶) we can represent a population through its size \\(N(t)\\); this will increase through inputs due to births and outputs due to deaths (if immigration and emigration are not considered). Hence \\[\nN'(t) = B(t) - D(t),\n\\]\nwhere \\(B(t) = \\text{births}\\) and \\(D(t) = \\text{deaths}\\).\nMalthus model assumes\n\nwithin a (short) time period of length \\(\\Delta t\\), each individual gives, on average, birth to \\(\\beta\\Delta t\\) new individuals; hence \\(B(t) = \\beta N(t)\\);\nwithin the same time period \\(\\Delta t\\), each individual has probability \\(\\mu\\Delta t\\) of dying; hence \\(D(t) = \\mu N(t)\\).\n\nWe get the following equation \\[\nN'(t) = \\beta N(t) - \\mu N(t) = (\\beta - \\mu)N(t),\n\\]\nthat represents the Malthus model. The parameter \\(\\beta\\) is known as fertility rate, while \\(\\mu\\) is the mortality rate. Finally, \\[\nr = \\beta - \\mu\n\\]\nis the (instantaneous) growth rate and is also called Malthus parameter or biological potential of the population.\nWith the initial condition \\[\nN(0) = N_0,\n\\]\nthe evolution of the population is completely determined. In fact, the solution is \\[\nN(t) = N_0 e^{rt},\n\\]\nand we see that the population will go to extinction or will grow without limits if \\(r &lt; 0\\) or \\(r &gt; 0\\), respectively. If instead \\(r = 0\\), the population size is constant (births and deaths compensate.)\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_theme(\"notebook\", style=\"whitegrid\")\nt = np.linspace(0,1,100)\n\nplt.plot(t,np.exp(1*t),label='r &gt; 0')\nplt.plot(t,np.exp(-1*t),label='r &lt; 0')\nplt.plot(t,np.exp(0*t),label='r = 0')\nplt.grid()\nplt.legend()\nplt.xlabel('Time')\nplt.ylabel('Population')\nplt.show()\n\n\n\n\n\nExample of solutions of Malthus equation",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Bathtub model</span>"
    ]
  },
  {
    "objectID": "02_Population.html",
    "href": "02_Population.html",
    "title": "2¬† Population dynamics",
    "section": "",
    "text": "2.1 Population growth model\nWe have seen, using the bathtub analogy, that the fundamental balance equation of population dynamics take the form \\[\nN'(t) = B(t) - D(t) + I(t) - E(t),\n\\]\nwhere at time \\(t\\) we have that\nThere is also an integral interpretation, in fact: \\[\nN(t) - N(t_0) =\n\\underbrace{\\int_{t_0}^t B(s)\\,\\mathrm{d}s}_\\text{births} - \\underbrace{\\int_{t_0}^t D(s)\\,\\mathrm{d}s}_\\text{deaths} + \\underbrace{\\int_{t_0}^t I(s)\\,\\mathrm{d}s}_\\text{immigration} - \\underbrace{\\int_{t_0}^t E(s)\\,\\mathrm{d}s}_\\text{emigration}.\n\\]\nA population growth model is an ODE of the above form with some specific form for each term above. Note that we could provide functions above explicitly, as function of time alone. However, it should be clear that some of them like \\(B(t)\\) or \\(D(t)\\) should depend on \\(N\\).",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Population dynamics</span>"
    ]
  },
  {
    "objectID": "02_Population.html#population-growth-model",
    "href": "02_Population.html#population-growth-model",
    "title": "2¬† Population dynamics",
    "section": "",
    "text": "\\(N(t)\\) is the population,\n\\(B(t)\\) is the birth rate,\n\\(D(t)\\) is the death rate,\n\\(I(t)\\) is the immigration rate, and\n\\(E(t)\\) is the emigration rate.\n\n\n\n\n2.1.1 Malthus model\nQuoting Thomas R. Malthus (1766-1834):\n\nPopulation, when unchecked, increases in a geometrical ratio. Subsistence increases only in an arithmetical ratio. A slight acquaintance with numbers will show the immensity of the first power in comparison of the second.\nEssay on the Principle of Population, 1798\n\nMalthus‚Äô model is mathematical formulation of the above statement. We already derived Malthus model, but let us recall the hypotheses.\n\nThe population is homogeneous, that is all individuals are identical. We have a single class to represent them, that is \\(N(t)\\).\nThe population is isolated, so \\(E(t)=I(t)=0\\).\nThe habitat is invariant, so resources and life conditions are not affected by the environment nor the population itself.\nThe population is very large, so we can consider continuous functions.\nOn a short time scale \\(\\Delta t\\), each individual gives birth to \\(\\beta \\Delta t\\) new individuals, \\(B(t) = \\beta N(t)\\).\nOn a short time scale \\(\\Delta t\\), each individual has probability \\(\\mu \\Delta t\\) of dying, \\(D(t) = \\mu N(t)\\).\n\nThe non-negative parameters \\(\\beta\\) and \\(\\mu\\) are the fertility and mortality rate, respectively. From hypothesis 4, we have that \\(\\mu\\) and \\(\\beta\\) are constant. We introduce the growth rate \\[\nr = \\beta - \\mu,\n\\]\nalso called Malthus parameter or biological potential.\nThe Malthus‚Äô model reads: \\[\nN' = rN, \\quad\\Rightarrow\\quad N(t) = e^{rt}N(0),\n\\]\nthus, when \\(r&gt;0\\), the growth of population is geometrical and unbounded, as predicted by Malthus.\n\n\n2.1.2 Expected life\nWe have seen that, in absence of births, the population goes like \\[\nN(t) = e^{-\\mu t} N_0,\n\\]\nthus, we could say that the probability of surviving up to time \\(t\\) is \\(e^{-\\mu t}\\). Specifically, the life expectancy \\(L\\) is a random variable such that \\[\n\\mathbb{P}[L &gt; t] = e^{-\\mu t}.\n\\]\nThus, the cumulative distribution of \\(L\\) is \\[\nF_L(t) = \\mathbb{P}[L &lt; t] = 1 - \\mathbb{P}[L &gt; t] = 1-e^{-\\mu t},\n\\]\nand the probability density function is \\[\nf_L(t) = F_L'(t) = \\mu e^{-\\mu t}.\n\\]\nWe conclude that \\(L\\sim \\mathrm{Exp}[\\mu]\\), the exponential distribution. The average life expectancy is: \\[\n\\mathbb{E}[L] = \\int_0^\\infty s f_L(s)\\:\\mathrm{d}s = \\int_0^\\infty s \\mu e^{-\\mu s}\\:\\mathrm{d}s = \\frac{1}{\\mu}.\n\\]\nWe have another interpretation of \\(\\mu\\): it is the reciprocal of the expected life time.\n\n\n2.1.3 Basic reproduction number\nLet us rescale the equation and put it in non-dimensionalized form. This is a fundamental step in general, because\n\nit reduces the number of parameters,\nit removed scale effects (units are removed),\nit highlights the determining factors of the model (maybe what matters is not this or that parameter, by their ratio or sum).\n\nHere, we rescale as follows: \\[\n\\tau = \\mu t, \\quad u = N,\n\\]\nso that time is now in units of ‚Äúexpected life time‚Äù: \\(\\tau=1\\) means \\(t=\\mu^{-1} = \\mathbb{E}[L]\\). We have that \\[\nN' = \\frac{\\mathrm{d}N}{\\mathrm{d}t} = \\frac{\\mathrm{d}u}{\\mu^{-1}\\mathrm{d}\\tau} = \\mu\\frac{\\mathrm{d}u}{\\mathrm{d}\\tau} = \\mu\\dot{u}.\n\\]\nWe use the ‚Äúdot‚Äù notation \\(\\dot{u}\\) for the derivative for the non-dimensional form, just to remember that now the time is \\(\\tau\\) and not \\(t\\). We finally obtain: \\[\n\\dot{u} = \\mu^{-1}N' = \\frac{\\beta}{\\mu} N - N = (R_0 - 1) u,\n\\]\nwhere we defined the basic reproduction number \\[\nR_0 = \\frac{\\beta}{\\mu} = \\beta\\mathbb{E}[L].\n\\]\nWe could interpret it as the average number of newborns produced by one individual during his whole life. Note that \\(R_0\\) is non-dimensional.\n\n\n\n\n\n\nExercise\n\n\n\nWhy it does not make much sense to use the scaling \\(\\tau = r t\\)?\n\n\n\n\n2.1.4 Migration\nIn the presence of migration, say with a constant rate, we have the ODE: \\[\nN' = rN + m = f(N),\n\\tag{2.1}\\]\nwhere \\(m = I - E\\). If positive, there is a net immigration, otherwise emigration.\nIn order to study how the model will behave, we have 3 options:\n\nSolve the problem analytically, that is finding \\(N(t) = \\ldots\\) explicitly.\nSolve the problem numerically, which is always possible.\nStudy the problem qualitatively.\n\nThe last option has the advantage that we can be generic, there is no need for a specific value of the parameters or the initial condition. The qualitative study consists in the following steps:\n1) Fixed points of the system. A fixed point or equilibrium (see Definition¬†A.12) is a constant solution of the ODE. We can find it by setting the right hand side to zero: \\[\nN'=0 \\quad\\Leftrightarrow\\quad r N + m = 0.\n\\]\nThe model has a single equilibrium for \\[N = N^* = -\\frac{m}{r} = \\frac{m\\mathbb{E}[L]}{1-R_0}\\]\n2) Biological feasibility. Equilibria must be biologically feasible. For this model, we need to check that \\(N^*\\) is non-negative, otherwise it doesn‚Äôt make sense biologically speaking. Therefore \\[\nN^* \\ge 0, \\quad\\Leftrightarrow\\quad \\text{$m$ and $r$ have opposite sign and $r\\neq 0$.}\n\\]\n3) Local stability. Informally, an equilibrium is locally stable when, starting from a neighborhood of it, the solution stays close to it for \\(t\\to\\infty\\). It is asymptotically stable when the solution converges toward the equilibrium for \\(t\\to\\infty\\). It is unstable otherwise. See Definition¬†A.13 for a more precise statement.\nThe local stability is determined by the sign of the derivative of the right hand side, that is \\(f'(N^*)\\) for \\(f(N)=rN+m\\). In general (see Section A.5.5),\n\nwhen \\(f'(N^*)&lt;0\\), the equilibrium is asymptotically stable, and\nwhen \\(f'(N^*)&gt;0\\), the equilibrium is unstable.\n\nTo see this in this specific case, let us define \\(w(t)=N(t)-N^*\\). Then, \\[\nw' = N' = rN + m = r(N - N^*) = rw.\n\\]\nNote that \\(f'(N) = r\\). We have that: \\[\nw(t) = w_0 e^{rt},\n\\]\nhence,\n\nif \\(r &lt; 0\\), then \\(w \\to 0\\) for all \\(w_0\\), so \\(N(t)\\to N^*\\). The equilibrium is locally asymptotically stable.\nif \\(r &gt; 0\\), then \\(w \\to \\infty\\) and the equilibrium is unstable.\n\n4) Global stability. What if we start very far away from the equilibrium? In this particular case, with a linear ODE, the local stability argument applies also globally, thus the equilibrium is globally attractive. But for general, nonlinear ODEs this may not be the case, so we perform the analysis anyway. Note that if \\(N(0)=N_0 &gt; N^*\\), then \\[\nN'(0) = rN(0) + m = r (N_0 - N^*) &lt; 0,\n\\]\nso the derivative of the solution is negative (assuming \\(r&lt;0\\)). Furthermore, for \\(N(t) &gt; N^*\\), the derivative is always negative. So, the solution must be monotonically decreasing. But the solution is bounded from below by the equilibrium \\(N(t)=N^*\\), so we conclude that: \\[\nN(t) \\to N^*\n\\]\nfor all \\(N_0 \\ge N^*\\). Symmetrically, when \\(N_0 &lt; N^*\\), the derivative is positive and stays positive for all \\(t\\), so the solution is monotonically increasing. Hence: \\[\nN(t) \\to N^*\n\\]\nfor all \\(N^0\\). The equilibrium is therefore globally stable when \\(r&lt;0\\).\n5) Phase portrait. The phase portrait of a dynamical system is the collection of all possible orbits. Here, the phase space is \\(\\Omega=[0,\\infty)\\). The only equilibrium we have, \\(N^*\\), is a barrier to other orbits, because orbits cannot intersect (See Proposition¬†A.2). Therefore:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.reset_defaults()\nsns.set_context('notebook')\nr,m = -0.5,1.7\nNeq = -m/r\n\nf = lambda N: r*N + m\nN = np.linspace(1.5,5,10)\n\nfig, ax = plt.subplots()\nax.spines[[\"bottom\"]].set_position((\"data\", 0))\nax.spines[[\"top\", \"right\", \"left\"]].set_visible(False)\nax.xaxis.set_ticks([])\nax.yaxis.set_ticks([])\nax.plot(1, 0, \"&gt;k\", transform=ax.get_yaxis_transform(), clip_on=False)\nax.set_xlabel('N', loc='right', labelpad=10.0)\n\nax.plot(Neq,0,'r.',markersize=16, zorder=99)\nax.text(Neq, 3e-3, r'$N^*$', fontsize=12, ha='center', va='bottom')\n\nax.quiver(N,0*N, f(N), 0.0, color='blue', zorder=80)\nax.set_xlim((0,6))\nax.set_ylim((-1e-2,1e-2))\nfig.subplots_adjust(left=0, right=1, top=0.1, bottom=0.05)\nplt.show()\n\n\n\n\n\nPhase space\n\n\n\n\nFor this problem we actually have the general solution \\[\nN(t) = (N_0 - N^*)e^{rt} + N^*.\n\\]\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nr,m = -0.5,1.7\nNeq = -m/r\n\nsns.set_theme(\"notebook\", style=\"whitegrid\")\nt = np.linspace(0,10,100)\nN = lambda N0: (N0-Neq)*np.exp(r*t) + Neq\n\nfig, ax = plt.subplots()\nax.plot(t,N(Neq - 1),'b',label='$N_0 &lt; N^*$')\nax.plot(t,N(Neq + 1),'b',label='$N_0 &gt; N^*$')\nfor delta in np.arange(0.1,0.9,0.1):\n    ax.plot(t,N(Neq + delta),'b',lw=0.4)\n    ax.plot(t,N(Neq - delta),'b',lw=0.4)\nax.plot(t,N(Neq),'r-',lw=2,label='$N^*$')\nax.grid()\nax.legend()\nax.set_xlabel('Time')\nax.set_ylabel('Population')\nplt.show()\n\n\n\n\n\nExample of trajectories\n\n\n\n\nThe case \\(m&lt;0\\) and \\(r&gt;0\\) is also interesting. Here, the equilibrium \\(N^*\\) is positive, but unstable. For \\(N_0 &gt; N^*\\) the population still grows exponentially, so emigration as no effect on the overall population. But for \\(N_0 &lt; N^*\\) the solution will become negative in finite time: the model is not correct. The reason is simple: the emigration \\(m\\) cannot be constant (like the immigration), it must depend on \\(N\\) as well.\n\n\n\n\n\n\nExercise\n\n\n\nSolve Equation¬†2.1 with separation of variables.\n\n\n\n\n2.1.5 Exogenous variability\nBy exogenous variability we mean variability in the parameter \\(r\\) that does not depend on the population. (Endogenous variability is when the parameters depend on internal variables like \\(N\\).) Thus, we consider the problem: \\[\nN' = r(t)N,\n\\]\nfor \\(r(t)\\) continuous function. An example could births and deaths that depend on climate or temperature. The solution is: \\[\nN(t) = N_0 e^{\\int_{t_0}^t r(s)\\,\\mathrm{d}s}\n\\]\nWe can also rewrite the solution as: \\[\nN(t) = N_0 e^{(t-t_0)\\frac{1}{t-t_0}\\int_{t_0}^t r(s)\\,\\mathrm{d}s},\n\\]\nshowing that if the limit \\[\nr^* = \\lim_{t\\to\\infty}\\frac{1}{t-t_0}\\int_{t_0}^t r(s)\\,\\mathrm{d}s\n\\]\nexists then the asymptotic behavior of the solution is: \\[\nN(t) \\approx e^{r^*(t-t_0)}, \\quad t \\gg 1.\n\\]\nAn interesting case is when \\(r(t)\\) is periodic, that is there exists \\(T&gt;0\\) such that \\(r(t+T)=r(t)\\). In this case the above formula still applies, but we can ‚Äúforget‚Äù the limit (check Assignments for a proof) and take: \\[\n\\bar{r} = \\frac{1}{T} \\int_0^T r(s)\\,\\mathrm{d}s,\n\\]\nand write the solution for any \\(t \\ge t_0\\) as: \\[\nN(t) = e^{\\bar{r}(t-t_0)}N_\\pi(t),\n\\]\nfor some \\(T\\)-periodic function \\(N_\\pi\\). Note that \\[\nN(t_0 + kT) = e^{\\bar{r}kT}N_\\pi(t_0) = e^{\\bar{r}kT}N(t_0),\n\\]\nso after \\(k\\) periods the solution grows by a factor \\(e^{\\bar{r}kT}\\). The factor \\(e^{\\bar{r}}\\) is called Floquet multiplier.\n\n\n\n\nIannelli, Mimmo, and Andrea Pugliese. 2014. An Introduction to Mathematical Population Dynamics. Vol. 79. Springer. https://strutture-provincia.primo.exlibrisgroup.com/permalink/39SBT_INST/ghkipk/alma991006883209706186.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Population dynamics</span>"
    ]
  },
  {
    "objectID": "03_Logistic.html",
    "href": "03_Logistic.html",
    "title": "3¬† Logistic model",
    "section": "",
    "text": "3.1 Logistic model\nWhile populations can follow a phase of exponential growth for a limited amount of time, it seems impossible that this can go forever and that populations can grow to infinity. Indeed, we expect that there exists a negative effect of crowding, which can be stated in words as follows:\nThis simple statement tries to summarize the complex phenomenology of intraspecific competition due to many factors such as resource availability, habitat pollution and waste, predation increase, energy consumption for social organization.\nThe simplest way to include this effect into a model, is to suppose that fertility decreases and mortality increases linearly with the number of individuals; namely \\[\n\\begin{aligned}\n\\beta(N) &= \\beta_0 - \\tilde{\\beta}N, \\\\\n\\mu(N)   &= \\mu_0   + \\tilde{\\mu}N,\n\\end{aligned}\n\\]\nwhere \\(\\beta_0\\), \\(\\mu_0\\), \\(\\tilde{\\beta}\\), and \\(\\tilde{\\mu}\\) are non-negative constants. Hence: \\[\n\\begin{aligned}\nB(N) &= \\beta(N)N = \\beta_0 N - \\tilde{\\beta}N^2, \\\\\nD(N) &= \\mu(N)N   = \\mu_0   N + \\tilde{\\mu}N^2.\n\\end{aligned}\n\\]\nThe resulting equation is generally written, after simple algebraic steps, as \\[\n\\left\\{\\begin{aligned}\nN' &= r \\Bigl(1 - \\frac{N}{K} \\Bigr)N, \\\\\nN(0) &= N_0,\n\\end{aligned}\\right.\n\\]\nwhere \\(r=\\beta_0 - \\mu_0\\) and \\(K = r / (\\tilde{\\beta}+\\tilde{\\mu})\\). These parameters are usually called intrinsic growth rate and carrying capacity.\nA couple of comments are necessary: first of all, either \\(\\tilde{\\beta}\\) or \\(\\tilde{\\mu}\\) can be 0, but not both, otherwise there is no effect of crowding and \\(K\\) is not well-defined. Note also that, if \\(\\tilde{\\beta}&gt; 0\\), the birth rate \\(B(t)\\) would become negative if \\(N(t)\\) is too large, which does not make sense biologically. However, this does not cause mathematical problems and the biological nonsense would occur only at population levels not normally reached, so we neglect this problem.\nWe will generally assume that \\(r &gt; 0\\), so that also \\(K &gt; 0\\). In that case, the behaviour of solutions to that equation displays a first phase of exponential growth, followed by convergence to the limiting value \\(K\\). The general solution is \\[\nN(t) = \\frac{KN_0}{N_0 + (K-N_0)e^{-rt}}.\n\\]\nWhen \\(N_0 &lt; K/2\\), the resulting sigmoid curve have been called logistic curve, so that equation is also named logistic equation.\nThe logistic equation is extremely common in experimental biology. Below, data fitted to a logistic for micro-organisms.\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nsns.set_theme(\"notebook\", style=\"whitegrid\")\n\n# from CRAN gauseR gause_1934_book_f04.rda\nt = [1.0,2.0,3.0,4.0,5.0,5.0,6.0,6.0]\nN = [ 22,129,334,374,376,356,397,367]\nK = 375\nr = 2.309\nN0 = 2.0\ntt = np.linspace(0,6,1000)\nlogistic = K*N0/(N0 + (K-N0)*(np.exp(-r*tt)))\n\nplt.axhline(y=K,color='k',linestyle='--',linewidth=1.0)\nplt.plot(tt, logistic,label='$\\\\frac{K N_0}{N_0+(K-N_0)e^{rt}}$')\nplt.plot(t,N,'.',markersize=16,label='Data')\nplt.xlabel('Days')\nplt.ylabel('Number of individuals')\nplt.title(f'Fit with K = {K}, $N_0$ = {N0}, r = {r}')\nplt.annotate(f'K = {K}',(1,K),ha='center',va='bottom')\nplt.legend()\n\n\n\n\n\nFitting of Paramecium caudatum data using the logistic model. Data from (Gause 1934, fig. 4).\nAs before, we can non-dimensionalize the equation. Now we select: \\[\n\\tau = rt, \\quad u = \\frac{N}{K},\n\\]\nso that \\(u=1\\) means that we are at carrying capacity. Substituting: \\[\n\\dot{u} = u(1-u).\n\\]\nThe new equation has no parameters. Thus, the general solution of the logistic equation is just \\[\nN(t) = K u(r t).\n\\]\nThe absence of parameters in the non-dimensional equation means that its dynamic is always the same, up to a rescaling. We say that all parametric solutions (as we vary \\(r\\) and \\(K\\)) are topologically equivalent. Here, we cannot expect bifurcation, as we shall see.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Logistic model</span>"
    ]
  },
  {
    "objectID": "03_Logistic.html#logistic-model",
    "href": "03_Logistic.html#logistic-model",
    "title": "3¬† Logistic model",
    "section": "",
    "text": "An increase of the population size produces a fertility decrease and a mortality increase; since resources are limited, if the population size exceeds some threshold level, the habitat cannot support the growth.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Logistic model</span>"
    ]
  },
  {
    "objectID": "03_Logistic.html#generalized-logistic-model",
    "href": "03_Logistic.html#generalized-logistic-model",
    "title": "3¬† Logistic model",
    "section": "3.2 Generalized logistic model",
    "text": "3.2 Generalized logistic model\nIn general, we can take \\(\\beta\\) and \\(\\mu\\) as generic functions of \\(N\\), thus: \\[\nr(N) = \\beta(N)-\\mu(N)\n\\]\nis also a generic function of \\(N\\). A generic growth model reads: \\[\nN' = r(N)N = f(N).\n\\]\nWe can implicitly solve this equation, in fact: \\[\nN(t) = N(0) e^{\\int_{t_0}^t r(N(s)) \\:\\mathrm{d}s}.\n\\]\nThus, if \\(N(0)&gt;0\\), then \\(N(t)&gt;0\\) for all time. This is important to verify, since a population cannot be negative. Here, we need no further restriction on \\(r(N)\\) so to verify the condition.\nHow about equilibria? We have that \\(E_0 = 0\\) is always an equilibrium. (Thus, since orbits cannot cross it, they positive stay positive foreover. This is another possible proof.) It is called extinction equilibrium. Its stability follows from: \\[\nf'(N)|_{N=E_0} = r'(0)\\cdot 0 + r(0) = r(0).\n\\]\nThe stability of \\(E_0\\) is given by \\(r(0)\\), which is called intrinsic growth rate. It is the growth rate we observe for very small population size. When \\(r(0)&gt;0\\) the extinction equilibrium is unstable. Equivalently, it is unstable when \\[\nR_0 = \\frac{\\beta(0)}{\\mu(0)} &gt; 1.\n\\]\nHow to capture a general logistic effect? By general we mean what we quoted above: a population increase should correspond to a decrease of fertility and an increase of mortality. Thus: \\[\nr'(N) &lt; 0, \\quad\\text{and}\\quad \\lim_{N\\to\\infty} r(N) &lt; 0.\n\\]\nThe second hypothesis avoids the existence of positive horizontal asymptotes. Biologically, a sufficiently large population has always a negative growth rate.\nWe can now study more equilibria, those corresponding to \\(r(N)=0\\).\n\nIf \\(r'(0)&lt;0\\), then by monotonicity we conclude that \\(r(N)&lt;0\\) for all \\(N\\), so \\(E_0=0\\) is the only equilibrium and the population is doomed.\nIf \\(r'(0)&gt;0\\), we have one additional (unique) equilibrium \\(N^*\\), that we denote by \\(K\\): that is, \\(r(K)=0\\). Since \\[\nf'(K) = r'(K)K + \\underbrace{r(K)}_{=0} = r'(K)K &lt; 0,\n\\]\n\nthis equilibrium is (globally) asymptotically stable. This equilibrium is also called carrying capacity.\nThe classic logistic equation has \\(r(N) = r(1-N/K)\\). The \\(\\theta\\)-logistic model (or Bernoulli model) has \\[\nr(N) = r\\bigl(1-(N/K)^\\theta\\bigr),\n\\]\nfor \\(\\theta&gt;0\\). There are a multitude of models for \\(r(N)\\), some we will explore in the assignments (see also figure below). Nonetheless, the above hypotheses always imply a sigmoid growth, when \\(N(0)\\in(0,K)\\).\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nsns.set_theme(\"notebook\", style=\"whitegrid\")\n\nN = np.linspace(0,1.2,1000)\n\nplt.plot(N, 1-N,label='$r=1-u$ (Verhulst)')\nplt.plot(N, 1-N**2.0,\n         label='$r=1-u^\\\\theta$ (Bernoulli, $\\\\theta=2$)')\nplt.plot(N, (1-N)/(1+2*N),\n         label='$r = \\\\frac{1-u}{1+\\\\alpha u}$ (Smith, $\\\\alpha=2$)')\nplt.plot(N, (np.exp(3*(1-N))-1)/(np.exp(3)-1),\n         label='$r = \\\\frac{e^{\\\\gamma u}-1}{e^\\\\gamma -1}$ (Ricker, $\\\\gamma = 3$)')\nplt.plot(N[1:], -np.log(N[1:]),label='$r = -\\\\log u$ (Gompertz)')\nplt.ylabel('Growth rate')\nplt.xlabel('N / K')\nplt.ylim([-0.5,1.5])\nplt.legend()\n\n\n\n\n\nDifferent growth model giving a logistic effect. We set \\(u=N/K\\), where \\(K\\) is the carrying capacity.\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nVerify that the above models are ‚Äúlogistic‚Äù, in the sense that they satisfy the hypotheses. Show that the Bernoulli model yields sigmoid solutions. (Hint: study \\(N'\\) and \\(N''\\).)\n\n\n\n\n\n\n\n\nExercise\n\n\n\nIntegrate the Gompertz model, \\(\\dot{u} = -u \\log u\\). This model is very common in the study of tumor cells proliferation. (Hint: set \\(u(t) = e^{w(t)}\\).)",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Logistic model</span>"
    ]
  },
  {
    "objectID": "03_Logistic.html#allee-effect",
    "href": "03_Logistic.html#allee-effect",
    "title": "3¬† Logistic model",
    "section": "3.3 Allee effect",
    "text": "3.3 Allee effect\n(Story time) The great auk was a bird that became extinct by the end of the 19th century. They were found at the northern Atlantic (Canada, Scotland, Iceland), usually on rocky islands. The overall population was composed by millions of individuals, before sailors and hunters started killing them for their meat, feathers, and oily fat.\n\n\n\nThe Great Auk (from Wikipedia)\n\n\nAs it could be expected, the auk population drastically reduced and the bird disappeared from many islands very quickly. Governments became aware of the situation in the 18th century, and some laws forbidden the hunt of the great auk, although with limited success.\nSome species like the great auk are not able to fully recover, even in the absence of predation (or with a small one). In order for a species to go to excintion with no predation, say for \\[\nN' = r(N)N,\n\\]\nwe would need \\(r(0)&lt;0\\), otherwise the equilibrium \\(N = 0\\) is not stable. But this would lead to no further equilibria under the hypotheses of general logistic growth. Hence, we can further generalize the hypotheses as follows: the function \\(r(N)\\) is such that\n\nthere exists \\(N_m &gt; 0\\) such that \\(r'(N) &gt; 0\\) for \\(N &lt; N_m\\) and \\(r'(N)&lt;0\\) otherwise;\n\\(r(N_m) &gt; 0\\);\n\\(\\lim_{N\\to\\infty} r(N) &lt; 0\\).\n\nTherefore, we certainly have the equilibrium \\(K &gt; N_m\\) (the carrying capacity). However, if \\(r(0)&lt;0\\), we also have another equilibrium at \\(T\\in(0,N_m)\\). In this case, we say that we have a strong Allee effect. On the other hand, for \\(r(0)&gt;0\\) there are no further equilibria, thus we have a weak Allee effect.\nLet us study the stability for \\(r(0)&lt;0\\). Remember that \\[\nf'(N) = r'(N)N + r(N).\n\\]\nWe have 3 equilibria:\n\n\\(N=E_0 = 0\\), which is asymptotically stable, since \\(f'(0) = r(0)&lt;0\\).\n\\(N=T\\in(0,N_m)\\), which is unstable, since \\(f'(T) = r'(T)T &gt; 0\\).\n\\(N=K &gt; N_m\\), which is asymptotically stable, since \\(f'(K) = r(K)K&lt;0\\).\n\nHence, we have the following result: if \\(N(0) &lt; T\\), then \\(N(t)\\to 0\\) (extinction), otherwise if \\(N(0)&gt;T\\), then \\(N(t)\\to K\\) (survival). We call \\(T\\) the threshold population for survival.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\nsns.set_theme(\"notebook\", style=\"whitegrid\")\n\nr = 0.5\nT,K = 0.2,1.0\nf = lambda t,N: r*(N/T-1)*(1-N/K)*N\n\nfig,ax = plt.subplots()\nu0 = [0.1,0.15,0.25,0.3,0.6,0.8]\nsol = solve_ivp(f,[0,8],u0,max_step=0.05)\nax.plot(sol.t,sol.y.T,lw=2.0)\nax.axhline(y=T,color='r',linestyle='--')\nax.grid(True)\nax.legend([f'$u_0 = {u}$' for u in u0])\nplt.show()\n\n\n\n\n\nSolutions of the equation \\(N'=r N(N/T-1)(1-N/K)\\). Note the threshold value for \\(N = T\\).\n\n\n\n\nThe Allee effect may be found in other situations, for instance when the predation rate is a non-linear function of \\(N\\).\n\n\n\n\nGause, G. F. 1934. The Struggle for Existence. William & Wilkins Company.\n\n\nIannelli, Mimmo, and Andrea Pugliese. 2014. An Introduction to Mathematical Population Dynamics. Vol. 79. Springer. https://strutture-provincia.primo.exlibrisgroup.com/permalink/39SBT_INST/ghkipk/alma991006883209706186.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Logistic model</span>"
    ]
  },
  {
    "objectID": "04_Predation.html",
    "href": "04_Predation.html",
    "title": "4¬† Predation models",
    "section": "",
    "text": "4.1 Generalist predation\nPredation is a fundamental topic in ecology and the main interaction between species. It must be understood in a broader sense: fishing, harvesting, hunting are all form of predation.\nThe simplest model of predation consists in formally increasing the mortality rate of \\(N\\) by an extra rate \\(aP^*\\), that is \\[\nN' = r(N)N - aP^* N = f(N),\n\\]\nwhere \\(P^*&gt;0\\) is the number of predators and \\(a&gt;0\\) is the attack rate or effective killing rate. We note that the death rate \\(aP^*N\\) is proportional to both the number of predators \\(P^*\\) and the number of preys \\(N\\), according to the law of mass action, as suggested by Volterra with the method of encounters. The idea is similar to collision theory for ideal gases.\nThis type of predation is also called generalist, in the sense that predators‚Äô survival does not depend on the survival of the prey population \\(N\\). If the preys go to extinction, the predator will hunt something else. A specialized predator, on the other hand, will suffer from a low level of \\(N\\), as we shall see next week.\nThe equilibria of the equation are: \\(N=E_0 = 0\\), as usual, and the zeros of: \\[\nr(N) = aP^*.\n\\]\nThe stability of the second equilibrium follows from \\[\nf'(N^*) = r'(N^*)N^* + r(N^*) - aP^* = r'(N^*)N^* - aP^* &lt; 0,\n\\]\nbecause \\(r'(N)&lt;0\\). So \\(N^*\\) is asymptotically stable.\nThe stability of \\(E_0\\) is similar: \\[\nf'(E_0) = r'(0)\\cdot 0 + r(0) - aP^* = r(0) - aP^* &gt; 0,\n\\]\nmeaning that it is unstable. Concluding:\nThe case \\(aP^* = r(0)\\) is delicate. We have a single equilibrium, \\(E_0=0\\), but \\(f'(E_0) = 0\\), so we cannot deduce the stability from the linearization (why?). By inspecting the sign of \\(E_0\\), we observe that \\(f(N)&lt;0\\) for \\(N&gt;0\\), so \\(E_0\\) is attractive. However, for \\(N&lt;0\\) we have that \\(f(N)&lt;0\\), thus it is repulsive (biologically, we do not care because \\(N&lt;0\\) is irrelevant.) This type of equilibrium is called saddle-node.\nThe point \\(aP^* = r(0)\\) is a bifurcation point, specifically a transcritical bifurcation (See Section B.6). Roughly speaking, the dynamic before and after the bifurcation point is topologically different: in one case we have one equilibrium, in the other 2 equilibria. Actually, we still have 2 equilibria for \\(aP^* &gt; r(0)\\), one being negative. Thus, what really happens is that as we increase \\(aP^*\\) the two curves of equilibria (\\(N=0\\) and \\(N=E_0\\)) crosses and swap stability.\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nsns.set_theme(\"notebook\", style=\"whitegrid\")\n\nwith plt.xkcd(scale=0.5):\n    fig, ax = plt.subplots()\n    sns.despine()\n    ax.xaxis.set_ticks([])\n    ax.yaxis.set_ticks([])\n\n    ax.plot([0,1],[0,0],'r-')\n    ax.plot([0,1,1.2],[1,0,0],'b-')\n    ax.plot([1,1.2],[0,-0.2],'r-',alpha=0.5)\n    ax.plot([1],[0],'k.',markersize=16)\n    ax.grid(False)\n    ax.set_xlabel('$P^*$')\n    ax.set_ylabel('$N^*$')\n    ax.set_xlim([0,1.2])\n\n    [N,P] = np.mgrid[0.01:1.2:30j,0.01:1.2:30j]\n    D = N*(1-N) - P*N\n    ax.quiver(P,N,np.zeros_like(D),D,alpha=0.5)\n    plt.show()\n\n\n\n\n\nBifurcation diagram for the model with generalist predation. In blue, the stable equilibrium, in red the unstable one. Beyond the bifurcation point (transcritical, in black) there is only one equilibrium.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Predation models</span>"
    ]
  },
  {
    "objectID": "04_Predation.html#generalist-predation",
    "href": "04_Predation.html#generalist-predation",
    "title": "4¬† Predation models",
    "section": "",
    "text": "If \\(aP^* &gt; r(0)\\), then we have no additional equilibria, because \\(r'(N)&lt;0\\).\nIf \\(aP^* &lt; r(0)\\), then there exists a second equilibrium \\(N^* &gt; 0\\).\n\n\n\n\n\n\nIf \\(aP^* &gt; r(0)\\), we have one stable equilibrium \\(E_0=0\\), thus the population will go to extinction. In fact, the predation level is high.\nIf \\(aP^* &gt; r(0)\\), \\(E_0=0\\) is unstable but we have another equilibrium \\(0&lt;N^*&lt;K\\) asymptotically stable. Thus, the predation is sustainable.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Predation models</span>"
    ]
  },
  {
    "objectID": "04_Predation.html#holling-type-predation-models",
    "href": "04_Predation.html#holling-type-predation-models",
    "title": "4¬† Predation models",
    "section": "4.2 Holling type predation models",
    "text": "4.2 Holling type predation models\nMore generally, we define the predation as: \\[\nN' = r(N)N - \\pi(N) P^*,\n\\]\nwhere \\(\\pi(N)\\) is called functional response. The functional response can be interpreted as follows: \\(\\pi(N)\\Delta t\\) is the number of preys killed in \\(\\Delta t\\) units of time by a single predator.\nFor the simplest model, we have \\(\\pi(N) = aN\\). However, this is not realistic, because for very large \\(N\\) the predation rate cannot grow undefinitely, it must reach some limiting value. For instance, a predator needs some time to consume the prey, and this time cannot be reduced below a certain limit. Similarly, it is reasonable that at low density of preys, say when \\(N\\) is small, predation is harder.\nIn general, we can assume that:\n\n\\(\\pi(0)=0\\), no preys no predation, and\n\\(\\pi'(N)&gt;0\\), the more preys, the higher the predation rate.\n\nHolling (1965) proposed the following types of functional responses:\n\nHolling type I \\[\n  \\pi(N) = \\begin{cases} aN, & 0 \\le N \\le N^*, \\\\ aN^*, & N &gt; N^*. \\end{cases}\n  \\]\nThis model is exactly like the linear one, but it assumes that for \\(N &gt; N^*\\) we have constant predation rate \\(aN^*\\). The parameter \\(a\\) is called attack rate, and it measures, after an encounter between prey and predator, the success rate of predation. Note that the function \\(\\pi(N)\\) is not \\(\\mathcal{C}^1\\) but it is Lipschitz.\nHolling type II\n\\[\n  \\pi(N) = \\dfrac{a N}{1 + a \\tau N},\n  \\]\nwith \\(a, \\tau&gt;0\\). This is a smooth version of Holling type I. For small \\(N\\), \\(\\pi(N) \\approx aN\\), so the meaning of \\(a\\) is the same as above. For large \\(N\\), we have that \\(\\pi(N)\\to 1/\\tau =: \\alpha\\), which is called maximum killing rate. That is, \\(\\alpha\\) is the number of preys killed by one predator in a unit of time, when the number of preys is very large. Alternatively, we can interpret \\(\\tau\\) as the time required by the predator to consume the prey.\nHolling type III \\[\n  \\pi(N) = \\dfrac{\\alpha N^\\theta}{\\nu^\\theta + N^\\theta},\n  \\]\nwith \\(\\alpha,\\nu&gt;0\\) and \\(\\theta &gt; 1\\). The last type also accounts for a lower predation rate as low density of preys. In fact, for small \\(N\\) and \\(\\theta &gt; 1\\) we have \\(\\pi'(0)=0\\). For \\(N=\\nu\\), \\(\\pi(\\nu) = \\frac{\\alpha}{2}\\), and for \\(N\\to\\infty\\) we have \\(\\pi(N)\\to \\alpha\\), the maximum killing rate. Thus, \\(\\nu\\) is the number of preys at which the killing rate is exactly half of the maximum one.\n\n\n\n\nDifferent types of Holling predations\n\n\nIt is possible to justify Holling type II in a few ways. One, we will see in the completely different context of enzymatic reactions. A simpler one is as follows. In a time \\(T\\), a single predator will spend \\(1/\\pi(N)\\) time in hunting. (Always keep in mind the bathtub example!) But the total time splits into \\(T_s\\), the time spent seeking for a prey, and \\(\\tau\\), the time needed to consume a prey. The time \\(\\tau\\) is fixed, no matter how large is \\(N\\). The time \\(T_s\\), however, is exactly \\((a N)^{-1}\\), because the more preys the easier is to catch them. Putting all together we have: \\[\n\\frac{1}{\\pi(N)} = T = \\tau + T_s = \\tau + \\frac{1}{a N},\\quad\\Rightarrow\\quad \\pi(N) = \\frac{a N}{1+a\\tau N}.\n\\]",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Predation models</span>"
    ]
  },
  {
    "objectID": "04_Predation.html#spruce-budworm-model",
    "href": "04_Predation.html#spruce-budworm-model",
    "title": "4¬† Predation models",
    "section": "4.3 Spruce budworm model",
    "text": "4.3 Spruce budworm model\n\n\n\nThe effect of the spruce budworm on a forest.\n\n\nThe spruce budworm is an insect that feeds on needles of balsam fir trees (see this website). If needles are removed, the tree dies. Historical data in Canada evergreen forests, where the budworm is present, shows that in most years the budworm density is very low. However, in a outbreak year, the budworm population spikes and can kill up to 80% of mature trees in the forest. The period of the outbreak is roughly 30-70 years.\nThe spruce budworm model has been introduced in Ludwig et al. (1978). They proposed a system of 3 variables:\n\n\\(N(t)\\), the budworm density,\n\\(S(t)\\), the habitat space for larvae, and\n\\(E(t)\\), a measure of food energy reserves available to the budworm.\n\nWe focus here on the equation for \\(N(t)\\). In fact, \\(S(t)\\) and \\(E(t)\\) will vary very slowly compared to \\(N(t)\\), thus they can be assumed constant. (This argument can be made rigorous, as we shall see for enzymatic reactions.)\nThe equation for \\(N(t)\\) reads as follows: \\[\nN' = rN\\Bigl(1-\\frac{N}{K}\\Bigr) - \\frac{\\alpha P^* N^2}{\\nu^2 + N^2}.\n\\]\nThe first term is the logistic growth. The second one is the predation of the budworms due to birds. This is Holling type III. Note that the parameters are fixed numbers (they do not depend on time), but they will depend on \\(S\\) and \\(E\\), somehow. Thus, it will be interesting to see what happens to the system as we change them.\nSince we have too many parameters, we proceed with non-dimensionalization. Here we select: \\[\n\\tau = \\frac{t}{T}, \\quad u = \\frac{N}{\\nu},\n\\]\nfor some \\(T &gt; 0\\) to be selected. We have: \\[\n\\frac{\\nu}{T} \\dot{u} = r \\nu u \\Bigl(1 - \\frac{\\nu u}{K} \\Bigr) - \\frac{\\alpha P^* u^2}{1 + u^2},\n\\]\nthus by selecting \\[\nT = \\frac{\\nu}{\\alpha P^*}, \\quad \\rho = \\frac{r\\nu}{\\alpha P^*}, \\quad q = \\frac{K}{\\nu}\n\\]\nwe arrive at \\[\n\\dot{u} = \\rho u \\Bigl(1 - \\frac{u}{q} \\Bigr) - \\frac{u^2}{1 + u^2},\n\\]\nwhere we are left with only 2 parameters:\n\n\\(\\rho\\) is proportional to the intrinsic growth rate, while\n\\(q\\) is the carrying capacity normalized to the half-saturation population \\(\\nu\\).\n\n\n4.3.1 Equilibria and stability\nAs usual, we start by looking for equilibria of the system, that is solutions of \\[\n\\rho u \\Bigl(1 - \\frac{u}{q} \\Bigr) - \\frac{u^2}{1 + u^2} = 0.\n\\]\nWe have that \\(u=0\\) is an equilibrium. The others solve the equation \\[\n\\rho \\Bigl(1 - \\frac{u}{q} \\Bigr) - \\frac{u}{1 + u^2} = 0,\n\\]\nwhich would lead to a 3rd-order polynomial equation, thus we can expect up to 3 real solutions. An analytical approach is not practical. However, equilibria are intersections of the two curves \\(f(u)\\) and \\(g(u)\\) where \\[\nf(u) = \\rho \\Bigl(1 - \\frac{u}{q} \\Bigr), \\quad g(u) = \\frac{u}{1 + u^2}.\n\\]\nThe function \\(f(u)\\) represents the per capita growth rate of \\(u\\), whereas \\(g(u)\\) is the per capita death rate due to predation. Thus, solutions of the equation are equilibria of the system. Since the function \\(g(u)\\) does not depend on any parameters, we can fix it and simply change \\(f(u)\\), which is a segment.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nsns.set_theme(\"notebook\", style=\"whitegrid\")\n\nu  = np.linspace(0,12,1000)\n\nfig,axs = plt.subplots(1,2,figsize=(9,3))\nq = 4\nuq = np.linspace(0,q,2)\naxs[0].plot(u, u/(1+u**2))\nfor rho in np.linspace(0.2,0.8,10):\n    pr = np.polynomial.Polynomial([q*rho,-q-rho,q*rho,-rho])\n    rr = np.real(np.array([r for r in pr.roots() if np.isreal(r) and r &gt;= 0]))\n    axs[0].plot(uq,rho*(1-uq/q),'k',lw=0.5)\n    axs[0].plot(rr,rho*(1-rr/q),'k.')\n\nq = 10\nuq = np.linspace(0,q,2)\naxs[1].plot(u, u/(1+u**2))\nfor rho in np.linspace(0.1,0.8,6):\n    pr = np.polynomial.Polynomial([q*rho,-q-rho,q*rho,-rho])\n    rr = np.real(np.array([r for r in pr.roots() if np.isreal(r) and r &gt;= 0]))\n    axs[1].plot(uq,rho*(1-uq/q),'k',lw=0.5)\n    axs[1].plot(rr,rho*(1-rr/q),'k.')\n\np = np.polynomial.Polynomial([q,0,-q,2])\nrho = lambda u: q*u/((1+u**2)*(q-u))\nbif_u   = np.real(np.array([r for r in p.roots() if np.isreal(r) and r &gt;= 0]))\nbif_rho = rho(bif_u)\nfor r,u in zip(bif_rho,bif_u):\n    axs[1].plot(uq,r*(1-uq/q),'r',lw=1.0)\n    axs[1].plot(u,r*(1-u/q),'r.',markersize=8)\n\nfor ax in axs:\n    ax.grid(False)\n    sns.despine()\n    ax.xaxis.set_ticks([])\n    ax.yaxis.set_ticks([])\n    ax.set_xlabel('u',loc='right')\n    ax.set_xlim(0,12)\n    ax.set_ylim(0,1)\n\naxs[1].yaxis.set_ticks(bif_rho)\naxs[1].yaxis.set_ticklabels(['$\\\\rho_2$','$\\\\rho_1$'])\n\nplt.show()\n\n\n\n\n\nEquilibria for the spruce budworm model. On the left, the case of small \\(q\\), on the right the case of large \\(q\\).\n\n\n\n\nWe can see from the figure that when \\(q\\) is small, we only have a single equilibrium. When \\(q\\) is large, we can have 1, 2, or 3 additional equilibria, depending on \\(\\rho\\): given \\(\\rho_1 &lt; \\rho_2\\) we have:\n\nIf \\(\\rho &lt; \\rho_1\\), then there exists a single equilibrium \\(u^*_1 \\ll q\\). This is the refuge equilibrium (low population). The equilibrium is globally stable, because \\(f(u)-g(u)&gt;0\\) for \\(u&lt;u_1^*\\) and \\(f(u)-g(u)&lt;0\\) for \\(u&gt;u_1^*\\).\nIf \\(\\rho &gt; \\rho_2\\), then there exists a single equilibrium \\(u^*_3\\) close to \\(q\\). This is the outbreak equilibrium (large population). The equilibrium is globally stable, because \\(f(u)-g(u)&gt;0\\) for \\(u&lt;u_3^*\\) and \\(f(u)-g(u)&lt;0\\) for \\(u&gt;u_3^*\\).\nIf \\(\\rho_1 &lt; \\rho &lt; \\rho_2\\), then there exist 3 equilibria \\(u^*_1 &lt; u^*_2 &lt; u^*_3\\). The stability of \\(u^*_1\\) and \\(u^*_3\\) is as above, but now locally. The equilibrium \\(u_2^*\\) is unstable.\n\nThe case 3. is the most interesting one. If \\(u(0)&lt;u_2^*\\), then \\(u\\to u_1^*\\), otherwise \\(u\\to u_3^*\\). This is again a threshold effect.\n\n\n4.3.2 Bifurcation diagram\nAs we change \\(\\rho\\), we may have a different number of equilibria. Thus, there must be some bifurcation occurring. The equilibria are on the curve defined by \\[\nh(u,\\rho) = \\rho \\Bigl(1-\\frac{u}{q}\\Bigr) - \\frac{u}{1+u^2} = 0.\n\\]\nThe function \\(h(u,\\rho)\\) is smooth in \\(\\rho\\), since \\(\\partial_\\rho h \\neq 0\\) for \\(u\\in[0,q)\\). Thus we can write \\(\\rho\\) as a function of \\(u\\): \\[\n\\rho(u) = \\frac{q u}{(1+u^2)(q-u)}.\n\\]\nSince \\(h(u,\\rho(u))=0\\), the curve \\((u,\\rho(u))\\) with \\(u\\in[0,q)\\) defines a curve of equilibria, shown below:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nsns.set_theme(\"notebook\", style=\"whitegrid\")\n\nq = 12 #3*np.sqrt(3)\nu = np.linspace(0,0.9*q,1000)\nrho = lambda u: q*u/((1+u**2)*(q-u))\n\np = np.polynomial.Polynomial([q,0,-q,2])\nbif_u   = np.array([r for r in p.roots() if np.isreal(r) and r &gt;= 0])\nbif_rho = rho(bif_u)\n\nrho_neg = p(u) &lt; 0\n\nwith plt.xkcd(scale=0.5):\n    fig, ax = plt.subplots()\n    sns.despine()\n    ax.xaxis.set_ticks([])\n    ax.yaxis.set_ticks([])\n\n    ax.axhline(y=q,color='k',linestyle='--',linewidth=1.0)\n    ax.plot(rho(u),0*u,'r')\n    ax.plot(rho(u), u, 'b')\n    ax.plot(rho(u)[rho_neg], u[rho_neg], 'r')\n    ax.plot([0],[0],'k.',markersize=16)\n    ax.plot(bif_rho,bif_u,'k.',markersize=16)\n    ax.grid(False)\n    ax.set_xlabel('$\\\\rho$')\n    ax.set_ylabel('$u^*$')\n    ax.fill_between(rho(u), q, where=rho_neg, facecolor='gray', alpha=.2)\n    \n    if len(bif_u) &gt; 0:\n        ax.annotate('  $\\\\rho_2$',(bif_rho[0],bif_u[0]),ha='left',va='center')\n        ax.annotate('$\\\\rho_1$  ',(bif_rho[1],bif_u[1]),ha='right',va='center')\n        ax.annotate('$q$',(rho(u).max(),q),ha='left',va='top')\n\n    plt.show()\n\n\n\n\n\nBifurcation diagram with respect to \\(\\rho\\) for the spruce budworm system, for \\(q=12\\). In blue, the stable equilibrium, in red the unstable one. The shaded region is bistable.\n\n\n\n\nThe plot above is a bifurcation diagram. We can interpret it as follows: given \\(\\rho=\\bar{\\rho}\\), the equilibria are those corresponding to the intersections between \\(\\rho=\\bar{\\rho}\\) and the curve \\(\\rho=\\rho(u)\\). So, for \\(\\rho &lt; \\rho_1\\) we have one equilibrium, for \\(\\rho_1 &lt; \\rho &lt; \\rho_3\\) we have 3, and for \\(\\rho &gt; \\rho_3\\) we have one.\nFor \\(\\rho = \\rho_1\\) or \\(\\rho = \\rho_2\\), we have a tangent bifurcation (See Section B.5). As we can see, the curve of equilibria is always smooth, with no branching or crossing on another curve of equilibria, like in the case of transcritical bifurcation. However, there is a change in stability: in fact, the branch between \\(\\rho_1\\) and \\(\\rho_2\\) corresponds to \\(u_2^*\\), which is unstable.\nWhen varying also \\(q\\), the tangent bifurcations points \\(\\rho_1\\) and \\(\\rho_2\\) moves as well. In particular, as \\(q\\) is reduced, the two bifurcation points will get closer until they meet for \\(q=3\\sqrt{3}\\). This point is another bifurcation, called cusp bifurcation. Beyond this point, the system is never bistable.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nsns.set_theme(\"notebook\", style=\"whitegrid\")\n\nrho = lambda u,q: q*u/((1+u**2)*(q-u))\nQ = []\nR = []\nfor q in np.arange(1,15,0.01):\n    p = np.polynomial.Polynomial([q,0,-q,2])\n    bif_u = np.array([r for r in p.roots() if np.isreal(r) and r &gt;= 0])\n    if not len(bif_u): continue\n    Q.append(q)\n    R.append([rho(u,q) for u in bif_u])\n\nR = np.array(R)\nQ = np.array(Q)\n\nq_cusp = 3*np.sqrt(3)\nu_cusp = np.sqrt(3)\nr_cusp = rho(u_cusp,q_cusp)\n\nfig, ax = plt.subplots()\nax.plot(Q,R.min(axis=1),'r-')\nax.plot(Q,R.max(axis=1),'r-')\nax.plot(q_cusp, r_cusp, 'k.',markersize=16)\nax.fill_between(Q,R.min(axis=1),R.max(axis=1),color='r',alpha=0.2)\nax.set_xlim([2,None])\nax.set_ylim([0.2,0.8])\nax.set_ylabel('$\\\\rho$')\nax.set_xlabel('$q$')\n\nax.annotate('  Cusp point',(q_cusp,r_cusp),ha='left')\nax.annotate('Bistable region',(12,0.45),ha='center')\nax.annotate('Refuge region',(10,0.25),ha='center')\nax.annotate('Outbreak region',(12,0.65),ha='center')\nplt.show()\n\n\n\n\n\nBifurcation diagram with respect to \\((q,\\rho)\\) for the spruce budworm system. The shaded region is bistable. The cusp point is a bifurcation point of co-dimension 2. It occurs for \\(q=3\\sqrt{3}\\).\n\n\n\n\nIf we plot the surface of equilibria in the space \\((q,\\rho,u^*)\\), as the solution of the equation \\(h(u,\\rho,q)=0\\), we obtain the plot below. This plot clearly shows the bistable region and the threshold value (in red).\n\n\nCode\nimport numpy as np\nimport pyvista as pv\npv.set_jupyter_backend('static')\n\nn = 50\n\nbnd = np.array([[0.1,0.1,0],[20,1.0,20]])\ngrid = pv.ImageData(dimensions=(n,n,n),\n                    spacing=(bnd[1,:]-bnd[0,:])/n,\n                    origin=bnd[0,:])\nQ,R,U = grid.points[:,0],grid.points[:,1],grid.points[:,2]\nvals = R*(1-U/Q)-U/(1+U**2)\n\nsols = U.copy()\n\ngrid.point_data['sols'] = sols\nout = grid.contour(1,scalars=vals,rng=[0,0])\nout.compute_normals(inplace=True,auto_orient_normals=True)\nout.point_data['normals_u'] = out.point_data['Normals'][:,2] &gt; 0\nplotter = pv.Plotter()\nplotter.add_mesh(out,scalars='normals_u',cmap=['red','blue'],\n                 smooth_shading=True)\nplotter.set_scale(xscale=1, yscale=20, zscale=0.5)\nplotter.remove_scalar_bar()\nplotter.add_axes(xlabel='q',ylabel='rho',zlabel='u')\nplotter.camera_position = [\n    (41, -11, 24),\n    (11, 11, 1.7),\n    (-0.33,0.4,0.84),\n]\nplotter.show()\n\n\n\n\n\nBifurcation diagram with respect to \\((q,\\rho)\\) in 3D.\n\n\n\n\n\n\n4.3.3 Hysteresis\nNow that we have the bifurcation diagram, we can use it to find the periodic outbreaks. Remember that \\(\\rho\\) and \\(q\\) are functions of \\(S\\) and \\(E\\), but they vary very slowly compared to \\(N(t)\\). On the other hand, when \\(N\\) is at equilibrium, it does not vary anymore, so even slow and small variations of \\(E\\) and \\(S\\) could matter.\nSuppose to start with small \\(\\rho\\), say \\(\\rho &lt; \\rho_1\\). The spruce budworms is at refuge state \\(u_1^*\\). Now we slowly increase \\(\\rho\\). The equilibrium \\(u_1^*(\\rho)\\) will only slightly increase. Once we react the point \\(\\rho=\\rho_2\\), the equilibrium \\(u_1^*\\) disappears, so \\(u\\to u_3^*\\) (outbreak), the only other stable equilibrium. Note the the outbreak is fast, even for a small change of \\(\\rho\\). For this reason, the tangent bifurcation is a catastrophic bifurcation. As we keep increasing \\(\\rho\\), the outbreak equilibrium keeps increasing, but again slowly.\nSince \\(\\rho\\) is the intrinsic growth rate, we can assume that \\(\\rho\\) will now start to decrease, because there are too many spruce budworm consuming the resources. As we go back, decreasing \\(\\rho\\), we follow a specular path, jumping at \\(\\rho=\\rho_1\\) from the outbreak to the refuge equilibrium. We go back to the original situation.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nsns.set_theme(\"notebook\", style=\"whitegrid\")\nq = 12\nu = np.linspace(0,0.9*q,1000)\nrho = lambda u: q*u/((1+u**2)*(q-u))\n\np = np.polynomial.Polynomial([q,0,-q,2])\nbif_u   = np.array([r for r in p.roots() if np.isreal(r) and r &gt;= 0])\nbif_rho = rho(bif_u)\n\nrho1 = bif_rho.min()\nrho2 = bif_rho.max()\n\np1 = np.polynomial.Polynomial([q*rho1,-q-rho1,q*rho1,-rho1])\np2 = np.polynomial.Polynomial([q*rho2,-q-rho2,q*rho2,-rho2])\n\nu0 = p1.roots().min()\nu1 = bif_u.min()\nu2 = p2.roots().max()\nu3 = bif_u.max()\n\nrho_neg = p(u) &lt; 0\n\nwith plt.xkcd(scale=0.5):\n    fig, ax = plt.subplots()\n    sns.despine()\n    ax.xaxis.set_ticks([])\n    ax.yaxis.set_ticks([])\n\n    ax.plot(rho(u), u, 'k', lw=1.0)\n    for lb,ub in [[u0,u1],[u2,u3]]:\n        uu = np.linspace(lb,ub,100)\n        ax.plot(rho(uu),uu, 'r')\n    for lb,ub in [[u1,u2],[u3,u0]]:\n        ax.plot([rho(lb),rho(ub)],[lb,ub], 'r')\n    ax.plot([rho(.7*u0+.3*u1)],[.7*u0+.3*u1],'r&gt;')\n    ax.plot([rho(0.5*(u2+u3))],[0.5*(u2+u3)],'r&gt;')\n    ax.plot([rho2],[0.5*(u1+u2)],'r^')\n    ax.plot([rho1],[0.5*(u3+u0)],'rv')\n    ax.grid(False)\n    ax.set_xlabel('$\\\\rho$')\n    ax.set_ylabel('$u^*$')\n    \n    ax.annotate('  $\\\\rho_2$',(bif_rho[0],bif_u[0]),ha='left',va='center')\n    ax.annotate('$\\\\rho_1$  ',(bif_rho[1],bif_u[1]),ha='right',va='center')\n    ax.annotate('$q$',(rho(u).max(),q),ha='left',va='top')\n\n    plt.show()\n\n\n\n\n\nBifurcation diagram with respect to \\(\\rho\\) for the spruce budworm system. In blue, the stable equilibrium, in red the unstable one. The shaded region is bistable.\n\n\n\n\nThe result is a hysteresis loop, which could explain the periodic outbreaks.\nWe will analyze this model more in depth during the Lab session.\n\n\n\n\nHolling, Crawford Stanley. 1965. ‚ÄúThe Functional Response of Predators to Prey Density and Its Role in Mimicry and Population Regulation.‚Äù The Memoirs of the Entomological Society of Canada 97 (S45): 5‚Äì60. https://doi.org/10.4039/entm9745fv.\n\n\nIannelli, Mimmo, and Andrea Pugliese. 2014. An Introduction to Mathematical Population Dynamics. Vol. 79. Springer. https://strutture-provincia.primo.exlibrisgroup.com/permalink/39SBT_INST/ghkipk/alma991006883209706186.\n\n\nLudwig, Donald, Dixon D Jones, Crawford S Holling, et al. 1978. ‚ÄúQualitative Analysis of Insect Outbreak Systems: The Spruce Budworm and Forest.‚Äù Journal of Animal Ecology 47 (1): 315‚Äì32. https://doi.org/10.2307/3939.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Predation models</span>"
    ]
  },
  {
    "objectID": "05_LotkaVolterra.html",
    "href": "05_LotkaVolterra.html",
    "title": "5¬† Lotka-Volterra model",
    "section": "",
    "text": "5.1 Prey-predator models\nIn all previous models, predation was only at cost of preys. Generalist predators, like humans, can survive even in absence of preys. Thus, \\(P^*\\) can be assumed constant.\nWhat if the predator is a specialist, that is it can only hunt a specific prey? When the prey population gets too low, also survival of the predator could be at risk. One equation is not enough.\nA classic example is the Hare-Lynx dynamics, shown below:\nThe data actually shows pelt-trading data taken from the Hudson‚Äôs Bay Company Elton and Nicholson (1942). We observe that data is oscillatory with a 10-year period. How is that?",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Lotka-Volterra model</span>"
    ]
  },
  {
    "objectID": "05_LotkaVolterra.html#lotka-volterra-model",
    "href": "05_LotkaVolterra.html#lotka-volterra-model",
    "title": "5¬† Lotka-Volterra model",
    "section": "5.2 Lotka-Volterra model",
    "text": "5.2 Lotka-Volterra model\n(Story time) During the 1920s, the biologist Umberto D‚ÄôAncona (who married Volterra‚Äôs daughter) noticed that during World War I, in his fishery data in the Adriatic Sea, the frequency of predation increased significantly. He could not explain this phenomenon, but it should have been somehow related to a decrease of fishery during the war. Furthermore, the data was oscillatory.\nIn 1926, Vito Volterra proposed a model for the prey-predator interaction. (After the publication on Nature, Alfred Lotka wrote a short communication, still on Nature, where he points out that his book ‚ÄúElements of Physical Biology‚Äù, published in 1924, covered the same model. See also the book Volterra (1932).) Suppose that\n\n\\(H(t)\\) is the population of preys over time, and\n\\(P(t)\\) is the population of predators over time.\n\nVolterra made the following hypotheses:\n\nThe growth rate for prey and predator is Malthusian;\nIn absence of predators, \\(H(t)\\) increases;\nIn absence of preys, \\(P(t)\\) decreases;\nIn a unit of time, the number of preys consumed by predators is \\(-a H(t)P(t)\\);\nIn a unit of time, successful predation increases the number of predators by \\(\\gamma a H(t)P(t)\\).\n\nThe Lotka-Volterra model readily follows: \\[\n\\left\\{\\begin{aligned}\nH' &= r H - a H P, \\\\\nP' &= -\\mu P + \\gamma a H P.\n\\end{aligned}\\right.\n\\]\nWe can also rewrite the system as: \\[\n\\left\\{\\begin{aligned}\nH' &= H F(P), \\\\\nP' &= P G(H),\n\\end{aligned}\\right.\n\\]\nfor \\(F(P) = r - aP\\) and \\(G(H) = -\\mu + \\gamma a H\\). The system is in Kolmogorov form: orbits cannot cross the axes \\(H=0\\) and \\(P=0\\), so trajectories starting in the positive quadrant remain in the positive quadrant.\n\n5.2.1 Equilibria\nWe have the trivial equilibrium \\((H,P)=(0,0)=E_0\\), and another one satisfying: \\[\n\\left\\{\\begin{aligned}\nr - a P &= 0, \\\\\n-\\mu + \\gamma a H &= 0.\n\\end{aligned}\\right.\n\\]\nSo, \\(E^* = (H^*,P^*) = (\\frac{\\mu}{\\gamma a}, \\frac{r}{a})\\). It is positive, thus biologically feasible.\nThe stability follows from the Jacobian (see Section A.5.4.2): \\[\nJ(H,P) = \\begin{pmatrix} r - aP & -aH \\\\ \\gamma a P & -\\mu + \\gamma a H \\end{pmatrix}.\n\\]\nFor \\(E_0\\), we have \\(J(0,0)\\) diagonal with eigenvalues \\(r\\) and \\(-\\mu\\). It is a saddle.\nFor \\(E^*\\), we have \\[\nJ(H^*,P^*) = \\begin{pmatrix} 0 & - a H^* \\\\ \\gamma a P^* & 0 \\end{pmatrix}\n\\]\nwhich has eigenvalues with zero real part, because the determinant is positive but the trace is zero. Thus, we have a center. The solutions of the linear equation are periodic with a period \\[\nT_\\mathrm{linear} = \\frac{2\\pi}{\\sqrt{\\det J(H^*,P^*)}} = \\frac{2\\pi}{\\sqrt{\\mu r}}.\n\\]\nUnfortunately, the linearization does not help to conclude anything for the non-linear case.\n\n\n5.2.2 Phase portrait\nOn the other hand, the system is in Hamiltonian form (see Section A.6.1). To find the Hamiltonian, here denoted by \\(\\mathcal{H}(H,P)\\) to avoid confusion with \\(H(t)\\), we notice that: \\[\n\\frac{\\mathrm{d}H}{\\mathrm{d}P} = \\frac{\\mathrm{d}H/\\mathrm{d}t}{\\mathrm{d}P/\\mathrm{d}t} = \\frac{H F(P)}{P G(H)},\n\\]\nand the ODE is in separable form, so we have: \\[\n\\int_{H_0}^{H(t)} \\frac{G(h)}{h}\\mathrm{d}h = \\int_{P_0}^{P(t)} \\frac{F(p)}{p}\\mathrm{d}p.\n\\]\nWe can easily carry out the integration: \\[\n\\Bigl. -\\mu\\ln h + \\gamma a h \\Bigr|_{H_0}^{H(t)} = \\Bigl. r \\ln p - a p \\Bigr|_{P_0}^{P(t)},\n\\]\nand we find that: \\[\n-\\mu \\ln H(t) + \\gamma a H(t) - r \\ln P(t) + a P(t) = -\\mu \\ln H_0 + \\gamma a H_0 - r \\ln P_0 + a P_0.\n\\]\nSince the right hand side is a constant, we have found that along the trajectory parallel to flow \\(\\mathrm{d}H/\\mathrm{d}P\\) the quantity \\[\n\\mathcal{H}\\bigl(H(t),P(t)\\bigr) = -\\mu \\ln H(t) + \\gamma a H(t) - r \\ln P(t) + a P(t)\n\\]\nremains constant. Thus, \\(\\mathcal{H}\\) is an Hamiltonian. We can check this again by noting that \\[\n\\begin{split}\n\\frac{\\mathrm{d}}{\\mathrm{d}t} \\mathcal{H}\\bigl(H(t),P(t)\\bigr) &= \\frac{\\partial \\mathcal{H}}{\\partial H} H' + \\frac{\\partial \\mathcal{H}}{\\partial P} P' \\\\\n&= \\Bigl( -\\frac{\\mu}{H} + \\gamma a \\Bigr) H F(P) + \\Bigl( -\\frac{r}{P} + a \\Bigr) P G(H) \\\\\n&= G(H) F(P) - F(P) G(H) = 0.\n\\end{split}\n\\]\nWe conclude that level sets of \\(\\mathcal{H}\\) are orbits of the system. Are they closed? Yes, because the function \\(\\mathcal{H}(H,P)\\to+\\infty\\) as \\((H,P)\\to+\\infty\\) or \\((H,P)\\to 0^+\\). Moreover, \\(\\mathcal{H}\\) is bounded from below, with minimum for \\((H,P)=(H^*,P^*)\\). In fact, since \\(E^*\\) is an equilibrium, \\(\\nabla\\mathcal{H}|_{E^*} = 0\\). Furthermore, the Hessian \\(\\nabla^2 \\mathcal{H}|_{E^*}\\) is positive definite.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nsns.set_theme(\"notebook\", style=\"whitegrid\")\n\nr,mu,a,gamma = 0.5,1.0,1.0,0.75\nHam = lambda H,P: -mu*np.log(H) + gamma*a*H - r*np.log(P) + a*P\nHs,Ps = mu/(gamma*a), r/a\n\n[HH,PP] = np.mgrid[0.01:4.5:100j,0.01:2.5:100j]\nlvls = [1.6,1.7,1.8,2.0,2.5]\n\nfig, ax = plt.subplots()\nsns.despine()\nax.xaxis.set_ticks([Hs])\nax.yaxis.set_ticks([Ps])\nax.xaxis.set_ticklabels(['$H^*$'])\nax.yaxis.set_ticklabels(['$P^*$'])\n\nax.contour(HH,PP,Ham(HH,PP),\n    levels=lvls,\n    colors=['k']*len(lvls),\n    linewidths=[1.0,1.0,1.0,2.0,1.0])\nax.plot([Hs],[Ps],'k.',markersize=16)\n\nax.grid(True)\nplt.show()\n\n\n\n\n\nLevel sets of the Hamiltonian function. All orbits are closed, and they correspond to periodic solutions of the Lotka-Volterra model.\n\n\n\n\nTherefore, the curve \\[\n\\mathcal{H}(H(t),P(t)) = \\mathcal{H}(H(0),P(0)),\n\\]\nis a closed orbit of the system. Depending on the initial conditions, we end up with a different orbit, but all of them are closed. It still can be that as the period of revolution around the orbit is not finite. This can happen for homoclinic orbits, for instance (see Figure¬†A.1.) We can exclude this option as follows: we define the angular coordinate as: \\[\n\\theta(t) = \\arctan\\Bigl( \\frac{P(t)-P^*}{H(t)-H^*} \\Bigr),\n\\]\nwhich measures the angle of the vector \\((H(t),P(t)) - E^*\\) with respect to the abscissa. The angular velocity is: \\[\n\\begin{split}\n\\theta'(t) &= \\frac{(H-H^*)P' - (P-P^*)H'}{(H-H^*)^2 - (P-P^*)^2} \\\\\n&= \\frac{\\gamma a P (H-H^*)^2 + a H(P-P^*)^2}{(H-H^*)^2 - (P-P^*)^2} \\\\\n&\\ge \\min\\{ \\gamma a \\min P, a \\min H \\} &gt; 0,\n\\end{split}\n\\]\nwhere \\(\\min P\\) and \\(\\min H\\) are taken on the curve and are certainly positive thanks to the logarithmic term in \\(\\mathcal{H}\\). Since the velocity is strictly positive, and the length of the curve is finite, the period must be finite as well.\nIn conclusion, all orbits are periodic. The temporal behavior of \\(P(t)\\) and \\(P(t)\\) is then periodic, as observed in data.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\nsns.set_theme(\"notebook\", style=\"whitegrid\")\n\nr,mu,a,gamma = 0.5,1.0,1.0,0.75\nf = lambda t,y: [ r*y[0]-a*y[0]*y[1], -mu*y[1] + gamma*a*y[1]*y[0] ]\n\ny0 = [3.0,2.0]\n\nsol = solve_ivp(f, [0,40], y0, rtol=1e-8, atol=1e-10)\n\nfig, ax = plt.subplots()\nax.plot(sol.t,sol.y.T)\nax.legend(['$H(t)$', '$P(t)$'])\nax.set_xlabel('time')\n\nplt.show()\n\n\n\n\n\nExample of solution of the Lotka-Volterra model\n\n\n\n\n\n\n5.2.3 The fishery problem\nNow we have a model that we can use to address D‚ÄôAncona question. If we regularly capture some predators and preys (both are fishes, we can catch both), their quantity is going to be proportional to the actual level of population. So, we can check the average value of \\(H\\) and \\(P\\).\nLet \\(T\\) be the period of an orbit. Then we‚Äô d like to compute \\[\n\\begin{aligned}\n\\bar{H} &= \\frac{1}{T}\\int_0^T H(t)\\,\\mathrm{d}t, \\\\\n\\bar{P} &= \\frac{1}{T}\\int_0^T P(t)\\,\\mathrm{d}t.\n\\end{aligned}\n\\]\nMoreover, \\[\n\\begin{aligned}\n\\int_0^T \\frac{H'}{H}\\,\\mathrm{d}t &= \\int_0^T \\frac{H F(P)}{H}\\,\\mathrm{d}t = \\int_0^T (r - a P) \\,\\mathrm{d}t = r T - a T \\bar{P}, \\\\\n\\int_0^T \\frac{P'}{P}\\,\\mathrm{d}t &= \\int_0^T \\frac{P G(H)}{P}\\,\\mathrm{d}t = \\int_0^T (-\\mu + \\gamma a H) \\,\\mathrm{d}t = -\\mu T + \\gamma a T \\bar{H}.\n\\end{aligned}\n\\]\nSince the solution is periodic, \\(H(T)=H(0)\\) and \\(P(t)=P(0)\\). Hence: \\[\n\\begin{aligned}\n\\int_0^T \\frac{H'}{H}\\,\\mathrm{d}t &= \\int_{H(0)}^{H(T)} \\frac{1}{H}\\,\\mathrm{d}H = \\ln \\frac{H(T)}{H(0)} = 0, \\\\\n\\int_0^T \\frac{P'}{P}\\,\\mathrm{d}t &= \\int_{P(0)}^{P(T)} \\frac{1}{P}\\,\\mathrm{d}P = \\ln \\frac{P(T)}{P(0)} = 0.\n\\end{aligned}\n\\]\nWe end up with the system: \\[\n\\left\\{\\begin{aligned}\nr - a \\bar{P} &= 0, \\\\\n-\\mu + \\gamma a \\bar{H} &= 0,\n\\end{aligned}\\right.\n\\]\nfrom which we deduce that \\[\n\\bar{H} = \\frac{\\mu}{\\gamma a} = H^*, \\quad \\bar{P} = \\frac{r}{a} = P^*.\n\\]\nThe average quantities match the value of the equilibrium, irrespective of the initial condition! This result is not obvious. Biologically, the system has an equilibrium, although non constant in time.\nNow, we introduce harvesting (fishing). Consider a parameter \\(E \\ge 0\\), called the effort. This is an abstract concept mutuated from bioeconomics. A small or zero effort means that we do not harvest much, a large effort is the opposite. We modify the Lotka-Volterra system as follows: \\[\n\\left\\{\\begin{aligned}\nH' &= r H - a H P - \\alpha E H, \\\\\nP' &= -\\mu P + \\gamma a H P - \\beta E P,\n\\end{aligned}\\right.\n\\]\nwhere \\(\\alpha, \\beta &gt; 0\\) are the returns per unit effort. That is, \\(\\alpha H \\Delta t\\) is the number of preys obtained in \\(\\Delta t\\) units of time per unit effort. If we rewrite the system as follows: \\[\n\\left\\{\\begin{aligned}\nH' &= H (r - \\alpha E - a P) = H (\\tilde{r} - a P), \\\\\nP' &= P (-\\mu P - \\beta E + \\gamma a H) = P(-\\tilde{\\mu} + \\gamma a H),\n\\end{aligned}\\right.\n\\]\nwith \\(\\tilde{r} = r - \\alpha E\\) and \\(\\tilde{\\mu} = \\mu + \\beta E\\), the system is identical to the original one, provided that \\(r &gt; \\alpha E\\). The new equilibrium \\(E^{**} = (H^{**}, P^{**})\\) reads: \\[\nH^{**} = \\frac{\\tilde{\\mu}}{\\gamma a} =\\frac{\\mu + \\beta E}{\\gamma a}, \\quad\nP^{**} = \\frac{\\tilde{r}}{a} =\\frac{r - \\alpha E}{a}.\n\\]\nThus, for \\(E &gt; 0\\) we are simply shifting the equilibrium. But the equilibrium is also the average value of \\(H(t)\\) and \\(P(t)\\) over the period, thus we should observe the same shift in the data. As a matter of fact, for \\(E &gt; 0\\), the number of prey diminishes, but the number of predator increases! During the World War I, fishery reduced, so \\(E\\) decreased: an increase of predators occurred, as observed by D‚ÄôAncona.\nIn conclusion, Volterra stated the following laws:\n\nthe fluctuations of the two species are periodic;\nthe average number of individuals over the period is conserved and does not depend on the initial datum;\n(also called Volterra‚Äôs principle) with \\(E&gt;0\\), the average number of preys increases and the number of predators decreases.\n\nVolterra‚Äôs principle is important in ecology. It somewhat paradoxically explain why, when applying non-specific killing (e.g., pesticides) some species may actually increase instead of decreasing.\n\n\n\n\nElton, Charles, and Mary Nicholson. 1942. ‚ÄúThe Ten-Year Cycle in Numbers of the Lynx in Canada.‚Äù The Journal of Animal Ecology, 215‚Äì44. https://doi.org/10.2307/1358.\n\n\nIannelli, Mimmo, and Andrea Pugliese. 2014. An Introduction to Mathematical Population Dynamics. Vol. 79. Springer. https://strutture-provincia.primo.exlibrisgroup.com/permalink/39SBT_INST/ghkipk/alma991006883209706186.\n\n\nVolterra, Vito. 1932. Le√ßons Sur La Th√©orie Math√©matique de La Lutte Pour La Vie. Edited by Gauthier-Villars et Cie √âditeurs. Paris. https://matematicaitaliana.sns.it/opere/427/.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Lotka-Volterra model</span>"
    ]
  },
  {
    "objectID": "06_Gause.html",
    "href": "06_Gause.html",
    "title": "6¬† Gause type models",
    "section": "",
    "text": "6.1 Gause type models\nThe Lotka-Volterra model is powerful, is spite of its simplicity. However, the model fails to predict many situations observed in experiments, such as those performed on bacteria by G.F. Gause in 1930s. He observed that with preys (Paramecium) and predators (Didinium) in the same test tube, first predators killed all preys, and then predators starved to death. No oscillations. This is never predicted by Lotka-Volterra original model. On the other hand, when he artificially introduced an immigration of preys into the tube, oscillations were present. Thus, stable oscillations may result from spatial structure of a population (metapopulation).\nHere we focus on an alternative scenario, where oscillations may arise from nonlinear trophic interaction, such as more complex growth rates for the preys, or more complex predation types.\nThe most general form for a prey-predator model, called of Gause type, is as follows: \\[\n\\left\\{\\begin{aligned}\nH' &= r(H)H - \\pi(H)P, \\\\\nP' &= -\\mu P + \\omega(H)P.\n\\end{aligned}\\right.\n\\]\nWe already analyzed \\(r(H)\\), the growth rate, and \\(\\pi(H)\\), the functional response. The function \\(\\omega(H)\\) is the numerical response, that is the number of new predators produced in a unit time by a single predator. Usually, it is an increasing function of \\(\\pi(H)\\), so \\(\\omega'(H)\\ge 0\\) and \\(\\lim_{H\\to\\infty}\\omega(H) &gt; \\mu\\) (with a lot of preys, predators survive.) An example is \\(\\omega(H) = \\gamma \\pi(H)\\).",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Gause type models</span>"
    ]
  },
  {
    "objectID": "06_Gause.html#lotka-volterra-model-with-logistic-growth",
    "href": "06_Gause.html#lotka-volterra-model-with-logistic-growth",
    "title": "6¬† Gause type models",
    "section": "6.2 Lotka-Volterra model with logistic growth",
    "text": "6.2 Lotka-Volterra model with logistic growth\nWe start from the simplest modification: \\[\n\\left\\{\\begin{aligned}\nH' &= r H (1-H/K) - a H P, \\\\\nP' &= -\\mu P + \\gamma a H P.\n\\end{aligned}\\right.\n\\]\nHere, preys grow according to the logistic equation, but predation is only linear.\n\n6.2.1 Well-posedness\nBefore moving into equilibria and phase portrait, we should check the well-posedness. The right hand side is polynomial, thus local well-posedness applies. It is also possible to show that trajectories are bounded, since the triangular region \\(\\mathcal{T}\\) formed by the axes and the line \\(P = m(q-H)\\), with \\(q, m\\) sufficiently large, is trapping the orbits.\nThe axes are both orbits, so they act as barriers. The diagonal side of \\(\\mathcal{T}\\) comes from the observation that for large \\(H\\) and \\(P\\), we have \\(H' &lt; 0\\) and \\(P'&lt;0\\). The actual computation is \\[\n\\begin{split}\n0 &gt; (H',P')\\cdot(m,1) &= mH' + P' \\\\\n&= mrH - mr \\frac{H^2}{K} -\\mu P + a(\\gamma-m) HP \\\\\n&= mr\\Bigl(1-\\frac{q}{K}\\Bigr)H + m\\frac{r}{K}(q-H)H -\\mu P + a(\\gamma-m) HP \\\\\n&= mr\\Bigl(1-\\frac{q}{K}\\Bigr)H + \\frac{r}{K}HP -\\mu P + a(\\gamma-m) HP \\\\\n&= mr\\Bigl(1-\\frac{q}{K}\\Bigr)H-\\mu P + \\Bigl( a(\\gamma-m) + \\frac{r}{K} \\Bigr) HP.\n\\end{split}\n\\]\nSince \\(H,P&gt;0\\), all coefficient must be negative so that the function is always negative. Thus we require: \\[\n\\begin{aligned}\na(\\gamma - m) + \\frac{r}{K} &&lt; 0 &\\Rightarrow&& m &gt; \\frac{r}{aK} + \\gamma, \\\\\n1 - \\frac{q}{K} &&lt; 0 &\\Rightarrow&& q &gt; K. \\\\\n\\end{aligned}\n\\]\n\n\n6.2.2 Equilibria\nAs usual, we have the extinction equilibrium \\(E_0 = (0,0)\\). Additionally, we have \\(E_K = (K,0)\\), called exclusion state, and \\(E^* = (H^*,P^*)\\), the coexistence equilibrium. The last solves the equation: \\[\n\\left\\{\\begin{aligned}\n0 &= r \\Bigl(1-\\frac{H^*}{K}\\Bigr) - a P^*, &\\Rightarrow P^* &= \\frac{r}{a}\\Bigl(1-\\frac{H^*}{K}\\Bigr), \\\\\n0 &= -\\mu + \\gamma a H^*, &\\Rightarrow H^* &= \\frac{\\mu}{\\gamma a}.\n\\end{aligned}\\right.\n\\]\nNote that \\(P^*\\) can be negative, if \\[\n1 - \\frac{H^*}{K} &lt; 0, \\quad\\Rightarrow\\quad H^* = \\frac{\\mu}{\\gamma a} &gt; K.\n\\]\nThus, we have 2 situations:\n\n\\(H^* &gt; K\\): we only have 2 equilibria, \\(E_0\\) and \\(E_K\\);\n\\(H^* &lt; K\\): we additionally have \\(E^*\\).\n\nThe stability follows from the Jacobian: \\[\nJ(H,P) = \\begin{pmatrix} r - 2rH/K - aP & -aH \\\\ \\gamma a P & -\\mu + \\gamma a H \\end{pmatrix}.\n\\]\nWe have: \\[\n\\begin{aligned}\nJ(0,0) &= \\begin{pmatrix} r & 0 \\\\ 0 & -\\mu \\end{pmatrix}, \\\\\nJ(K,0) &= \\begin{pmatrix} -r & -aK \\\\ 0 & -\\mu + \\gamma a K \\end{pmatrix}, \\\\\nJ(H^*,P^*) &= \\begin{pmatrix} - rH^*/K & -aH^* \\\\ \\gamma a P^* & 0 \\end{pmatrix}.\n\\end{aligned}\n\\]\nThus:\n\n\\(E_0\\) is always a saddle;\n\\(E_K\\) is asymptotically stable when \\(H^* &gt; K\\), otherwise is a saddle, as we can see from the eigenvalues on the diagonal;\n\\(E^*\\), when there exists, is asymptotically stable. In fact, \\(\\det J(H^*,P^*) = \\gamma a^2 H^* P^* &gt; 0\\) and \\(\\operatorname{tr} J(H^*,P^*) = -r H^* / K &lt; 0\\).\n\nIn conclusion, we have 2 cases: \\(H^* &gt; K\\) and \\(H^* &lt; K\\).\n\n\n6.2.3 Phase portrait: the case \\(H^* &gt; K\\)\nWe have 3 regions of the positive quadrant, as depicted below:\n\nOn the right of \\(H = H^*\\), orbits always flow to the left, since \\(H'&lt;0\\). Thus, starting from there, we enter into the region between \\(H=H^*\\) and \\(P=\\frac{r}{a}(1-\\frac{H}{K})\\). Again, orbits either flow into the triangle, or approach the only available equilibrium \\(E_K\\). Finally, within the triangle, orbits flow towards \\(E_K\\). We conclude that \\(E_K\\) is globally stable.\nIf \\(K \\le H^*\\), then for any \\((H(0),P(0))&gt;0\\) we have \\[\n\\lim_{t\\to\\infty}(H,P) = (K,0).\n\\]\nThe predators go to extinction, while preys survive.\n\n\n6.2.4 Phase portrait: the case \\(H^* &lt; K\\)\nWe have 4 regions, as follows:\n\nFirst, we show that we cannot have a limit cycle. Using Dulac‚Äôs criterium with factor \\(1/(HP)\\) we have: \\[\n\\operatorname{div}\\Bigl(\\frac{\\mathbf{f}(H,P)}{HP}\\Bigr) = \\frac{\\partial}{\\partial H}\\Bigl( \\frac{r(1-H/K)-aP}{P} \\Bigr) + \\frac{\\partial}{\\partial P}\\Bigl( \\frac{-\\mu + \\gamma a H}{H} \\Bigr) = -\\frac{r}{KP} &lt; 0.\n\\]\nSince \\(\\mathcal{T}\\) is a trapping region with no limit cycles, the \\(\\omega\\)-limit set is an equilibrium point. The only stable one is \\(E^*\\), a spiral, thus all trajectories should converge there.\nAn alternative proof is based on the Lyapunov function \\[\nV(H,P) = \\gamma(H - H^*\\log H) + (P - P^*\\log P).\n\\]\nWe recall that a Lyapunov function has the property that is positive, zero at the equilibrium, and (semi-)negative definite, then the equilibrium is stable. This is the case, since: \\[\n\\begin{aligned}\n\\frac{\\mathrm{d}}{\\mathrm{d}t}V\\bigl(H(t),P(t)\\bigr)\n&= \\gamma\\Bigl(1 - \\frac{H^*}{H}\\Bigr) H \\Bigl( r\\Bigl(1-\\frac{H}{K}\\Bigr) - aP \\Bigr) + \\Bigl(1 - \\frac{P^*}{P}\\Bigr) P(-\\mu + \\gamma a H) \\\\\n&= \\gamma a (H - H^* ) \\Bigl( \\frac{r}{a}\\Bigl(1-\\frac{H}{K}\\Bigr) - P \\Bigr) + \\gamma a (P - P^*)\\Bigl( -\\frac{\\mu}{\\gamma a} + H \\Bigr) \\\\\n&= \\gamma a (H - H^* ) \\Bigl( P^* - \\frac{r}{a}\\frac{H-H^*}{K} - P \\Bigr) + \\gamma a (P - P^*)( H - H^* ) \\\\\n&= - \\frac{r\\gamma}{K}(H-H^*)^2 \\le 0.\n\\end{aligned}\n\\]\nThus, orbits converge towards the line \\(H=H^*\\). But along this line we only have the equilibrium \\(E^*\\), thus \\(E^*\\) attracts all orbits.\nWe see here that the Lotka-Volterra system is not structurally stable, because a perturbation of it (adding the logistic term, which as little effect for \\(H\\) small) prevents the existence of a limit cycle.\n\n\n\n\nIannelli, Mimmo, and Andrea Pugliese. 2014. An Introduction to Mathematical Population Dynamics. Vol. 79. Springer. https://strutture-provincia.primo.exlibrisgroup.com/permalink/39SBT_INST/ghkipk/alma991006883209706186.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Gause type models</span>"
    ]
  },
  {
    "objectID": "07_RM.html",
    "href": "07_RM.html",
    "title": "7¬† Rosenzweig-MacArthur model",
    "section": "",
    "text": "7.1 Rosenzweig-MacArthur model\nWe see that a perturbation of the Lotka-Volterra model, obtained by adding the logistic term, destroys the structural stability of the original model: no limit cycles are present.\nIn 1963, Rosenzweig and MacArthur proposed a new model, of Gause-type, with a Holling type II predation. Again, this can bee seen as a small modification of the original system, thus we shall expect a similar dynamic for a wide range of parameters. The model reads as follows: \\[\n\\left\\{\\begin{aligned}\nH' &= r H (1-H/K) - a \\frac{H P}{1 + a T H}, \\\\\nP' &= -\\mu P + \\gamma a \\frac{H P}{1 + a T H}.\n\\end{aligned}\\right.\n\\]\nAll parameters are positive. The new parameter \\(T&gt;0\\) is the handling time, and the maximum killing rate is \\(1/T\\).",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Rosenzweig-MacArthur model</span>"
    ]
  },
  {
    "objectID": "07_RM.html#rosenzweig-macarthur-model",
    "href": "07_RM.html#rosenzweig-macarthur-model",
    "title": "7¬† Rosenzweig-MacArthur model",
    "section": "",
    "text": "7.1.1 Non-dimensionalization\nWe have 6 parameters in total, but we can reduce them by rescaling the quantities: \\[\nu = \\frac{H}{K}, \\quad v = \\frac{P}{\\gamma K}, \\quad \\tau = \\mu t.\n\\]\nThe rescaled equations are: \\[\n\\left\\{\\begin{aligned}\n\\dot{u} &= \\rho u(1-u) - \\alpha \\frac{\\delta uv}{1+\\delta u}, \\\\\n\\dot{v} &= -v + \\alpha \\frac{\\delta uv}{1+\\delta u},\n\\end{aligned}\\right.\n\\]\nwhere we defined the following quantities: \\[\n\\rho = \\frac{r}{\\mu}, \\quad \\alpha = \\frac{\\gamma}{T\\mu}, \\quad \\delta = a T K.\n\\]\nThe total number of parameters is now only 3.\n\n\n7.1.2 Equilibria\nThe nullclines are as follows: \\[\n\\left\\{\\begin{aligned}\n0 &= \\rho u(1-u) - \\alpha \\frac{\\delta uv}{1+\\delta u}, \\\\\n0 &= -v + \\alpha \\frac{\\delta uv}{1+\\delta u},\n\\end{aligned}\\right.\n\\]\nLet us rewrite the system as follows: \\[\n\\left\\{\\begin{aligned}\n0 &= f(u)\\bigl( \\Phi(u) - v \\bigr), \\\\\n0 &= v\\bigl(f(u)-1\\bigr),\n\\end{aligned}\\right.\n\\]\nwhere we defined: \\[\nf(u) = \\frac{\\alpha \\delta u}{1+\\delta u}, \\quad \\Phi(u) = \\frac{\\rho}{\\alpha\\delta} (1-u)(1+\\delta u).\n\\]\nNote that \\(f(0)=0\\), \\(0\\le f(u) &lt; \\alpha\\), and \\(f'(u)&gt;0\\). The equilibria are:\n\n\\(E_0 = (0,0)\\), and is always present for any choice of the parameters.\n\\(E_1 = (1,0)\\), because \\(\\Phi(1)=0\\). This equilibrium is also always present.\n\\(E^* = (u^*,v^*)\\) where \\(f(u^*)=1\\) and \\(v^* = \\Phi(u^*)\\). The existence depends on the parameters.\n\n\\(\\alpha&gt;1\\), otherwise there exists no \\(u^* \\ge 0\\) such that \\(f(u^*)=1\\). In this case, we have \\(u^* = \\frac{1}{\\delta(\\alpha-1)}\\).\n\\(v^* = \\Phi(u^*) \\ge 0\\), which implies \\(u^* \\le 1\\). In terms of parameters, \\(\\delta &gt; \\delta^* = \\frac{1}{\\alpha-1}\\). (We define \\(\\rho^*\\) because is a bifurcation point, as we shall see.)\n\n\nThus we can have 2 or 3 equilibria. Note that \\(\\alpha &gt; 1\\) means, in terms of the original parameters, \\[\n\\frac{\\gamma}{T} = \\lim_{H\\to 0} \\omega(H) &gt; \\mu,\n\\]\nconsistently with the hypothesis on \\(\\omega(H)\\) for general Gause-type models: for large \\(H\\), predator population should grow.\n\n\n7.1.3 Local stability\nThe Jacobian is \\[\nJ(u,v) = \\begin{pmatrix} f'(u)(\\Phi(u)-v) + f(u)\\Phi'(u) & -f(u) \\\\ vf'(u) & f(u)-1 \\end{pmatrix}.\n\\]\nWe have: \\[\n\\begin{aligned}\nJ(0,0) &= \\begin{pmatrix} f'(0)\\Phi(0) & 0 \\\\ 0 & -1 \\end{pmatrix}, \\\\\nJ(1,0) &= \\begin{pmatrix} f(1)\\Phi'(1) & -f(1) \\\\ 0 & f(1)-1 \\end{pmatrix}, \\\\\nJ(u^*,v^*) &= \\begin{pmatrix} \\Phi'(u^*) & -1 \\\\ v^*f'(u^*) & 0 \\end{pmatrix},\n\\end{aligned}\n\\]\nwhere we used the definition of the equilibria, when they exist. Therefore:\n\n\\(E_0 = (0,0)\\) has eigenvalues \\(\\lambda_1 = f'(0)\\Phi(0)&gt;0\\) and \\(\\lambda_2 = -1 &lt; 0\\), so is a saddle (always).\n\\(E_1 = (1,0)\\) has eigenvalues \\(\\lambda_1 = f(1)\\Phi'(1)&lt;0\\), because \\(\\Phi'(1)&lt;0\\), and \\(\\lambda_2 = f(1)-1\\), which depends on the parameters. In particular,\n\nif \\(\\delta &lt; \\delta^*\\) then \\(f(1)-1&lt;0\\), so the equilibrium \\(E_1\\) is asymptotically stable.\nif \\(\\delta &gt; \\delta^*\\), \\(E_1\\) is a saddle. Note that in this case we have the existence of \\(E^*\\).\n\nSo, at \\(\\delta = \\delta^*\\) (or \\(u^* = 1\\)) we have a bifurcation (transcritical).\n\\(E^* = (u^*,v^*)\\) has \\(\\det J^* = v^* f'(u^*) &gt; 0\\) and \\(\\operatorname{tr} J^* = \\Phi'(u^*)\\). Since \\(\\Phi(u)\\) is a concave parabola with a maximum for \\(u = u_\\mathrm{max} = \\frac{\\delta-1}{2\\delta}\\), we have that:\n\nif \\(u^* &gt; u_\\mathrm{max}\\) then \\(\\Phi'(u^*) &lt; 0\\), thus \\(E^*\\) is asympotically stable.\nif \\(u^* &lt; u_\\mathrm{max}\\) then if \\(\\Phi'(u^*) &gt; 0\\), thus \\(E^*\\) is a unstable.\n\nThere is a second bifurcation point for \\(u^* = u_\\mathrm{max}\\), because \\(E^*\\) loses stability. In terms of parameters, this happens for \\(\\delta^{**} = \\frac{\\alpha+1}{\\alpha-1}\\).\nAdditionally, the discriminant \\(\\Delta = (\\operatorname{tr}J^*) - 4\\det J^*\\) tells us whether \\(E^*\\) is a spiral or not. Since \\(\\Delta &gt; 0\\) for \\(u^* = 1\\) and \\(\\Delta &lt; 0\\) for \\(u^* = u_\\mathrm{max}\\), it changes sign at some point.\n\nIn summary, assuming \\(\\alpha &gt; 1\\), we have 3 cases:\n\n\\(\\delta \\in (0,\\delta^*)\\): 2 equilibria, \\(E_0\\) saddle and \\(E_1\\) asymptotically stable;\n\\(\\delta \\in (\\delta^*, \\delta^{**})\\): 3 equilibria, \\(E_0\\) and \\(E_1\\) are saddles, and \\(E^*\\) is asymptotically stable (and spiral at some point).\n\\(\\delta &gt; \\delta^{**}\\): 3 equilibria, non of which is stable.\n\n\n\n7.1.4 Limit cycles\nFor \\(\\delta &lt; \\delta^{**}\\) we also have no limit cycles. This can be shown by using the Dulac‚Äôs criterium with integrating factor \\[\nh(u,v) = \\frac{1}{2\\rho\\delta}\\frac{u v^{q-1}}{(1+\\delta u)}, \\quad q = \\rho u^* |u_\\mathrm{max}|.\n\\]\nComputing the divergence we have: \\[\n\\operatorname{div}\\bigl( h(u,v)\\mathbf{f}(u,v) \\bigr) = - u v^{q-1}\\Bigl( u^2 + (u_\\mathrm{max} + |u_\\mathrm{max}|)u - u^* u_\\mathrm{max} \\Bigr),\n\\]\nso if \\(u_\\mathrm{max} \\le 0\\) the divergence is always negative. If \\(u_\\mathrm{max}&gt;0\\) and \\(u_\\mathrm{max}\\le u^*\\): \\[\n\\begin{split}\n\\operatorname{div}\\bigl( h(u,v)\\mathbf{f}(u,v) \\bigr) &= - u v^{q-1}\\Bigl( u^2 + 2 u_\\mathrm{max}u - u^* u_\\mathrm{max} \\Bigr) \\\\\n&\\le - u v^{q-1}\\Bigl( u^2 + 2 u_\\mathrm{max}u - u_\\mathrm{max}^2 \\Bigr) \\\\\n&= - u v^{q-1}( u - u_\\mathrm{max})^2,\n\\end{split}\n\\]\nwhich is negative for \\(u \\neq u^*\\). So we cannot have limit cycles when the maximum of \\(\\Phi(u)\\) is on the negative side of, if present in the positive quadrant, when \\(u^* \\ge u_\\mathrm{max}\\). This is equivalent to \\(\\Phi'(u^*)&gt;0\\), as seen above.\nOn the other hand, for \\(u^* \\le u_\\mathrm{max}\\) we cannot conclude anything from the Dulac criterium. Actually, we have a stable limit cycle orbiting around \\(E^*\\) in this case. To prove it, we use the Poincar√© theorem, defining a trapping region for the orbits the does not contain any stable equilibrium. The region is the following:\n\nThe region \\(\\mathcal{R}\\) is delimited by the axes, the unstable manifold of \\(E_K\\), the segment \\(\\overline{AB}\\), and a small hole around \\(E^*\\). Trajectories are trapped, because the segment \\(\\overline{AB}\\) and the boundary of the hole are inflow for the vector field. On the boundary of the region there are 2 saddles, but trajectories inside of \\(\\mathcal{R}\\) cannot go there, otherwise they would be part of their stable manifold. We can also exclude the saddle-saddle connection, which is a heteroclinic orbit, because trajectories cannot go there. So, there must exists at least one limit cycle inside \\(\\mathcal{R}\\). It is also unique, but this is more difficult to prove.\n\n\n7.1.5 Bifurcations\nFor \\(\\delta = \\delta^*\\) we have a transcritical bifurcation and for \\(\\delta = \\delta^{**}\\) we have a Hopf bifurcation. Let‚Äôs see what we mean by this.\nConsider a system of the form \\[\n\\mathbf{y}' = \\mathbf{f}(\\mathbf{y},p),\n\\]\nfor some parameter \\(p\\in\\mathbb{R}\\) (in our case, \\(\\delta\\)), and suppose that \\(\\bar{\\mathbf{y}}\\) is an equilibrium for \\(p=\\bar{p}\\). Then, assuming that \\(\\det J(\\bar{\\mathbf{y}},\\bar{p})\\neq 0\\), we can locally define a curve of equilibria \\(\\bar{\\mathbf{y}}=\\bar{\\mathbf{y}}(p)\\) such that \\(\\mathbf{f}(\\bar{\\mathbf{y}}(p),p)=0\\) for \\(p\\in\\mathcal{B}_\\varepsilon(\\bar{p})\\). Moreover, the eigenvalues are continuous functions of \\(p\\), so stability is not affected.\nIf one real eigenvalue becomes zero, then we have a bifurcation and a change of stability. We consider 3 cases:\n\nTangent bifurcation. \\(J(\\bar{\\mathbf{y}},\\bar{p})\\) has eigenvalue 0 and \\(\\partial_p \\mathbf{f}(\\bar{\\mathbf{y}},\\bar{p})\\neq 0\\).\nTranscritical bifurcation. \\(J(\\bar{\\mathbf{y}},\\bar{p})\\) has eigenvalue 0 and \\(\\partial_p \\mathbf{f}(\\bar{\\mathbf{y}},\\bar{p}) = 0\\).\nHopf bifurcation. \\(J(\\bar{\\mathbf{y}},\\bar{p})\\) has pure imaginary eigenvalues \\(\\pm i\\omega\\).\n\nFor \\(\\delta = \\delta^*\\) the equilibria \\(E_1\\) and \\(E^*\\) coincide and one eigenvalue is 0. Since \\(E_1\\) does not depend on \\(\\delta\\), \\(\\partial_\\delta \\mathbf{f}(E_1,\\delta) = 0\\): we have a transcritical bifurcation. Graphically, the curve of equilibria of \\(E_1\\) and \\(E^*\\) crosses each other and exchange their stability.\nFor \\(\\delta = \\delta^{**}\\) we have a Hopf bifurcation: the equilibrium \\(E^*\\) becomes unstable and an infinitesimally small and stable limit cycle appears around it. We can detect a Hopf bifurcation from the eigenvalues of \\(J^*\\): they are complex conjugate of the form: \\[\n\\lambda(\\delta) = \\sigma(\\delta) \\pm i\\omega(\\delta).\n\\]\nWe have that \\(\\sigma(\\delta^{**}) = 0\\) but \\(\\omega(\\delta^{**}) = \\sqrt{\\rho / \\delta^{**}} \\neq 0\\). Moreover, when eigenvalues are imaginary, the real part is twice the trace, so \\(\\sigma(\\delta) = \\Phi'(u^*(\\delta))\\), and we can conclude that \\(\\sigma'(\\delta^{**})&gt;0\\). The eigenvalues are crossing the imaginary axis on the complex plane from the negative to the positive real part side, as \\(\\delta\\) crosses the bifurcation. Since the new limit cycle is stable, we have a supercritical Hopf bifurcation. When the limit cycle is unstable, we have a subcritical Hopf bifurcation.\n\n\n7.1.6 Summary\nIn conclusion, we have the following picture:\n\n\\(\\delta &lt; \\delta^*\\): \\(E_1\\) globally attractive.\n\\(\\delta = \\delta^*\\): transcritical bifurcation.\n\\(\\delta^* &lt; \\delta &lt; \\delta^{**}\\): \\(E^*\\) globally attractive.\n\\(\\delta = \\delta^{**}\\): Hopf bifurcation.\n\\(\\delta &gt; \\delta^{**}\\): limit cycle globally attractive.\n\nThe global stability of \\(E_1\\) and \\(E^*\\) follows from the same argument above, using a trapping region (the same in the case of \\(E^*\\), because we know there cannot be a limit cycle and \\(E^*\\) is stable.)\n\n\n\n\nIannelli, Mimmo, and Andrea Pugliese. 2014. An Introduction to Mathematical Population Dynamics. Vol. 79. Springer. https://strutture-provincia.primo.exlibrisgroup.com/permalink/39SBT_INST/ghkipk/alma991006883209706186.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Rosenzweig-MacArthur model</span>"
    ]
  },
  {
    "objectID": "08_MichaelinMenten.html",
    "href": "08_MichaelinMenten.html",
    "title": "8¬† Michaelin-Menten law",
    "section": "",
    "text": "8.1 Feedback control\nThe prey-predator model is an example of system with a feedback. The prey population has a positive effect on the predator population, whereas the predator population has a negative effect on the prey population.\nSpecifically, in this case we say that the system has a negative feedback loop. In a negative feedback, a positive value of a variable yields a decrease in that variable and, vice versa, a negative value leads to an increase. An important feat of negative loops is that they can lead to oscillations, as per prey-predator models.\nWe can find many good examples of negative loops in real life (try yourselves!) and in biology too:\nPositive feedback loops are important too. Using the analogy of ecology, consider the following (Kolmogorov) model: \\[\n\\left\\{\\begin{aligned}\nN_1' &= N_1 f(N_1,N_2), \\\\\nN_2' &= N_2 g(N_1,N_2),\n\\end{aligned}\\right.\n\\]\nWe can have 3 situations:\nExamples outside biology include climate change, for instance \\(\\ce{CO2}\\) emissions raise the global temperature, but at higher temperatures soil microbes have faster metabolism, thus breaking down organic matter faster, releasing more \\(\\ce{CO2}\\). Or methane release, which has much more greenhouse power than \\(\\ce{CO2}\\). Methane is trapped in permafrost, but its release increases temperatures which will melt permafrost and release more methane.\nIn general, life is an intricated network of positive and negative interactions. A positive effect is also called activation: a gene activates another gene, in the sense that its increase also increases the other one; a negative effect is called inhibition. An example is the lac operon shown below:\nThe lac operon is a feedback control loop present in the E. coli. When there is plenty of glucose, E. coli uses it for metabolism, even if other sugars are present. On the other hand, when glucose is absent, the bacterium activates other genes that are able to process lactose. This is an example of genetic switch.\nIn this series of lectures we will explore more in depth feedback loops. We will start with enzymatic reactions, a good example of how to create complex and non-linear interactions out of ‚Äúsimple‚Äù chemistry.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Michaelin-Menten law</span>"
    ]
  },
  {
    "objectID": "08_MichaelinMenten.html#feedback-control",
    "href": "08_MichaelinMenten.html#feedback-control",
    "title": "8¬† Michaelin-Menten law",
    "section": "",
    "text": "Gene regulation. Many genes inhibits their own transcription. An example is the protein Hes1, involved in the early stages of development of the embryo. Hes1 oscillates over time, because the protein inhibits its own transcription.\nCircadian rhythm. Our body has an internal clock, synchronized with the Sun, located in the hypothalamus. The protein PER inhibits its own transcription, thus creating oscillations with a period of 24 hours.\nInsuline-glucose oscillations. An intake of glucose (from a meal) causes the pancreas to secrete more insulin, which eventually lowers the level of glucose in the body.\nHormones. Many hormones like testosterone and progesterone are under negative feedback with the pituitary gland and the brain.\n\n\n\n\n\\(\\frac{\\partial f}{\\partial N_2} &lt; 0\\) and \\(\\frac{\\partial g}{\\partial N_1} &gt; 0\\): this is a prey-predator interaction, thus a negative feedback, because an increase of \\(N_1\\) increase \\(N_2'\\), which increases \\(N_2\\) and that has a negative effect on \\(N_1'\\) itself.\n\\(\\frac{\\partial f}{\\partial N_2} &lt; 0\\) and \\(\\frac{\\partial g}{\\partial N_1} &lt; 0\\): this is a competitive model, because an increase of \\(N_1\\) (resp. \\(N_2\\)) has a negative effect of \\(N_2\\) (resp. \\(N_1\\)). Here, there is no vicious or virtous loop, possible equilibria are the extinction of either of the two species. (Coexistence is also possible but more difficult to achieve.)\n\\(\\frac{\\partial f}{\\partial N_2} &gt; 0\\) and \\(\\frac{\\partial g}{\\partial N_1} &gt; 0\\): this is the case of cooperation or mutualism or symbiosis. Both species benefit from the presence of the other. This is a positive feedback, because as \\(N_1\\) increases, \\(N_2\\) increases too leading to an increase to \\(N_1\\) itself.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Michaelin-Menten law</span>"
    ]
  },
  {
    "objectID": "08_MichaelinMenten.html#chemical-kinetics",
    "href": "08_MichaelinMenten.html#chemical-kinetics",
    "title": "8¬† Michaelin-Menten law",
    "section": "8.2 Chemical kinetics",
    "text": "8.2 Chemical kinetics\nBefore delving deeper into the matter, we need to refresh a bit our chemistry knowledge, specifically chemical kinetics.\nChemical kinetics is the study of reaction rates. It is known that reaction rates depend on the concentration of reactants (and in some cases products) in characteristic ways that can be expressed in terms of differential equations known in chemistry as ‚Äúrate laws‚Äù.\nConsider the following reaction: \\[\n\\ce{A + B -&gt; C},\n\\]\nHere, the reaction involves 3 molecules, \\(\\ce{A}\\), \\(\\ce{B}\\), and \\(\\ce{C}\\), and it simply states ‚Äúthe reaction of \\(\\ce{A}\\) and \\(\\ce{B}\\) gives \\(\\ce{C}\\)‚Äù. The formula tells us that exactly one molecule of \\(\\ce{A}\\) and \\(\\ce{B}\\) is needed to form one molecule of \\(\\ce{C}\\) (stoichiometry of the reaction.) However, it does not give us information on the ‚Äúrate‚Äù of product of \\(\\ce{C}\\), for instance, or the consumption of \\(\\ce{A}\\) and \\(\\ce{B}\\).\nWe measure the concentration of a species \\(\\ce{X}\\) at time \\(t\\in\\mathbb{R}\\) by \\([\\ce{X}](t)\\). The unit of measure is mole per liter, that is \\([\\ce{X}]\\) is the molar concentration. (We assume that the volume of the system is constant.) A mole of \\(\\ce{X}\\) is exactly \\(N_A = 6.023\\times 10^{23}\\) (Avogadro constant) molecules of \\(\\ce{X}\\). So, the above reaction means that ‚Äúone mole of \\(\\ce{A}\\) plus one mole of \\(\\ce{B}\\) gives exactly one mole of \\(\\ce{C}\\)‚Äù. This is important, since it does not work that well with molecular weights!\nThe rate of consumption or production of \\(\\ce{X}\\) is simply \\[\n\\frac{\\mathrm{d}[\\ce{X}]}{\\mathrm{d} t},\n\\]\nwith positive sign the case of production, and negative otherwise. From the stoichiometry of \\(\\ce{A + B -&gt; C}\\) we have that \\[\n\\frac{\\mathrm{d}[\\ce{A}]}{\\mathrm{d} t} = \\frac{\\mathrm{d}[\\ce{B}]}{\\mathrm{d} t} = - \\frac{\\mathrm{d}[\\ce{C}]}{\\mathrm{d} t}.\n\\]\nFor the reaction \\(\\ce{A + 2B -&gt; 3C + D}\\) we have: \\[\n\\frac{\\mathrm{d}[\\ce{D}]}{\\mathrm{d} t} = \\frac{1}{3}\\frac{\\mathrm{d}[\\ce{C}]}{\\mathrm{d} t} = - \\frac{\\mathrm{d}[\\ce{A}]}{\\mathrm{d} t} = - \\frac{1}{2} \\frac{\\mathrm{d}[\\ce{B}]}{\\mathrm{d} t}.\n\\]\n\n8.2.1 Law of mass action\nThe fundamental ‚Äúlaw‚Äù of a chemical reaction is the law of mass action. This law describes the rate at which chemicals collide and interact to form different chemical combinations. For the reaction \\(\\ce{A + B -&gt; C}\\), the law of mass action tells us that \\[\n\\frac{\\mathrm{d}[\\ce{C}]}{\\mathrm{d} t} = k [\\ce{A}][\\ce{B}],\n\\]\nthat is, the rate of production of \\([\\ce{C}]\\) is proportional to the concentration of \\(\\ce{A}\\) and \\(\\ce{B}\\). The law is based on the idea that the bimolecular chemical reaction results from the collision of \\(\\ce{A}\\) and \\(\\ce{B}\\), thus it must be proportional to the concentration of both. We encountered the same law for prey-predator systems. As for prey-predator system, therefore, the law is not inviolable, because in many situations also \\(k\\) depends on the concentrations. Deviations may occur at very large or very low concentrations of \\(\\ce{A}\\) or \\(\\ce{B}\\), for instance.\nFor thermodynamical reasons, a chemical reaction always proceeds in both directions, that is: \\[\n\\ce{A + B &lt;=&gt;[$k_1$][$k_{-1}$] C},\n\\]\nwhere \\(k_1\\) is the rate of the forward reaction \\(\\ce{A + B -&gt; C}\\), and \\(k_{-1}\\) is the rate of the backward reaction \\(\\ce{C -&gt; A + B}\\). Since the two reactions (forward and backward) are independent, we have that \\[\n\\frac{\\mathrm{d}[\\ce{C}]}{\\mathrm{d} t} = k_1 [\\ce{A}][\\ce{B}] - k_{-1}[\\ce{C}].\n\\]\nNote that we can apply the law of mass action also to the backward reaction: the rate is proportional to the concentration of the reactants, in this case only \\(\\ce{C}\\).\nThe full system of equations results from the application of the law to each species: \\[\n\\left\\{ \\begin{aligned}\n{}[\\ce{A}]' &= -k_1[\\ce{A}][\\ce{B}] + k_{-1}[\\ce{C}], \\\\\n[\\ce{B}]' &= -k_1[\\ce{A}][\\ce{B}] + k_{-1}[\\ce{C}], \\\\\n[\\ce{C}]' &=  k_1[\\ce{A}][\\ce{B}] - k_{-1}[\\ce{C}]. \\\\\n\\end{aligned}\\right.\n\\]\nThere is some redundancy in the system, because of the conservation of mass. Indeed, \\(\\ce[A]+\\ce[C]\\) and \\(\\ce[B]+\\ce[C]\\) are constant. Another way to see this is noting that \\(\\ce[A]'+\\ce[C]' = 0\\) from the above system.\nAt equilibrium, the production and consumption rate of \\(\\ce{C}\\) are the same. So: \\[\n\\frac{\\mathrm{d}[\\ce{C}]}{\\mathrm{d} t} = 0, \\quad\\Rightarrow\\quad\nk_1 [\\ce{A}]_\\mathrm{eq}[\\ce{B}]_\\mathrm{eq} = k_{-1}[\\ce{C}]_\\mathrm{eq}.\n\\]\nThe equilibrium constant of the reaction is: \\[\nK_\\mathrm{eq} = \\frac{k_{-1}}{k_1} = \\frac{[\\ce{A}]_\\mathrm{eq}[\\ce{B}]_\\mathrm{eq}}{[\\ce{C}]_\\mathrm{eq}}.\n\\]\nThe equilibrium constant is very important. First, it provides a link to thermodynamics (see the end of the notes). Second, it can be used to understand the trend of the reaction: when \\(K_\\mathrm{eq}\\) is very small, then at equilibrium (so after waiting for long time) most of \\(\\ce{A}\\) and \\(\\ce{B}\\) are combined into \\(\\ce{C}\\).\nQuantitatively, since \\([\\ce{A}] + [\\ce{C}]\\) is constant, then \\([\\ce{A}]_\\mathrm{eq} + [\\ce{C}]_\\mathrm{eq} = [\\ce{A}](0) + [\\ce{C}](0)\\). Assuming that at the beginning \\([\\ce{C}](0)=0\\) and \\([\\ce{A}](0) = [\\ce{A}]_0&gt;0\\), then \\[\n[\\ce{C}]_\\mathrm{eq} = A_0 \\frac{[\\ce{B}]_\\mathrm{eq}}{K_\\mathrm{eq} + [\\ce{B}]_\\mathrm{eq}}.\n\\]\nIn other words, when \\([\\ce{B}]_\\mathrm{eq} = K_\\mathrm{eq}\\), half of \\(\\ce{A}\\) is combined into \\(\\ce{C}\\).\nConsider now the reaction \\[\n\\ce{\\alpha A + \\beta B -&gt; \\gamma C + \\delta D},\n\\]\nfor some positive constants \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\), and \\(\\delta\\). We have that: \\[\n\\frac{1}{\\delta} \\frac{\\mathrm{d}[\\ce{D}]}{\\mathrm{d} t} = \\frac{1}{\\gamma}\\frac{\\mathrm{d}[\\ce{C}]}{\\mathrm{d} t} = - \\frac{1}{\\alpha}\\frac{\\mathrm{d}[\\ce{A}]}{\\mathrm{d} t} = - \\frac{1}{\\beta} \\frac{\\mathrm{d}[\\ce{B}]}{\\mathrm{d} t}.\n\\]\nFor the law of mass action, we have: \\[\n\\frac{1}{\\alpha}\\frac{\\mathrm{d}[\\ce{A}]}{\\mathrm{d} t} = - k_1 [\\ce{A}]^\\alpha [\\ce{B}]^\\beta + k_{-1}[\\ce{C}]^\\gamma [\\ce{D}]^\\delta,\n\\]\nbecause we need \\(\\alpha\\) molecules of \\([\\ce{A}]\\) and \\(\\beta\\) molecules of \\([\\ce{B}]\\) for the forward reaction (and similar for the backward reaction). The equilibrium constant is: \\[\nK_\\mathrm{eq} = \\frac{[\\ce{A}]^\\alpha_\\mathrm{eq}[\\ce{B}]^\\beta_\\mathrm{eq}}{[\\ce{C}]^\\gamma_\\mathrm{eq}[\\ce{D}]^\\delta_\\mathrm{eq}}.\n\\]",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Michaelin-Menten law</span>"
    ]
  },
  {
    "objectID": "08_MichaelinMenten.html#michaelis-menten-law",
    "href": "08_MichaelinMenten.html#michaelis-menten-law",
    "title": "8¬† Michaelin-Menten law",
    "section": "8.3 Michaelis-Menten law",
    "text": "8.3 Michaelis-Menten law\nA fundamental property of biochemical reactions in living cells is the fact that the rate at which they occur is regulated by the presence of other molecules. An enzyme is a protein that makes it possible that a reaction (that by itself would be extremely slow) occurs over a short time-scale, as necessary for the cell functioning, but is apparently not involved in the reaction itself, in the sense that its concentration at the end is the same as at the beginning.\nEzymes are catalysts that converts substrates into products. The speed-up of the reaction obtained thanks to the enzyme can be of several order of magnitudes, like millions of times or more. Enzymes are also highly specific. More importantly, enzymes are the building blocks of regulatory mechanisms present in every living body.\nWe consider now the archetypal example of enzymatic reaction. The diagram is the following: \\[\n\\ce{S + E &lt;=&gt;[$k_1$][$k_{-1}$] C -&gt;[$k_2$] P + E}\n\\]\nIn the diagram \\(\\ce{S}\\) refers to the ‚Äúsubstrate‚Äù the molecule that is converted into \\(\\ce{P}\\), the ‚Äúproduct‚Äù of the reaction, while \\(\\ce{E}\\) refers to enzyme molecules. Looking only at the left and right end-points of the diagram, one sees that a molecule of \\(\\ce{S}\\) is converted into \\(\\ce{P}\\), while one is left with same molecule of \\(\\ce{E}\\) present in the beginning. However, one sees that \\(\\ce{E}\\) is involved in intermediate reactions (the complex), necessary to understand why the presence of the enzyme is necessary for the reaction to occur.\n\n\n\n(From Wikimedia)\n\n\nThe letters over each arrow in the diagram represent the rate constants at which each reaction (represented by an arrow) occurs. We assume that all reactions occur following the law of mass action: when a reaction involves two substances reacting, the rate at which the reaction occurs is proportional to the product of the concentrations of the substances involved; the rate constant is the proportionality constant. On the other hand, when a reaction involves a single substance, the reaction rate is the product of the rate constant times the concentrations of that substance.\nIt is easy transforming the diagram into a set of differential equations, using the principle that the concentration of each substance will increase at the sum of the rates of all reactions producing it, and decrease at those destroying it. Namely setting \\[\n\\begin{aligned}\n{}[\\ce{S}](t) &= \\text{concentration of substrate $\\ce{S}$ at time $t$}, \\\\\n[\\ce{E}](t) &= \\text{concentration of enzyme $\\ce{E}$ at time $t$}, \\\\\n[\\ce{C}](t) &= \\text{concentration of complex $\\ce{C}$ at time $t$}, \\\\\n[\\ce{P}](t) &= \\text{concentration of product $\\ce{P}$ at time $t$}.\n\\end{aligned}\n\\]\nwe obtain, \\[\n\\left\\{\n\\begin{aligned}\n{}[\\ce{S}]' &= k_{-1}[\\ce{C}] - k_1 [\\ce{S}][\\ce{E}], \\\\\n[\\ce{E}]' &= (k_{-1}+k_1)[\\ce{C}] - k_1 [\\ce{S}][\\ce{E}], \\\\\n[\\ce{C}]' &= k_1 [\\ce{S}][\\ce{E}] - (k_{-1} + k_2)[\\ce{C}], \\\\\n[\\ce{P}]' &= k_2 [\\ce{C}].\n\\end{aligned}\n\\right.\n\\]\nThis appears to be a system of 4 differential equations with 4 unknowns \\(([\\ce{S}](t), [\\ce{E}](t), [\\ce{C}](t), [\\ce{P}](t))\\). Typical initial conditions will be \\([\\ce{S}](0) = s_0 &gt; 0\\), \\([\\ce{E}](0) = e_0 &gt; 0\\), \\([\\ce{C}](0) = [\\ce{P}](0) = 0\\) (at time \\(t=0\\), beginning of the reaction, there is neither product nor complex).\nOne can immediately see that the first 3 equations do not depend on \\([\\ce{P}](t)\\); we can then solve the first 3 equations, then compute \\([\\ce{P}](t)\\) (the quantity presumably more relevant) as: \\[\n[\\ce{P}](t) = k_2 \\int_0^t [\\ce{C}](s)\\,\\mathrm{d}s.\n\\]\nMoreover, one may note that \\[\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\Bigl[ [\\ce{C}](t) + [\\ce{E}](t) \\Bigr] = 0, \\quad\\Rightarrow\\quad [\\ce{C}](t) + [\\ce{E}](t) = [\\ce{C}](0) + [\\ce{E}](0) = e_0.\n\\]\nWe can therefore remove one equation and substitute \\([\\ce{E}](t) = e_0 - [\\ce{C}](t)\\). We end up with a system of 2 equations: \\[\n\\left\\{\n\\begin{aligned}\n{}[\\ce{S}]' &= k_{-1}[\\ce{C}] - k_1 [\\ce{S}](e_0-[\\ce{C}]), \\\\\n[\\ce{C}]' &= k_1 [\\ce{S}](e_0-[\\ce{C}]) - (k_{-1} + k_2)[\\ce{C}], \\\\\n\\end{aligned}\n\\right.\n\\]\nwith initial conditions \\([\\ce{S}](0) = s_0 &gt; 0\\) and \\([\\ce{C}](0) = 0\\).\nIt is certainly possible analyse the behavior of system as it is. For instance, we would like to know how \\([\\ce{C}]\\), and hence \\([\\ce{P}]'\\) vary over time and depending on the concentration of the substrate. This type of analysis leads to the Michaelis-Menten law, which states that: \\[\n[\\ce{P}]' = V = \\frac{V_\\mathrm{max}[\\ce{S}]}{K_m + [\\ce{S}]},\n\\]\nfor \\(V_\\mathrm{max} &gt; 0\\) being the maximum reaction rate, that is \\(V\\to V_\\mathrm{max}\\) for \\([\\ce{S}]\\to\\infty\\), and \\(K_m\\) is the half-saturation rate, that is when \\([\\ce{S}]=K_m\\) then \\(V = \\frac{1}{2}V_\\mathrm{max}\\). Note that this law is exactly the Holling type II law from prey-predator models.\n\n\n8.3.1 Equilibrium and phase portrait\nAs usual, we may have a look at the equilibria and their stability. Here, however, this analysis in not very insightful.\nThe only equilibrium is the origin. It is also locally asympotically stable, since: \\[\nJ(0,0) = \\begin{pmatrix} - e_0 k_1 & k_{-1} \\\\ e_0 k_1 & -k_2-k_{-1} \\end{pmatrix}\n\\]\nso the trace is negative and \\(\\det J = e_0 k_1 k_2 &gt; 0\\).\nThe nullclines are: \\[\n\\begin{aligned}{}\n[\\ce{S}]' = 0 &&\\Leftrightarrow&& [\\ce{C}] &= \\frac{e_0 k_1 [\\ce{S}]}{k_{-1} + k_1 [\\ce{S}]}, \\\\\n[\\ce{S}]' = 0 &&\\Leftrightarrow&& [\\ce{C}] &= \\frac{e_0 k_1 [\\ce{S}]}{k_{-1} + k_2 + k_1 [\\ce{S}]}.\n\\end{aligned}\n\\]\n\nThe nullclines are similar: they have the same horizontal asymptote \\(e_0\\), they intersect at the origin, but the second one is slightly shifted to the right, because they reach \\(\\frac{1}{2}e_0\\) (half of the maximum) at different values: for \\([\\ce{S}]=\\frac{k_{-1}}{k_1}\\) in the first case, and \\([\\ce{S}]=\\frac{k_{-1}+k_2}{k_1}\\) in the second case.\nFrom the plot we see that once a trajectory enters into the corridor between the nullclines, it remains trapped. Since we only have the origin as equilibrium, all trajectories must converge there: the equilibrium is globally stable.\nThe corridor is shaped like a Michaelis-Menten law, in the sense that in the gap \\([\\ce{C}]\\) varies with respect to \\([\\ce{S}]\\) like in the Michaelis-Menten law. But which curve is the correct one? We will see it tomorrow.\n\n\n\n\nKeener, James, and James Sneyd. 2009. Mathematical Physiology: I: Cellular Physiology. Second. Vol. 8/I. Springer. https://strutture-provincia.primo.exlibrisgroup.com/permalink/39SBT_INST/ghkipk/alma991012367489706186.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Michaelin-Menten law</span>"
    ]
  },
  {
    "objectID": "09_FastSlow.html",
    "href": "09_FastSlow.html",
    "title": "9¬† Fast-Slow systems",
    "section": "",
    "text": "9.1 Michaelis-Menten law\nWe consider again the reaction: \\[\n\\ce{S + E &lt;=&gt;[$k_1$][$k_{-1}$] C -&gt;[$k_2$] P + E}\n\\]\nThe system is: \\[\n\\left\\{\n\\begin{aligned}{}\n[\\ce{S}]' &= k_{-1}[\\ce{C}] - k_1 [\\ce{S}][\\ce{E}], \\\\\n[\\ce{E}]' &= (k_{-1}+k_1)[\\ce{C}] - k_1 [\\ce{S}][\\ce{E}], \\\\\n[\\ce{C}]' &= k_1 [\\ce{S}][\\ce{E}] - (k_{-1} + k_2)[\\ce{C}], \\\\\n[\\ce{P}]' &= k_2 [\\ce{C}].\n\\end{aligned}\n\\right.\n\\]\nWe can remove one equation and substitute \\([\\ce{E}](t) = e_0 - [\\ce{C}](t)\\). We end up with a system of 2 equations: \\[\n\\left\\{\n\\begin{aligned}{}\n[\\ce{S}]' &= k_{-1}[\\ce{C}] - k_1 [\\ce{S}](e_0-[\\ce{C}]), \\\\\n[\\ce{C}]' &= k_1 [\\ce{S}](e_0-[\\ce{C}]) - (k_{-1} + k_2)[\\ce{C}], \\\\\n\\end{aligned}\n\\right.\n\\]\nwith initial conditions \\([\\ce{S}](0) = s_0 &gt; 0\\) and \\([\\ce{C}](0) = 0\\).\nHere, we would like to show that the production rate follows the Michaelis-Menten law: \\[\n[\\ce{P}]' = V = \\frac{V_\\mathrm{max}[\\ce{S}]}{K_m + [\\ce{S}]},\n\\]",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Fast-Slow systems</span>"
    ]
  },
  {
    "objectID": "09_FastSlow.html#michaelis-menten-law",
    "href": "09_FastSlow.html#michaelis-menten-law",
    "title": "9¬† Fast-Slow systems",
    "section": "",
    "text": "9.1.1 The quasi-equilibrium approximation\nOne way to derive the law is with the quasi-equilibrium assumption. This approach was proposed by Michaelis and Menten in 1913. They assumed that the first reaction \\(\\ce{S + E -&gt; C}\\) is approximately at equilibrium, thus: \\[\nK_1 = \\frac{k_{-1}}{k_1} = \\frac{[\\ce{S}][\\ce{E}]}{[\\ce{C}]}.\n\\]\nSince \\([\\ce{C}] + [\\ce{E}] = e_0\\), we conclude that: \\[\nK_1 = \\frac{[\\ce{S}](e_0-[\\ce{C}])}{[\\ce{C}]}, \\quad\\Rightarrow\\quad\n[\\ce{C}] = \\frac{e_0[\\ce{S}]}{K_1 + [\\ce{S}]}.\n\\]\nBut \\([\\ce{P}]' = V = k_2[\\ce{C}]\\), so: \\[\nV = \\frac{k_2 e_0[\\ce{S}]}{K_1 + [\\ce{S}]},\n\\]\nwhich is Michaelis-Menten law with \\(K_m = K_1\\) and \\(V_\\mathrm{max} = k_2 e_0\\).\nWe remark that this derivation is an approximation. In fact, the only true equilibrium of the system is the origin, which is quite useless as an approximation. Here, we can cannot conclude that \\([\\ce{S}]' = 0\\), in spite of the assumption. What we are actually saying is that, as we use \\(\\ce{S}\\) in the reaction, the concentration of the complex approximately follows the concentration of \\(\\ce{S}\\) according to the Michaelis-Menten law.\nWe can derive a law for \\([\\ce{S}]\\) by noting that (from the system) \\[\n[\\ce{S}]' + [\\ce{C}]' = -k_2 [\\ce{C}],\n\\]\nso \\[\n\\frac{\\mathrm{d}}{\\mathrm{d}t}\\bigl( [\\ce{S}] + [\\ce{C}] \\bigr) = \\frac{\\mathrm{d}}{\\mathrm{d}t}\\Bigl( [\\ce{S}] + \\frac{e_0[\\ce{S}]}{K_1 + [\\ce{S}]} \\Bigr) = -k_2 \\frac{e_0[\\ce{S}]}{K_1 + [\\ce{S}]},\n\\]\nand after rearranging we have \\[\n\\frac{\\mathrm{d}[\\ce{S}]}{\\mathrm{d}t}\\Bigl( 1 + \\frac{K_1 e_0}{(K_1 + [\\ce{S}])^2} \\Bigr) = -\\frac{k_2 e_0[\\ce{S}]}{K_1 + [\\ce{S}]},\n\\]\nwhich gives a differential equation for \\([\\ce{S}](t)\\).\nThe quasi-equilibrium approximation is very easy, powerful, and extensively applied in enzymatic reaction. It is possible to prove this approximation in a more mathematical way by assuming \\(k_{-1} \\gg k_2\\), but we will follow a different approach below.\n\n\n9.1.2 The quasi-steady-state approximation\nFirst, let us rescale the equations by the transformations: \\[\nx = \\frac{[\\ce{S}]}{s_0}, \\quad\ny = \\frac{[\\ce{C}]}{e_0}, \\quad\n\\tau = k_1 e_0 t.\n\\]\nAfter changing the variables, we get: \\[\n\\left\\{\n\\begin{aligned}\n\\frac{\\mathrm{d}x}{\\mathrm{d}\\tau} &= \\frac{k_{-1}}{s_0 k_1} y - x (1-y), \\\\\n\\frac{\\mathrm{d}y}{\\mathrm{d}\\tau} &= \\frac{s_0}{e_0} x (1-y) - \\frac{k_{-1} + k_2}{k_1 e_0} y, \\\\\n\\end{aligned}\n\\right.\n\\]\nthat we can rearrange in the following form: \\[\n\\left\\{\n\\begin{aligned}\n\\frac{\\mathrm{d}x}{\\mathrm{d}\\tau} &= -x + y(x+\\alpha), \\\\\n\\varepsilon \\frac{\\mathrm{d}y}{\\mathrm{d}\\tau} &= x -y(x + \\beta),\n\\end{aligned}\n\\right.\n\\]\nwhere: \\[\n\\alpha = \\frac{k_{-1}}{s_0 k_1}, \\quad\n\\beta = \\frac{k_{-1} + k_2}{k_1 s_0}, \\quad\n\\varepsilon = \\frac{e_0}{s_0}.\n\\]\nNext, suppose that \\(\\varepsilon \\ll 1\\), that is the initial concentration of the enzyme is, at the start, very low compared to the concentration of the substrate. Thus, we could assume that \\(\\varepsilon y' \\approx 0\\): \\[\n\\left\\{\n\\begin{aligned}\n\\frac{\\mathrm{d}x}{\\mathrm{d}\\tau} &= -x + y(x+\\alpha), \\\\\n0 &= x -y(x + \\beta).\n\\end{aligned}\n\\right.\n\\]\nThis approach is called quasi-steady-state approximation. From the second equation we conclude that \\[\ny = \\frac{x}{\\beta + x}, \\quad\\Rightarrow\\quad [\\ce{C}] = \\frac{[e_0 \\ce{S}]}{\\frac{k_{-1} + k_2}{k_1} + [\\ce{S}]}.\n\\]\nWe conclude that with the QSS approximation the Michaelis-Menten law has the same form with \\(V_\\mathrm{max} = k_2 e_0\\) (as before) and \\(K_m = \\frac{k_{-1}+k_2}{k_1}\\) (different, before was \\(K_m = K_1\\)). The difference is minimal: in the quasi-equilibrium approximation, we have \\(k_2 \\ll k_{-1}\\), so \\(K_m \\approx K_1\\), whereas for the QSS approximation we assume \\(\\varepsilon \\ll 1\\).\nThe Michaelis-Menten law is quite general and often evoked without deriving it (similarly to Holling type predations.) For us, it is the main ingredient for more complex reactions.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Fast-Slow systems</span>"
    ]
  },
  {
    "objectID": "09_FastSlow.html#singular-perturbation-theory",
    "href": "09_FastSlow.html#singular-perturbation-theory",
    "title": "9¬† Fast-Slow systems",
    "section": "9.2 Singular perturbation theory",
    "text": "9.2 Singular perturbation theory\nLet dive a bit deeper on the QSS approximation. The idea is the following: since \\(\\varepsilon\\) is small, we could try to approximate the solution of the IVP \\[\n\\left\\{\n\\begin{aligned}\n\\frac{\\mathrm{d}x_\\varepsilon}{\\mathrm{d}\\tau} &= -x_\\varepsilon + y_\\varepsilon(x_\\varepsilon+\\alpha), \\\\\n\\varepsilon \\frac{\\mathrm{d}y_\\varepsilon}{\\mathrm{d}\\tau} &= x_\\varepsilon -y_\\varepsilon(x_\\varepsilon + \\beta), \\\\\nx_\\varepsilon(0) &= 1, \\\\\ny_\\varepsilon(0) &= 0,\n\\end{aligned}\n\\right.\n\\]\nwith another IVP obtained from the limit \\(\\varepsilon\\to 0\\). Ideally, the solution of such IVP, say \\((x(t),y(t))\\), should be the limit of \\((x_\\varepsilon(t),y_\\varepsilon(t))\\) for \\(\\varepsilon\\to 0\\). This approach is called perturbation argument.\nVery often it is enough to take the limit in the ODE, as we did before. So we could approximate the problem with \\[\n\\left\\{\n\\begin{aligned}\n\\frac{\\mathrm{d}x}{\\mathrm{d}\\tau} &= -x + y(x+\\alpha), \\\\\n0 &= x -y(x + \\beta), \\\\\nx(0) &= 1, \\\\\ny(0) &= 0.\n\\end{aligned}\n\\right.\n\\]\nHowever, in this case it does not work, because the problem is not well-posed. In fact, we have 2 initial conditions, but only one ODE! The second ODE became an algebraic equation (this system is called Differential Algebraic Equation). We can only satisfy one initial condition. From the second equation we have: \\[\ny = \\frac{x}{x + \\beta},\n\\]\nfor \\(x(0)=1\\), \\(y(0)=\\frac{1}{1+\\beta}\\), whereas for \\(y(0)=0\\) we have \\(x(0)=0\\). The combination \\(x(0)=1\\) and \\(y(0)=0\\) does not work. (We say that the initial condition is not compatible.) The perturbation argument is singular.\nWe need a better strategy. Let us rescale the time as follows: \\[\n\\eta = \\frac{\\tau}{\\varepsilon}, \\quad\nx(\\tau) = x(\\varepsilon \\eta) = \\tilde{x}(\\eta), \\quad\ny(\\tau) = y(\\varepsilon \\eta) = \\tilde{y}(\\eta).\n\\]\nThe scale \\(\\eta\\) is fast, because one unit of \\(\\eta\\) time corresponds to \\(\\varepsilon\\) units of \\(\\tau\\) time. The new system reads: \\[\n\\left\\{\n\\begin{aligned}\n\\frac{\\mathrm{d}\\tilde{x}_\\varepsilon}{\\mathrm{d}\\eta} &= \\varepsilon \\Bigl( -\\tilde{x}_\\varepsilon + \\tilde{y}_\\varepsilon(\\tilde{x}_\\varepsilon+\\alpha) \\Bigr), \\\\\n\\frac{\\mathrm{d}\\tilde{y}_\\varepsilon}{\\mathrm{d}\\eta} &= \\tilde{x}_\\varepsilon -\\tilde{y}_\\varepsilon(\\tilde{x}_\\varepsilon + \\beta), \\\\\n\\tilde{x}_\\varepsilon(0) &= 1, \\\\\n\\tilde{y}_\\varepsilon(0) &= 0.\n\\end{aligned}\n\\right.\n\\]\nAs before, we formally take \\(\\varepsilon = 0\\), and obtain: \\[\n\\left\\{\n\\begin{aligned}\n\\frac{\\mathrm{d}\\tilde{x}}{\\mathrm{d}\\eta} &= 0, \\\\\n\\frac{\\mathrm{d}\\tilde{y}}{\\mathrm{d}\\eta} &= \\tilde{x} -\\tilde{y}(\\tilde{x} + \\beta), \\\\\n\\tilde{x}(0) &= 1, \\\\\n\\tilde{y}(0) &= 0.\n\\end{aligned}\n\\right.\n\\]\nThe perturbation in this case is not singular, we can solve it and find: \\[\n\\begin{aligned}\n\\tilde{x}(\\eta) &= 1, \\\\\n\\tilde{y}(\\eta) &= \\frac{1 - e^{-(1+\\beta)\\eta}}{1+\\beta}.\n\\end{aligned}\n\\]\nThe second equation follows from the integration of the second ODE, which is linear. The solution of the fast problem (in the variable \\(\\eta\\)) represents the dynamic from the initial condition up to‚Ä¶ something we will see. For \\(\\eta\\to\\infty\\), thus for long time in the metric of the fast system (but not so long time for the slow system), we have: \\[\n\\begin{aligned}\n\\tilde{x}(\\eta) &\\to 1, \\\\\n\\tilde{y}(\\eta) &\\to \\frac{1}{1+\\beta}.\n\\end{aligned}\n\\]\nInterestingly, for long \\(\\eta\\) the fast solution matches the slow solution \\((x(\\tau),y(\\tau))\\) for \\(\\tau=0\\)! In other words, the equilibrium of the fast system provides a compatible initial condition for the slow system.\nWe are now in condition to approximate the solution as \\(\\varepsilon\\to0\\):\n\nFor \\(\\tau\\in[0,\\varepsilon)\\), we have the fast solution \\[\n\\begin{aligned}\nx(\\tau) &= 1, \\\\\ny(\\tau) &= \\frac{1 - e^{-\\frac{1+\\beta}{\\varepsilon}\\tau}}{1+\\beta}.\n\\end{aligned}\n\\]\nFor \\(\\tau &gt; \\varepsilon\\), we have the slow solution: \\[\n\\begin{aligned}\n\\frac{\\mathrm{d}x}{\\mathrm{d}\\tau} &= -x + \\frac{x(x+\\alpha)}{x+\\beta}, \\\\\ny(\\tau) &= \\frac{x(\\tau)}{x(\\tau)+\\beta}.\n\\end{aligned}\n\\]\n\nNote that in the limit we ‚Äúforget‚Äù the fast dynamic.\n\n9.2.1 The Tichonov theorem\nIn general, we can consider the following problem: \\[\n\\left\\{\\begin{aligned}\n\\varepsilon \\frac{\\mathrm{d}z_\\varepsilon}{\\mathrm{d}t} &= F(z_\\varepsilon,y_\\varepsilon,\\varepsilon), \\\\\n\\frac{\\mathrm{d}y_\\varepsilon}{\\mathrm{d}t} &= f(z_\\varepsilon,y_\\varepsilon,\\varepsilon), \\\\\nz_\\varepsilon(t_0) &= y_0, \\\\\ny_\\varepsilon(t_0) &= z_0.\n\\end{aligned}\\right.\n\\]\nSuppose that we satisfy all the hypotheses for well-posedness.\nWhen \\(\\varepsilon \\ll 1\\), we identify \\(z(t)\\) as the fast variable, and \\(y(t)\\) as the slow variable.\nLet \\(z = \\phi(y)\\) be a root of the equation \\(F(z,y,0)=0\\) defined in some closed and bounded domain \\(D\\subset\\mathbb{R}^m\\), that is, \\(F(\\phi(y),y,0)=0\\) for \\(y\\in D\\). Consider the degenerate (slow) system: \\[\n\\left\\{\\begin{aligned}\n\\frac{\\mathrm{d}y}{\\mathrm{d}t} &= f\\bigl(\\phi(y),y,0\\bigr), \\\\\ny(t_0) &= y_0,\n\\end{aligned}\\right.\n\\]\nwhere we denote by \\(y(t)\\) its solution. We would like to show that the solution \\((\\phi(y(t)),y(t))\\) is the limit of the original solution \\((z_\\varepsilon(t),y_\\varepsilon(t))\\) as \\(\\varepsilon \\to 0\\).\nWe have the following result.\n\nTheorem 9.1 Suppose that\n\n\\(z=\\phi(y)\\) is an isolated root in \\(D\\), positively stable uniformly in \\(y\\in D\\) with respect to the adjointed system \\[\n\\frac{\\mathrm{d}z}{\\mathrm{d}\\tau} = F(z,y,0), \\quad \\tau = \\frac{t}{\\varepsilon}.\n\\]\nthe initial point \\((z_0,y_0)\\) is in the basin of attraction of the equilibrium \\(\\phi(y)\\),\nthe solution \\(y(t)\\) of the degerate system belongs to \\(D\\) for all \\(t\\in[t_0,T]\\).\n\nThen the solution \\((z_\\varepsilon(t),y_\\varepsilon(t))\\) of the overall system tends to the degenerate solution \\((\\phi(y(t)),y(t))\\) as \\(\\varepsilon \\to 0\\), in the sense that, for any \\(T_0 \\in (t_0, T)\\), \\[\n\\lim_{\\varepsilon\\to 0} (z_\\varepsilon(t), y_\\varepsilon(t)) = (\\phi(y(t)),y(t)),\n\\]\nfor \\(t_0 &lt; t &lt; T_0 \\le T\\).\n\nThe proof of the theorem is technical, but intuitive. At the beginning, we have the fast dynamics, that will bring the initial point \\((z_0,y_0)\\) at equilibrium for the fast system. This happens for \\(y\\) fixed and \\(z(\\tau)\\) converging towards the equilibrium \\(z=\\phi(y)\\). This dynamic occurs in \\(\\mathcal{O}(\\varepsilon)\\) units of time. So, as \\(\\varepsilon\\to 0\\) we can ignore it and suppose that we are already at equilibrium and starting from \\((\\phi(y_0),y_0)\\). Once at equilibrium, we have the slow dynamics in \\(y(t)\\). Note that we cannot deviate from the manifold \\(z = \\phi(y)\\), because it is asymptotically stable and we converge quickly towards it.\nWhat if we move off the manifold? For instance, consider the spruce-budworm system, \\[\n\\left\\{\\begin{aligned}\n\\varepsilon u' &= \\rho u (1-u/q) - \\frac{u^2}{1+u^2},\\\\\n\\rho' &= f(u,\\rho,q), \\\\\nq'    &= g(u,\\rho,q),\n\\end{aligned}\\right.\n\\]\nwhere \\(u(t)\\) is fast, and \\(\\rho(t)\\) and \\(q(t)\\) are slow. We know that \\[\n\\psi(u,\\rho,q) = \\rho (1-u/q) - \\frac{u}{1+u^2} = 0,\n\\]\ndefines a surface in \\(\\mathbb{R}^3\\) of equilibria for the fast dynamics. However, we cannot represent this as a function: \\[\nu = \\phi(\\rho,q),\n\\]\nor at least not always. The function \\(\\phi\\) would be multi-valued (for some choice of the parameters we can have up to 3 equilibria).\n\nWe can define multiple surfaces: \\[\n\\begin{aligned}\nu &= \\phi_1(\\rho,q), \\quad (\\rho,q) \\in D_1, \\\\\nu &= \\phi_2(\\rho,q), \\quad (\\rho,q) \\in D_2, \\\\\nu &= \\phi_3(\\rho,q), \\quad (\\rho,q) \\in D_3,\n\\end{aligned}\n\\]\none for each equilibrium. The domains overlap: in the bistable region, all functions are well-defined. Note also that \\(\\phi_2\\) is always unstable, so we will never go there.\nNow we can apply the Theorem, very some care concerning the initial conditions. Depending on where we start, we may go to a different branch.\nOnce on the surface, we have the slow dynamics: \\[\n\\left\\{\\begin{aligned}\n\\rho' &= f(\\phi_i(\\rho,q),\\rho,q), \\\\\nq'    &= g(\\phi_i(\\rho,q),\\rho,q),\n\\end{aligned}\\right.\n\\]\nwith \\(i=1,3\\) depending on the stable branch. Actually, if the initial condition \\((\\rho_0,q_0)\\in D_2\\), the bistable region, then \\(i=1\\) if \\(u_0 &lt; \\phi_2(\\rho_0,q_0)\\) otherwise \\(i=3\\). If \\((\\rho_0,q_0)\\in D_i \\setminus D_2\\), that is away from the bistable region, then the branch is \\(\\phi_i\\).\nUntil \\((\\rho(t),q(t))\\in D_i\\), the dynamic proceeds with no issues. However, if \\((\\rho(t),q(t))\\) reaches the boundary of \\(D_i\\), then the slow dynamic is over. Say that this happens at some time \\(T_0 &gt; t_0\\).\nWe can continue, and apply again the Tichonov theorem with initial condition slightly off the ‚Äúrim‚Äù of the surface, and look for another stable branch of equilibria. Then we go there and we will have a new slow dynamic.\nBelow 2 examples: in the first case we move counterclockwise in the \\((\\rho,q)\\) plane. In the other, we move clockwise.\n\n\nNote also the temporal dynamic, where \\(u\\) proceeds smoothly until a sudden drop (around \\(t=12\\) and \\(t=34\\)). This is when we have the fast dynamic.\n\n\n\n\n\nKeener, James, and James Sneyd. 2009. Mathematical Physiology: I: Cellular Physiology. Second. Vol. 8/I. Springer. https://strutture-provincia.primo.exlibrisgroup.com/permalink/39SBT_INST/ghkipk/alma991012367489706186.",
    "crumbs": [
      "Lectures",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Fast-Slow systems</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Elton, Charles, and Mary Nicholson. 1942. ‚ÄúThe Ten-Year Cycle in\nNumbers of the Lynx in Canada.‚Äù The Journal of Animal\nEcology, 215‚Äì44. https://doi.org/10.2307/1358.\n\n\nGause, G. F. 1934. The Struggle for Existence. William &\nWilkins Company.\n\n\nHastings, Alan, and Thomas Powell. 1991. ‚ÄúChaos in a Three-Species\nFood Chain.‚Äù Ecology 72 (3): 896‚Äì903.\n\n\nHolling, Crawford Stanley. 1965. ‚ÄúThe Functional Response of\nPredators to Prey Density and Its Role in Mimicry and Population\nRegulation.‚Äù The Memoirs of the Entomological Society of\nCanada 97 (S45): 5‚Äì60. https://doi.org/10.4039/entm9745fv.\n\n\nIannelli, Mimmo, and Andrea Pugliese. 2014. An Introduction to\nMathematical Population Dynamics. Vol. 79. Springer. https://strutture-provincia.primo.exlibrisgroup.com/permalink/39SBT_INST/ghkipk/alma991006883209706186.\n\n\nKeener, James, and James Sneyd. 2009. Mathematical Physiology: I:\nCellular Physiology. Second. Vol. 8/I. Springer. https://strutture-provincia.primo.exlibrisgroup.com/permalink/39SBT_INST/ghkipk/alma991012367489706186.\n\n\nLengyel, Istvan, Gyula Rabai, and Irving R Epstein. 1990.\n‚ÄúExperimental and Modeling Study of Oscillations in the Chlorine\nDioxide-Iodine-Malonic Acid Reaction.‚Äù Journal of the\nAmerican Chemical Society 112 (25): 9104‚Äì10.\n\n\nLudwig, Donald, Dixon D Jones, Crawford S Holling, et al. 1978.\n‚ÄúQualitative Analysis of Insect Outbreak Systems: The Spruce\nBudworm and Forest.‚Äù Journal of Animal Ecology 47 (1):\n315‚Äì32. https://doi.org/10.2307/3939.\n\n\nPulley, Melissa. 2020. ‚ÄúThe Marshmallow Lab: A Project-Based\nApproach to Understanding Functional Responses.‚Äù Master‚Äôs thesis,\nUtah State University. https://doi.org/10.26076/neyp-cc41.\n\n\nTyson, John J, Katherine C Chen, and Bela Novak. 2003. ‚ÄúSniffers,\nBuzzers, Toggles and Blinkers: Dynamics of Regulatory and Signaling\nPathways in the Cell.‚Äù Current Opinion in Cell Biology\n15 (2): 221‚Äì31.\n\n\nTyson, John J, Christian I Hong, C Dennis Thron, and Bela Novak. 1999.\n‚ÄúA Simple Model of Circadian Rhythms Based on Dimerization and\nProteolysis of PER and TIM.‚Äù Biophysical Journal 77 (5):\n2411‚Äì17.\n\n\nVolterra, Vito. 1932. Le√ßons Sur La Th√©orie Math√©matique de La Lutte\nPour La Vie. Edited by Gauthier-Villars et Cie √âditeurs. Paris. https://matematicaitaliana.sns.it/opere/427/.",
    "crumbs": [
      "Lectures",
      "References"
    ]
  },
  {
    "objectID": "Assignments/01_recap_ODEs.html",
    "href": "Assignments/01_recap_ODEs.html",
    "title": "Solving ODEs",
    "section": "",
    "text": "The logistic model\nConsider the following model for a population: \\[\nN' = \\beta(N)N - \\mu(N) N,\n\\]\nwhere \\(\\beta(N)\\) is the fertility rate and \\(\\mu(N)\\) is the mortality rate. We assume here both are function of the population size \\(N\\) as follows: \\[\n\\begin{aligned}\n\\beta(N) &= \\beta_0 - \\tilde{\\beta} N, \\\\\n\\mu(N)   &= \\mu_0   + \\tilde{\\mu} N.\n\\end{aligned}\n\\]",
    "crumbs": [
      "Assignments",
      "Solving ODEs"
    ]
  },
  {
    "objectID": "Assignments/01_recap_ODEs.html#the-logistic-model",
    "href": "Assignments/01_recap_ODEs.html#the-logistic-model",
    "title": "Solving ODEs",
    "section": "",
    "text": "Show that the model can be written as \\[N' = r\\Bigl(1-\\frac{N}{K}\\Bigr)N.\\tag{1}\\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSubstituting we get: \\[\n\\begin{split}\nN' &= \\beta(N)N - \\mu(N)N = (\\beta_0 - \\mu_0)N - (\\tilde{\\mu}+\\tilde{\\beta})N^2 \\\\\n&= (\\beta_0 - \\mu_0)\\Bigl( 1 - \\frac{\\tilde{\\mu}+\\tilde{\\beta}}{\\beta_0 - \\mu_0} N \\Bigr) N \\\\\n&= r \\Bigl( 1 - \\frac{N}{K} \\Bigr) N,\n\\end{split}\n\\]\nwith \\(r = \\beta_0 - \\mu_0\\) and \\(K = r / \\tilde{\\mu}+\\tilde{\\beta}\\).\n\n\n\n\nProve that the ODE, supplemented with an initial condition \\(N(0)=N_0\\), has a unique local solution. Hint: verify the hypotheses of the Cauchy-Lipschitz theorem.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe right hand side \\[\nf(t,y) = r \\Bigl( 1 - \\frac{N}{K} \\Bigr) N\n\\]\nis a polynomial in \\(y\\) with no explicit dependency on \\(t\\). Thus, it is continuous and also locally Lipschitz. We can apply the Cauchy-Lipschitz theorem and conclude that there exists a unique local solution.\n\n\n\n\nConsider \\(u(t) = 1/N(t)\\). Show that \\(u(t)\\) satisfies a linear ODE. Solve it, then use the solution to solve the original ODE in \\(N(t)\\) with the initial condition \\(N(0) = N_0 &gt; 0\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe have \\[u'= -\\frac{N'}{N^2} = -\\frac{r}{N^2}\\Bigl(1 - \\frac{N}{K}\\Bigr)N = r\\Bigl(\\frac{1}{K} - \\frac{1}{N}\\Bigr) = r\\Bigl(\\frac{1}{K} - u \\Bigr),\\]\nwhich is linear with respect to \\(u\\). To solve the ODE, we can set \\(\\tilde{u}=u-K^{-1}\\), \\(\\tilde{u}'=u'\\), so the ODE reduces to \\(\\tilde{u}'= -r\\tilde{u}\\). The general solution is: \\[u(t) = \\tilde{u}(t) + \\frac{1}{K} = \\frac{1}{K} + C e^{-r t}.\\]\nSince \\(u(0) = 1/N(0) = 1/N_0\\), we can determine the constant: \\[u(t) = \\frac{1}{K} + \\tilde{u}(t) = \\frac{1}{K} - \\Bigl(\\frac{1}{N_0} - \\frac{1}{K} \\Bigr)e^{-rt}.\\]\nIf we invert \\(u(t)\\) we obtain \\[N(t) = \\frac{K N_0}{N_0 + (K-N_0)e^{-rt}}\\]\nwhich is the sought solution of the logistic equation.\n\n\n\n\nMake the change of variables as follows: \\(\\tau = r t\\) and \\(y = N/K\\). Find the corresponding equation in \\(y(\\tau)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe simply note that \\[\\dot{y}=\\frac{\\mathrm{d}y}{\\mathrm{d}\\tau}=\\frac{1}{Kr}\\frac{\\mathrm{d}y}{\\mathrm{d} t} = \\frac{1}{Kr}r Ky(1-y) = y(1-y).\\]\nSo the equation does not depend on the parameters anymore.",
    "crumbs": [
      "Assignments",
      "Solving ODEs"
    ]
  },
  {
    "objectID": "Assignments/01_recap_ODEs.html#blow-up-of-solutions",
    "href": "Assignments/01_recap_ODEs.html#blow-up-of-solutions",
    "title": "Solving ODEs",
    "section": "Blow-up of solutions",
    "text": "Blow-up of solutions\nIt may be considered reasonable that, for a sexual species, births are proportional to the number of encounters, hence, disregarding the mortality process, we have the equation \\[N'(t) = \\beta N^2(t).\\]\n\nThis equation can be solved by the method of separation of variables. Show that the solutions of this equation with \\(N(0) &gt; 0\\) tend to infinity in a finite time. (Blow-up of solution.)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can solve by separation of variables: \\[\\int_{N_0}^N \\frac{\\mathrm{d}N}{N^2}=\\int_0^t \\beta \\mathrm{d}t,\\quad\\leadsto\\quad N(t) = \\frac{N_0}{1 - N_0 \\beta t}.\\]\nSince \\(N_0&gt;0\\) and \\(\\beta&gt;0\\), there is a vertical asymptote at \\(\\bar{t} = \\frac{1}{\\beta N_0}\\). Hence, we have a blow-up of the solution in finite time.\n\n\n\n\nLet us correct the equation, introducing deaths: \\[N'(t) = \\beta N^2(t) ‚àí \\mu N(t).\\]\n\nDiscuss how the dynamics changes. In particular, does the equation still have the problem of solutions going to infinity in a finite time? Hint: do not solve analytically the equation.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYes, it does. Consider \\(N_0\\) large, then \\(f(N)=\\beta N^2-\\mu N \\approx \\beta N^2\\) so we expect the same problem.\n\n\n\n\nSolve the above equation analytically and confirm the qualitative analysis of the previous point.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf we rewrite the problem as \\[\nN' = -\\mu N (1 - \\tfrac{\\beta}{\\mu} N),\n\\]\nwe recover the logistic equation ‚Äúwith the wrong sign‚Äù, with \\(r=-\\mu\\) and \\(K = \\tfrac{\\mu}{\\beta}\\). We can use the solution from the previous exercise: \\[N(t) = \\frac{K N_0}{N_0 + (N_0-K)e^{-rt}}.\\]\nSince \\(r = -\\mu &lt; 0\\), the denominator can be zero when: \\[\ne^{\\mu t} = \\frac{N_0}{K-N_0} \\ge 1,\n\\]\nThis is the case when \\(N_0 \\ge K = \\tfrac{\\mu}{\\beta}\\).",
    "crumbs": [
      "Assignments",
      "Solving ODEs"
    ]
  },
  {
    "objectID": "Assignments/01_recap_ODEs.html#mrna",
    "href": "Assignments/01_recap_ODEs.html#mrna",
    "title": "Solving ODEs",
    "section": "mRNA",
    "text": "mRNA\nZeisel et al. considered the following system for the concentration of mRNA. The variables are \\(M(t)\\), the concentration of mature mRNA, and \\(P(t)\\), the concentration of precursor mRNA: \\[\n\\left\\{\\begin{aligned}\nP'(t) &= b(t) ‚àí \\alpha_1 P(t), \\\\\nM'(t) &= \\alpha_1 P(t) ‚àí \\alpha_2(t) M(t).\n\\end{aligned}\\right.\n\\]\nwhere \\(b(t)\\) is the (gene-specific and time-dependent) production rate, \\(\\alpha_1\\) is the conversion (splicing) rate of pre-mRNA to mRNA, \\(\\alpha_2(t)\\) the (time-dependent) degradation rate of mRNA.\n\nAssume \\(b(t) \\equiv b\\) and \\(\\alpha_2(t) \\equiv \\alpha_2\\) are constant. Find the explicit solution given \\(P(0)=P_0\\) and \\(M(0)=M_0\\). Hint: use the method of variation of constants.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe note that the first equation does not depend on \\(M(t)\\), so it can be solved independently. The solution is: \\[\nP(t) = e^{-\\alpha_1 t} P_0 + \\frac{b}{\\alpha_1}(1-e^{-\\alpha_1 t}).\n\\]\nNow we can move this into the equation for \\(M(t)\\): \\[\nM'(t) = e^{-\\alpha_1 t} P_0 + b (1-e^{-\\alpha_1 t}) - \\alpha_2 M(t).\n\\]\nHere, we use the variation of constants. The solution of the homogeneous equation, denoted by \\(M_H(t)\\) is: \\[\nM_H(t) = C e^{-\\alpha_2 t},\n\\]\nso we look for solutions of the form: \\[\nM(t) = C(t) e^{-\\alpha_2 t}.\n\\]\nSubstituting: \\[\nM'(t) = C'(t) e^{-\\alpha_2 t} - \\alpha_2 C(t) e^{-\\alpha_2 t} =\nC'(t) e^{-\\alpha_2 t} - \\alpha_2 M(t).\n\\]\nEquating this with the original equation we find an equation for \\(C(t)\\): \\[\n\\begin{split}\nC'(t) &= \\alpha_1 e^{-(\\alpha_1-\\alpha_2) t} P_0 + b (1-e^{-\\alpha_1 t}) e^{\\alpha_2 t} \\\\\n&= b e^{\\alpha_2 t} + (\\alpha_1 P_0 - b) e^{-(\\alpha_1-\\alpha_2) t},\n\\end{split}\n\\]\nthat integrated gives \\(C(t)\\) \\[\nC(t) = C(0) + \\tfrac{b}{\\alpha_2} e^{\\alpha_2 t} - \\tfrac{\\alpha_1 P_0 - b}{\\alpha_1-\\alpha_2} e^{-(\\alpha_1-\\alpha_2) t}\n\\]\nand the final solution: \\[\nM(t) = C(0) e^{-\\alpha_2 t} + \\tfrac{b}{\\alpha_2} - \\tfrac{\\alpha_1 P_0 - b}{\\alpha_1-\\alpha_2} e^{-\\alpha_1 t}.\n\\]\nThe integration constant \\(C(0)\\) can be selected so that \\(M(0)=M_0\\): \\[\nC(0) = M_0 - \\tfrac{b}{\\alpha_2} + \\tfrac{\\alpha_1 P_0 - b}{\\alpha_1-\\alpha_2}.\n\\]\n\n\n\n\nAssume now that \\(\\alpha_2(t) \\equiv \\alpha_2\\) and \\[\nb(t) = \\begin{cases}\n\\bar{b} & t &lt; \\bar{t}, \\\\ 0, & t &gt; \\bar{t}.\n\\end{cases}\n\\]\n\nShow that \\(M(t)\\) tends to \\(0\\) as \\(t\\to\\infty\\). At which rate does it decay?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNote that \\(b(t)\\) is not continuous, so we cannot expect a classical solution. However, we can proceed step by step. Informally, we can integrate up to \\(t = \\bar{t}^-\\) with \\(b(t)=\\bar{b}\\). The solution \\(P(\\bar{t})\\) can then be used for a new problem with \\(P_0 = \\bar{P} = P(\\bar{t})\\) and \\(b(t) = 0\\). The solution for \\(P(t)\\) will therefore decay as \\(e^{-\\alpha_1 t}\\). For \\(M(t)\\), we use the above formula, which holds true for \\(t &gt; \\bar{t}\\), setting \\(b=0\\) and \\(P_0 = \\bar{P}\\): \\[\nM(t) = C(0) e^{-\\alpha_2 t} - \\tfrac{\\alpha_1 \\bar{P}}{\\alpha_1-\\alpha_2} e^{-\\alpha_1 t}.\n\\]\nWe see that \\(M(t)\\to 0\\), no matter of the value of the constants. However, if \\(\\alpha_1 &gt; \\alpha_2\\), the second term \\(e^{-\\alpha_1 t}\\) will vanish quickier than the first one, so \\(M(t) \\sim e^{-\\alpha_2 t}\\) for large \\(t\\). In general \\[\nM(t) \\sim e^{-\\min\\{ \\alpha_1, \\alpha_2 \\}t}.\n\\]\nAlternatively, we can do all the computations. First, we integrate up to \\(t = \\bar{t}\\), with \\(b(t) = \\bar{b}\\), the following problem (we neglect \\(M(t)\\) for now): \\[\n\\left\\{ \\begin{aligned}\nP_1'(t) &= \\bar{b} - \\alpha_1 P_1(t), \\\\\nP_1(0)  &= P_0,\n\\end{aligned}\\right.\n\\]\nwhich has the solution \\[\nP_1(t) = e^{-\\alpha_1 t}P_0 + \\frac{\\bar{b}}{\\alpha_1}(1-e^{-\\alpha_1 t}).\n\\]\nfor \\(0 \\le t &lt; \\bar{t}\\). Then, for \\(t &gt; \\bar{t}\\), we solve another problem with \\(b(t) = 0\\): \\[\n\\left\\{ \\begin{aligned}\nP_2'(t) &= 0 - \\alpha_1 P_2(t), \\\\\nP_2(\\bar{t})  &= P_1(\\bar{t}),\n\\end{aligned}\\right.\n\\]\nwhich has the solution \\[\nP_2(t) = e^{-\\alpha_1(t-\\bar{t})} P_1(\\bar{t})\n= e^{-\\alpha_1 t} \\Bigl[ P_0 + \\frac{\\bar{b}}{\\alpha_1} (e^{\\alpha_1 \\bar{t}} - 1 ) \\Bigr].\n\\]\nThe overall solution is (which is continuous at \\(t=\\bar{t}\\) but not differentiable): \\[\nP(t) = \\begin{cases}\nP_1(t), & t \\le \\bar{t}, \\\\\nP_2(t), & t &gt; \\bar{t}.\n\\end{cases}\n\\]\nClearly, \\(P(t)\\to 0\\) as \\(t\\to\\infty\\), and it decays as \\(e^{-\\alpha_1 t}\\). We can use this information for \\(M(t)\\). In general: \\[\nM(t) = e^{-\\alpha_2 t} M_0 + \\alpha_1 \\int_0^t e^{-\\alpha_2(t-s)} P(s)\\,\\mathrm{d}s.\n\\]\nSince we are interested in large \\(t\\), we can start from \\(\\bar{t}\\) instead of \\(t=0\\). So \\(P(t) = P_2(t) = c e^{-\\alpha_1 t}\\): \\[\n\\begin{split}\nM(t) &= e^{-\\alpha_2 t} M(\\bar{t}) + c \\alpha_1 \\int_{\\bar{t}}^t e^{-\\alpha_2(t-s)} e^{-\\alpha_1 s}\\,\\mathrm{d}s \\\\\n&= e^{-\\alpha_2 t} M(\\bar{t}) + c \\alpha_1 e^{-\\alpha_2 t} \\int_{\\bar{t}}^t e^{(\\alpha_2-\\alpha_1) s} \\,\\mathrm{d}s \\\\\n&= e^{-\\alpha_2 t} M(\\bar{t}) + c \\alpha_1 e^{-\\alpha_2 t} \\Bigl( \\frac{1}{\\alpha_2-\\alpha_1} e^{(\\alpha_2-\\alpha_1) t} - \\tilde{c} \\Bigr) \\\\\n&= e^{-\\alpha_2 t} \\hat{c} + \\frac{\\alpha_1}{\\alpha_2-\\alpha_1} e^{-\\alpha_1 t}.\n\\end{split}\n\\]",
    "crumbs": [
      "Assignments",
      "Solving ODEs"
    ]
  },
  {
    "objectID": "Assignments/01_recap_ODEs.html#thorium-uranium-dating",
    "href": "Assignments/01_recap_ODEs.html#thorium-uranium-dating",
    "title": "Solving ODEs",
    "section": "Thorium-Uranium dating",
    "text": "Thorium-Uranium dating\nThe thorium-uranium method for dating rocks is based on the fact that Uranium-234 decays into Thorium-230 which in turn decays into other elements. Set \\(t = 0\\) the rock formation time and denoting \\(U(t)\\) (resp. \\(T(t)\\)) the amount of Uranium-234 (resp. Thorium-230) in the rock at time \\(t\\) (measured in years), the following differential equation system is written: \\[\n\\left\\{\\begin{aligned}\nU'(t) &= -a U(t), \\\\\nT'(t) &= a U(t) - b T(t), \\\\\nU(0) &= U_0, \\\\\nT(0) &= 0,\n\\end{aligned}\\right.\n\\]\nwhere \\(a \\approx 5.9\\cdot 10^{-6} \\:\\text{years}^{-1}\\), \\(b \\approx 1.9\\cdot 10^{-5} \\:\\text{years}^{-1}\\), \\(U_0\\) represents the initial (generally unknown) amount of Uranium-234. Note that, based on geological principles, it is believed that there was no thorium at the time of rock formation.\n\nSolve the equation for \\(U(t)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe have \\(U(t) = e^{-a t} U_0\\).\n\n\n\n\nHow can the quantities of \\(a\\) and \\(b\\) be interpreted? From the data provided can we infer the half-life of Uranium-234 and Thorium-230?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe half-life is the time \\(t_{1/2}\\) that takes to have \\(U(t_{1/2}) = \\frac{1}{2} U_0\\). From the above solution we find \\(t_{1/2} = \\frac{\\log 2}{a}\\), so \\(\\frac{1}{a}\\) is proportional to the half-life. Actually, \\(\\frac{1}{a}\\) is the time that takes to \\(U_0\\) to reach \\(\\frac{1}{e}U_0 \\approx 0.36 U_0\\). So, the half-life of Uranium-234 is \\(\\approx 117\\,500\\) years, whereas for Thorium-230 (in absence of uranium) is \\(\\approx 36\\,500\\) years.\n\n\n\n\nCalculate \\(T(t)\\), solution of the second differential equation.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe substitute the solution \\(U(t)\\) into the second equation, so we need to solve: \\[\nT' = a e^{-a t} U_0 - b T,\n\\]\nwith \\(T(0)=0\\). This equation is linear but non-homogeneous. The Wronskian ‚Äúmatrix‚Äù is \\(e^{-bT}\\), so we have \\[\n\\begin{split}\nT(t) &= e^{-b T}T(0) + \\int_0^t e^{-b(t-s)} a e^{-a s} U_0\\,\\mathrm{d}s \\\\\n&= a U_0 e^{-bt} \\int_0^t e^{(b-a) s} \\,\\mathrm{d}s \\\\\n&= \\frac{a U_0}{b-a} e^{-bt} \\Bigl( e^{(b-a) t} - 1 \\Bigr) \\\\\n&= \\frac{a U_0}{b-a} \\Bigl( e^{-a t} - e^{-b t} \\Bigr).\n\\end{split}\n\\]\n\n\n\n\nCompute \\[\n\\lim_{t\\to\\infty}\\frac{T(t)}{U(t)}.\n\\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe have the full solution, so we can easily compute it: \\[\n\\frac{T(t)}{U(t)} = \\frac{\\frac{a U_0}{b-a}(e^{-a t} - e^{-b t})}{e^{-a t} U_0} = \\frac{a}{b-a}\\bigl(1 - e^{-(b-a)t}\\bigr) \\to \\frac{a}{b-a}.\n\\]\nNote that we could have proceeded differently, by observing that: \\[\n\\left(\\frac{T(t)}{U(t)}\\right)' = \\frac{T'(t)}{U(t)} - \\frac{T(t)}{U^2(t)}U'(t) = a - (b-a)\\frac{T(t)}{U(t)}.\n\\]\nThe differential equation for \\(w(t) = T(t)/U(t)\\) is: \\[\nw' = a - (b-a)w.\n\\]\nThe solution is the same as above.\n\n\n\n\nExplain why it is possible to estimate the rock age from the knowledge of \\(T / U\\) at current time, but it is not possible from the knowledge of \\(T\\) alone. Hint: study the function \\(T(t)/U(t)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe function \\(w(t) = T(t)/U(t)\\) is monotonically increasing, bounded from above by \\(\\frac{a}{b-a}\\). It does not depend on the initial concentration \\(U_0\\). Thus, it is always possible to find a unique time \\(\\tilde{t}\\) such that \\[\nw(\\tilde{t}) = \\frac{T(\\tilde{t})}{U(\\tilde{t})} = \\tilde{w},\n\\]\nwhere \\(\\tilde{w}\\) is the thorium-uranium ratio at time \\(\\tilde{t}\\). In particular: \\[\n\\tilde{t} = \\frac{1}{b}\\ln\\Bigl( \\frac{a}{a-(b-a)\\tilde{w}} \\Bigr).\n\\]\nWe cannot use \\(T(t)\\), because it is not a monotone function. Thus, given \\(\\tilde{T}\\), we cannot find a unique \\(\\tilde{t}\\) such that \\(T(\\tilde{t}) = \\tilde{T}\\).",
    "crumbs": [
      "Assignments",
      "Solving ODEs"
    ]
  },
  {
    "objectID": "Assignments/02_population.html",
    "href": "Assignments/02_population.html",
    "title": "Population dynamics",
    "section": "",
    "text": "Periodic solutions\nConsider the time dependent Malthus model \\[\n\\begin{cases}\nN(t) = r(t)N(t), &\\\\\nN(0) = N_0. &\n\\end{cases}\n\\tag{1}\\]\nwith periodic Malthus parameter: \\(r(t + T) = r(t)\\) and denote by \\(\\bar r\\) the average over one period: \\[\n\\bar r = \\frac{1}{T}\\int_0^T r(s)\\,\\mathrm{d}s.\n\\tag{2}\\]\nis periodic with period \\(T\\), prove that \\[\n\\bar r = \\lim_{t\\to\\infty} \\frac{1}{t}\\int_0^t r(s)\\,\\mathrm{d}s.\n\\]\nwhere \\(N_\\pi(t)\\) is a periodic function.\nassuming both \\(r(t)\\) and \\(m(t)\\) continuous and periodic with period \\(T\\). Using Equation¬†2, show that, if \\(\\bar{r} &lt; 0\\), then the function \\[\nN_\\infty(t) = \\int_{-\\infty}^t e^{\\int_s^t r(\\sigma)\\,\\mathrm{d}\\sigma} m(s)\\,\\mathrm{d}s\n\\]\nis well-defined, is a solution to the ODE, and is periodic with period \\(T\\).\nand is such that \\(\\lim_{t\\to\\infty}(N(t) - N_\\infty(t)) = 0\\).",
    "crumbs": [
      "Assignments",
      "Population dynamics"
    ]
  },
  {
    "objectID": "Assignments/02_population.html#periodic-solutions",
    "href": "Assignments/02_population.html#periodic-solutions",
    "title": "Population dynamics",
    "section": "",
    "text": "After showing that the function \\[\n\\pi(t) = \\int_0^t r(s)\\,\\mathrm{d}s - \\bar{r} t\n\\]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst, we need to show that \\(\\pi(t+T)=\\pi(t)\\) for all \\(t\\). Hence, with \\(t\\in(0,T)\\), we have: \\[\\begin{split}\\pi(t+T) &= \\int_0^{t+T} r(s)\\,\\mathrm{d}s - \\bar{r} (t+T)\\\\ &= \\int_0^t r(s)\\,\\mathrm{d}s + \\int_t^{t+T} r(s)\\,\\mathrm{d}s - \\bar{r}T - \\bar{r}t \\\\ [\\tau=s-t] &= \\pi(t) + \\int_0^T r(\\tau+T)\\,\\mathrm{d}\\tau - \\bar{r}T \\\\ [r(\\tau+T)=r(t)] &= \\pi(t) + \\int_0^T r(\\tau)\\,\\mathrm{d}\\tau - \\bar{r}T \\\\  &= \\pi(t) + \\bar{r}T - \\bar{r}T = \\pi(t). \\end{split}\\]\nNow we can compute the limit: \\[\\lim_{t\\to\\infty} \\frac{1}{t}\\int_0^t r(s)\\,\\mathrm{d}s = \\lim_{t\\to\\infty} \\frac{\\pi(t)+\\bar{r}t}{t} = \\bar{r} + \\lim_{t\\to\\infty} \\frac{\\pi(t)}{t}.\\]\nThe second term is zero: in fact, since \\(\\pi(t)\\) is periodic, we can take \\(t=t_0+nT\\), \\(n=0,1,\\ldots\\) and see that \\(a_n := \\frac{\\pi(t_0+nT)}{t_0+nT} = \\frac{\\pi(t_0)}{t_0+nT}\\to 0\\) as \\(n\\to\\infty\\).\n\n\n\n\nShow that the solution of the ODE is \\[\nN(t) = e^{\\bar{r}t} N_\\pi(t),\n\\]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe solution of the ODE is \\[\nN(t) = N_0 e^{\\int_0^t r(s)\\,\\mathrm{d}s},\n\\]\nso using the previous point we get \\[\nN(t) = N_0 e^{\\bar{r}t + \\pi(t)} = e^{\\bar{r}t}N_\\pi(t),\n\\]\nwhere \\(N_\\pi(t) = N_0 e^{\\pi(t)}\\) is clearly \\(T\\)-periodic.\n\n\n\n\nModify the ODE by adding a time-dependent migration \\[\nN'(t) = r(t)N(t) + m(t),\n\\tag{3}\\]\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe observe that \\[\n\\int_s^t r(\\sigma)\\,\\mathrm{d}\\sigma = \\bar{r}(t-s) + \\pi(t)-\\pi(s),\n\\]\nso the function reads: \\[\nN_\\infty(t) = e^{\\int_0^t r(\\sigma)\\,\\mathrm{d}\\sigma} \\int_{-\\infty}^t e^{-\\bar{r}s}e^{-\\pi(s)} m(s)\\,\\mathrm{d}s.\n\\]\nThe integrand goes to zero as \\(s\\to-\\infty\\), since \\(\\bar{r}&lt;0\\) and \\(e^{-\\pi(s)}m(s)\\) is continuous and periodic, hence bounded. So \\(N_\\infty(t)\\) is well-defined. It is also continuously differentiable, because the integrand is continous. Differentiating we have: \\[\n\\begin{split}N'_\\infty(t) &= r(t) N_\\infty(t) + e^{\\int_0^t r(\\sigma)\\,\\mathrm{d}\\sigma} e^{-\\bar{r}t}e^{-\\pi(t)} m(t) \\\\\n&= r(t) N_\\infty(t) + m(t),\\end{split}\n\\]\nthus it satisfies the ODE.\nThe periodicity is shown as follows: \\[\n\\begin{split}N_\\infty(t+T) &= e^{\\bar{r}(t+T)+\\pi(t+T)} \\int_{-\\infty}^{t+T} e^{-\\bar{r}s}e^{-\\pi(s)} m(s)\\,\\mathrm{d}s \\\\ [\\tau=s-T] &= e^{\\bar{r}(t+T)+\\pi(t)} \\int_{-\\infty}^{t} e^{-\\bar{r}(\\tau+T)}e^{-\\pi(\\tau+T)} m(\\tau+T)\\,\\mathrm{d}\\tau \\\\  &= e^{\\int_0^t r(\\sigma)\\mathrm{d}\\sigma} e^{\\bar{r}T} \\int_{-\\infty}^{t} e^{-\\bar{r}T} e^{-\\bar{r}\\tau}e^{-\\pi(\\tau)} m(\\tau)\\,\\mathrm{d}\\tau = N_\\infty(t). \\end{split}\n\\]\n\n\n\n\nShow that if \\(\\bar{r} &lt; 0\\), then the solution to Equation¬†3 with initial condition \\(N(0)=N_0\\) is \\[\nN(t) = e^{\\int_0^t r(\\sigma)\\mathrm{d}\\sigma} N_0 + \\int_0^t e^{\\int_s^t r(\\sigma)\\mathrm{d}\\sigma} m(s)\\,\\mathrm{d}s\n\\]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe differentiate the solution: \\[\\begin{split}N'(t) &= r(t) e^{\\int_0^t r(\\sigma)\\,\\mathrm{d}\\sigma} N_0 + r(t) e^{\\int_0^t r(\\sigma)\\,\\mathrm{d}\\sigma} \\int_0^t e^{\\int_s^t r(\\sigma)\\,\\mathrm{d}\\sigma} m(s)\\mathrm{d}s \\\\ &+ e^{\\int_0^t r(\\sigma)\\mathrm{d}\\sigma} e^{-\\int_0^t r(\\sigma)\\mathrm{d}\\sigma} m(t) \\\\ &= r(t) N(t) + m(t).\\end{split}\\]\nFor the initial condition, \\(N(0)=N_0\\). Thus it satisfies the IVP.\nFor the limit: \\[\n\\begin{split}N(t)-N_\\infty(t) &= e^{\\int_0^t r(\\sigma)\\mathrm{d}\\sigma} N_0 - e^{\\int_0^t r(\\sigma)\\mathrm{d}\\sigma} \\int_{-\\infty}^0 e^{-\\int_0^s r(\\sigma)\\mathrm{d}\\sigma} m(s)\\mathrm{d}s \\\\ &= e^{\\bar{r}t}\\Bigl( N_\\pi(t) - N_\\infty(0) \\Bigr),\\end{split}\n\\]\nwhich tends to 0 as \\(t\\to\\infty\\) since \\(\\bar{r}&lt;0\\).",
    "crumbs": [
      "Assignments",
      "Population dynamics"
    ]
  },
  {
    "objectID": "Assignments/02_population.html#logistic-model",
    "href": "Assignments/02_population.html#logistic-model",
    "title": "Population dynamics",
    "section": "Logistic model",
    "text": "Logistic model\nConsider the logistic equation \\[\nN'(t) = r\\Bigl(1-\\frac{N}{K}\\Bigr)N.\n\\]\n\nFind the equilibria of the ODE and study their stability.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe equilibria are the zeros of the function \\[\nf(N) = rN\\Bigl(1-\\frac{N}{K}\\Bigr),\n\\]\nwhich cancels for \\(N=0\\) and \\(N=K\\). Thus there are 2 equilibria.\nThe local stability is determined by the sign of \\(f'(N)\\) at the equilibrium. We have \\[\nf'(N) = r\\Bigl(1-\\frac{2N}{K}\\Bigr),\n\\]\nso \\(f'(0) = r\\) and \\(f'(K)=-r\\). Hence, the equilibrium \\(N=0\\) is locally unstable whereas \\(N=K\\) is asymptotically stable.\n\n\n\n\nStudy the global stability of the equilibria.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSince \\(f(N)&gt;0\\) for \\(N\\in(0,K)\\), we have \\(N'&gt;0\\) so \\(N(t)\\) grows monotonically for its initial condition \\(N_0\\in(0,K)\\). But \\(N=K\\) is an equilibrium, which the trajectory cannot cross, hence \\(N(t)\\to K\\) from the left. Similarly, on the right of the equilibrium \\(N=K\\), we have that \\(N'=f(N)&lt;0\\) for \\(N&gt;K\\), thus \\(N(t)\\to K\\) from the right when \\(N_0&gt;K\\). In conclusion, the basin of attraction of \\(N=K\\) is \\(\\mathbb{R}^+\\) (zero excluded).\n\n\n\n\nWould it make sense to assume \\(r&lt;0\\) and \\(K&gt;0\\)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf \\(r&lt;0\\), the stability of the equilibria is swapped, that is \\(N=0\\) is locally stable and \\(N=K\\) is unstable, respectively. If \\(N_0\\in(0,K)\\), then \\(N(t)\\to 0\\). However, for \\(N_0&gt;K\\), the solution diverges. These assumptions do not make sense biologically speaking.",
    "crumbs": [
      "Assignments",
      "Population dynamics"
    ]
  },
  {
    "objectID": "Assignments/02_population.html#global-well-posedness",
    "href": "Assignments/02_population.html#global-well-posedness",
    "title": "Population dynamics",
    "section": "Global well-posedness",
    "text": "Global well-posedness\nConsider the following ODE model: \\[\nN'(t) = ( \\beta N^2 ‚àí \\mu N )(1-\\gamma N).\n\\]\nFind all non-negative equilibria and discuss their stability. From the direction field conclude that all solutions are bounded, and hence solutions are globally defined (no blow-up in finite time).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nEquilibria are: \\(N_1 = 0\\), \\(N_2 = \\mu/\\beta\\), and \\(N_3 = 1/\\gamma\\). With no loss of generality we can assume \\(N_1 &lt; N_2 &lt; N_3\\). From the sign of the right hand side, we see that \\(N_1\\) and \\(N_3\\) are asymptotically stable, and \\(N_2\\) is unstable. In particular, the basin of attraction of \\(N_1\\) is \\((-\\infty,N_2)\\), and the one for \\(N_2\\) is \\((N_2,\\infty)\\). Thus, if \\(N(0)\\in I\\), then \\(N(t)\\in I\\) for all \\(t\\). Since solutions are bounded, they are globally defined.",
    "crumbs": [
      "Assignments",
      "Population dynamics"
    ]
  },
  {
    "objectID": "Assignments/03_spruce.html",
    "href": "Assignments/03_spruce.html",
    "title": "Predation",
    "section": "",
    "text": "Fishery model\n(From Strogatz, 2015, exercise 3.7.4)\nA fishery model with harvesting has the form \\[\nN' = r\\Bigl(1-\\frac{N}{K}\\Bigr)N - E\\frac{N}{A+N},\n\\]\nwhere all parameters are positive. The parameter \\(E\\) is the harvest effort. Note that harvest rate increases with \\(N\\) because it is harder to catch fish when the population size is small.\nfor a suitable choice of quantities.",
    "crumbs": [
      "Assignments",
      "Predation"
    ]
  },
  {
    "objectID": "Assignments/03_spruce.html#fishery-model",
    "href": "Assignments/03_spruce.html#fishery-model",
    "title": "Predation",
    "section": "",
    "text": "Give a biological interpretation of \\(A\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst note that the functional response is Holling type III, with \\(\\theta=1\\). So, when \\(N = A\\), the fishing term is exactly \\(E/2\\). Thus, \\(A\\) is the half-saturation value.\n\n\n\n\nShow that the system can be rewritten in dimensionless form as \\[\n\\dot{u} = u(1-u) - h\\frac{u}{a+u}\n\\]\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe select \\[\nu = \\frac{N}{K}, \\quad \\tau = rt,\n\\]\nso we obtain: \\[\n\\dot{u} = u(1-u) - \\frac{E}{rK}\\frac{u}{A/K + u} = u(1-u) - h \\frac{u}{a + u},\n\\]\nwhere we selected \\[\nh = \\frac{E}{rK}, \\quad a = \\frac{A}{K}.\n\\]\n\n\n\n\nShow that the system can have one, two, or three equilibria, depending on the values of \\(a\\) and \\(h\\). Classify the stability in each case.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nEquilibria are solutions of: \\[\nf(u) = u(1-u) - h \\frac{u}{a + u} = 0.\n\\]\nOne fixed point is \\(u = E_0 = 0\\). All other fixed points satisfy: \\[\n(1-u) - h\\frac{1}{a + u} = 0, \\quad\\Leftrightarrow\\quad p(u):=(1-u)(a+u) = h.\n\\]\nThe function on the left hand side is a parabola \\(p(u)\\): if its maximum is below \\(h\\), then we have no equilibria. Otherwise, we can have one or two depending on whether the maximum is on the boundary or not.\nThe position of the maximum is: \\[\nu_\\text{max} = \\frac{1-a}{2}, \\quad\np_\\text{max} = p(u_\\text{max}) = \\frac{(1+a)^2}{4}.\n\\]\nIf \\(h &gt; p_\\text{max}\\) there are no fixed points.\nIf \\(h \\le p_\\text{max}\\), we have two fixed points \\(E_1\\) and \\(E_2\\), specifically: \\[\nE_{1,2} = \\frac{1-a}{2} \\pm \\frac{1}{2}\\sqrt{(1+a)^2 - 4 h}.\n\\]\nNote that \\(E_2\\) is always non-negative, when it exists. However, \\(E_1\\) is not always biologically feasible, because it could be negative. This is the case when \\(u_\\text{max} &lt; 0\\), that is \\(a&gt;1\\). When \\(a&lt;1\\), then it only exists when \\(p_\\text{max}\\le h \\le p(0) = a\\).\nFor the stability, note that: \\[\nf(u) = \\frac{u}{a+u} (p(u)-h), \\quad\\Rightarrow\\quad f'(u) = \\underbrace{\\frac{\\mathrm{d}}{\\mathrm{d}u}\\Bigl(\\frac{u}{a+u}\\Bigr)}_{&gt;0} (p(u)-h) + \\underbrace{\\frac{u}{a+u}}_{\\ge 0} p'(u).\n\\]\nFor \\(E_0\\), \\(f'(E_0)\\) has the sign of \\(p(0)-h\\), so \\(E_0\\) is asympotically stable when \\(a &lt; h\\), otherwise is unstable.\nFor \\(E_{1,2}\\), \\(f'(E_{1,2})\\) has the sign of \\(p'(E_{1,2})\\): thus, \\(E_1\\) is unstable (\\(p'(E_1)&gt;0\\)) and \\(E_2\\) is asymptotically stable (when they exist).\n\n\n\n\nPlot the bifurcation diagram with respect to \\(h\\), for a fixed value of \\(a\\). Determine all bifurcation points and whether they are catastrophic or not. Is there a bistable region?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf \\(a&gt;1\\), the parabola has a maximum in \\(u=0\\), thus there can only exist \\(E_2\\), depending on \\(h\\). For \\(h=0\\) we have 2 equilibria, \\(E_0=0\\) and \\(E_2=1\\). As \\(h\\) increases, \\(E_2\\) moves along the parabola until \\(h = p(0) = a\\), where we have a bifurcation point (transcritical bifurcation). After that, only \\(E_0\\) exists. The bifurcation is not catastrophic, and there is no bistable region.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nsns.set_theme(\"notebook\", style=\"whitegrid\")\n\na = 2\nu = np.linspace(0,1,1000)\nh = lambda u: (1-u)*(a+u)\n\nwith plt.xkcd(scale=0.5):\n    fig, ax = plt.subplots()\n    sns.despine()\n    ax.xaxis.set_ticks([])\n    ax.yaxis.set_ticks([])\n\n    ax.plot([0,a],[0,0],'r')\n    ax.plot([a,1.2*a],[0,0],'b')\n    ax.plot(h(u),u,'b')\n    ax.plot([a],[0],'k.',markersize=16)\n    ax.set_xlabel('$h$')\n    ax.set_ylabel('$u^*$')\n    ax.set_xlim([0,None])\n    \n    plt.show()\n\n\n\n\n\nBifurcation diagram with respect to \\(h\\) and for \\(a=2\\).\n\n\n\n\nIf \\(a &lt; 1\\), then we can also have \\(E_1\\). For \\(h=0\\), we start as above. As \\(h\\) increase, there will be a bifurcation point for \\(h = p(0) = a\\), when \\(E_1\\) appears. Here we have 3 equilibria, until \\(h = p_\\text{max} = \\frac{(1+a)^2}{4}\\), where \\(E_1\\) and \\(E_2\\) collides in a tangent bifurcation. For \\(h &gt; p_\\text{max}\\) there is only \\(E_0\\). The tangent bifurcation is catastrophic, and there is bistable region when \\(E_1\\) exists.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nsns.set_theme(\"notebook\", style=\"whitegrid\")\n\na = 0.2\npmax = (1+a)**2/4\numax = (1-a)/2\nu1 = np.linspace(0,1,1000)\nh = lambda u: (1-u)*(a+u)\n\nwith plt.xkcd(scale=0.5):\n    fig, ax = plt.subplots()\n    sns.despine()\n    ax.xaxis.set_ticks([])\n    ax.yaxis.set_ticks([])\n\n    ax.plot([0,a],[0,0],'r')\n    ax.plot([a,1.2*pmax],[0,0],'b')\n    ax.plot(h(u[u &gt; umax]),u[u &gt; umax],'b')\n    ax.plot(h(u[u &lt; umax]),u[u &lt; umax],'r')\n    ax.plot([a],[0],'k.',markersize=16)\n    ax.plot([pmax],[umax],'k.',markersize=16)\n    ax.fill_between(h(u), 1, where=(u &lt; umax), facecolor='gray', alpha=.2)\n    ax.set_xlabel('$h$')\n    ax.set_ylabel('$u^*$')\n    ax.set_xlim([0,None])\n    \n    plt.show()\n\n\n\n\n\nBifurcation diagram with respect to \\(h\\) and for \\(a=0.2\\). The shared area is the bistable region.\n\n\n\n\n\n\n\n\nSuppose that we slowly increase the fishing effort from zero, assuming that the fish population is at the carrying capacity at the beginning. Show the dynamics.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSuppose \\(a&gt;1\\). Then, as we increase \\(h\\) the equilibrium population will decrease with carrying capacity \\(E_2\\), until the bifurcation point, where it goes to extinction. It is reversible: if we decrease \\(h\\) beyond the bifurcation point and the population is not exactly zero, then it will recover up to \\(E_2\\).\nIf \\(a&lt;1\\), the initial path is similar until the tangent bifurcation. Then, there is a fast collapse of the population down to zero. If we decrease \\(h\\), there is no hope for recovery of the population unless \\(h &lt; a\\).\n\n\n\n\nPlot the stability diagram of the system in \\((a,h)\\) parameter space. Can hysteresis occur in any of the stability regions?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can have an hysteresis for \\(a&lt;1\\). In the \\((a,h)\\) space, the bistable region (3 equilibria) is for \\(a = p(0) &lt; h &lt; p_\\text{max} = \\frac{(1+a)^2}{4}\\).\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nsns.set_theme(\"notebook\", style=\"whitegrid\")\n\namax = 1.2\na = np.linspace(0,amax,100)\n\nfig, ax = plt.subplots()\nsns.despine()\nax.xaxis.set_ticks([0.0,1.0])\nax.yaxis.set_ticks([0.0,1.0])\n\nax.fill_between(a, (1+a)**2/4, a, where=(a &lt; 1), facecolor='red', alpha=.2)\nax.fill_between(a, a, facecolor='blue', alpha=.1)\nax.fill_between(a, amax, facecolor='blue', alpha=.1)\nax.plot(a, a, 'k--', linewidth=1.0)\nax.plot(a[a&lt;=1], (1+a[a&lt;=1])**2/4, 'k--', linewidth=1.0)\nax.set_xlabel('$a$')\nax.set_ylabel('$h$')\nax.set_xlim([0,None])\nax.set_ylim([0,None])\nax.plot([1.0],[1.0],'k.',markersize=16)\n\nax.set_aspect('equal')\nax.annotate('Bistable',(0.15,0.15),ha='center',rotation=45)\nax.annotate('Cusp',(0.98,1.02),ha='right',va='bottom')\n\nax.annotate('Two equilibria',(0.7,0.25),ha='center')\nax.annotate('One equilibrium $E_0$',(0.35,0.8),ha='center')\n\nplt.show()\n\n\n\n\n\nBifurcation diagram with respect to \\((a,h)\\).",
    "crumbs": [
      "Assignments",
      "Predation"
    ]
  },
  {
    "objectID": "Assignments/03_spruce.html#spruce-budworm-model",
    "href": "Assignments/03_spruce.html#spruce-budworm-model",
    "title": "Predation",
    "section": "Spruce-budworm model",
    "text": "Spruce-budworm model\nThe spruce-budworm model describes the population of budworms over time, for a given initial population. There are only 2 relevant parameters: \\(\\rho\\), the intrinsic growth rate of budworms; and \\(q\\), the carrying capacity. An outbreak is when the stable population is very large, whereas a refuge is when it is very low.\nDepending on \\(\\rho\\), we may have various stable population levels, according to the following diagrams:\n\nIn diagram (A) we see the effect of changing \\(\\rho\\) with a fixed value of \\(q = \\bar{q}\\). In diagram (B) we visualize the region in the parameters space where 3 equilibria are present (bistable region).\nWe would like to analyze various strategies to efficiently go from an outbreak to a refuge.\n\nStrategy (1): we reduce the population by reducing \\(\\rho\\) (for instance, by preventing mating). Does it work? Use the diagrams to show how the population varies as we reduce \\(\\rho\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can use diagram (A). By reducing \\(\\rho\\), we slowly reduce \\(u^*\\) (the stable equilibrium level for the population) as well, up to the tipping point (first tangent bifurcation). Then, the population suddenly drops from outbreak level to refuge level, and it will eventually continue to decrease smoothly with \\(\\rho\\). So, the strategy works.\n\n\n\n\nStrategy (2): we reduce the population by using an insecticide. Does it work? Use diagram (A). Observe that if we kill worms, we move the population away from the equilibrium, hence it will quickly rebound to the closest stable equilibrium.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe insecticide does not always work. If \\(\\rho\\) is large, so we are on the right of the bistable region, the outbreak equilibrium is the only one and it is stable. So, after the application of the insecticide, the population will fully recover to the outbreak level. On the other hand, if we are in the bistable region, we need to apply a sufficient amount of insecticide to bring the population below the threshold value. In this case, the population will naturally decrease to the refuge level.\n\n\n\n\nStrategy (3): we reduce \\(q\\), for instance by spraying a defoliant. Does it work? Use diagram (B).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is a bit more difficult to visualize. If we fix \\(\\rho\\) and reduce \\(q\\), the curve in diagram (A) will be squeezed vertically, and the bistable region will also become smaller, up to a critical point where it vanishes. We can visualize this in diagram (B), moving horizontally from large \\(q\\) to small \\(q\\). Thus, we need to reduce \\(q\\) as much as we need to cross the bistable region. So, it works.\n\n\n\n\nStrategy (1+2+3): we reduce \\(q\\) and \\(\\rho\\) simultaneously, and then use the insecticide. Does it work?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is a great strategy. With little effort (and money), we can reduce both so to move into the bistable region, diagram (B), then we apply the insecticide a move the population below the threshold. The population will then naturally go towards the refuge level.\n\n\n\n\nAfter the application of Strategy (1), \\(\\rho\\) starts to increase again back to its original value (large). Explain how the population varies. Does it increase in the exactly same matter as it decreased?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe proceed along the lower branch in diagram (A), up to the tipping point on the right, and then jump to the upper branch. The path is different from the previous point.",
    "crumbs": [
      "Assignments",
      "Predation"
    ]
  },
  {
    "objectID": "Assignments/03_spruce.html#logistic-with-resources",
    "href": "Assignments/03_spruce.html#logistic-with-resources",
    "title": "Predation",
    "section": "Logistic with resources",
    "text": "Logistic with resources\nAssume that the growth rate of a population depends on available resources \\(\\rho(t)\\), according to a general law \\(r(t) = G(\\rho(t))\\).\nAssume for the moment that the total amount of resources is a fixed constant \\(C\\), but they can be free (hence available, denoted by \\(\\rho(t)\\)) or used by the population, denoted by \\(H(N(t))\\), where dependence on the population is shown. In other words \\[\n\\rho(t) = C ‚àí H(N(t)),\n\\tag{1}\\]\nand the resulting model \\[\nN' = G(C ‚àí H (N)) N,\n\\tag{2}\\]\nwill be specified when the functions \\(G(\\cdot)\\) and \\(H(\\cdot)\\) are given.\n\nExplain why reasonable assumptions are that both \\(G\\) and \\(H\\) are increasing functions with \\(G(0) &lt; 0\\), \\(H(0) = 0\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe are making the following assumptions:\n\n\\(G'(\\rho) &gt; 0\\): more resources, higher growth rate;\n\\(H'(N) &gt; 0\\): more individuals, higher use of resources;\n\\(H(0)=0\\): no individuals, no usa of resources;\n\\(G(0)&lt;0\\): no resources, negative growth rate.\n\n\n\n\n\nChoose a linear form for \\(G\\) and \\(H\\) and show that the growth rate has the form \\(r(N) = r ‚àí \\alpha N\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAssuming, consistently with the above hypotheses, that \\[\n\\begin{aligned} G(\\rho) &= a\\rho - b, && a,b&gt;0, \\\\ H(N) &= d N, && d &gt;0,\\end{aligned}\n\\]\nwe have that \\[\n\\begin{split} r(N) &= G(C-H(N)) = G(C - d N) = a(C-dN)-b \\\\ &= -adN + aC - b = \\bar r - \\alpha N,\\end{split}\n\\]\nwith \\(\\bar{r}=aC - b\\) and \\(\\alpha = ad &gt; 0\\).\n\n\n\n\nWith the previous assumptions, does Equation¬†2 always have a positive equilibrium? If not, find the conditions under which it does and find the expressions for the intrinsic rate of growth \\(r\\) and the carrying capacity \\(K\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe have the equilibrium \\(N=N_0=0\\), as usual. The other equilibrium is for \\(r(N_1)=0\\), that is \\(N^* = \\bar r / \\alpha\\). This equilibrium exists only for \\(\\bar r &gt; 0\\), so \\(aC - b &gt; 0\\).\nIn the logistic equation, \\(r(N)=\\bar{r}(1-N/K)\\), so by comparing the expressions we have that the intrinsic growth rate is \\(\\bar{r}\\) whereas the carrying capacity is \\[K=\\frac{\\bar{r}}{\\alpha} = \\frac{aC-b}{ad} = \\frac{C}{d} - \\frac{b}{ad}.\\]\nThe stability analysis (here not required) follows from the standard logistic equation.\n\n\n\n\nUsing generic functions \\(G\\) and \\(H\\) satisfying the assumptions in 1., find under which conditions the equation \\((2)\\) has a unique positive equilibrium. When is it asymptotically stable?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe equilibrium \\(N_0=0\\) is always present. For the positive equilibrium \\(N^*\\), we need to satisfy \\(G(C-H(N^*))=0\\). Even though \\(G(0)&lt;0\\) and \\(G'(\\rho)&gt;0\\), there is no guarantee that \\(G\\) will cross the zero at some finite value \\(\\rho^*\\). However, if \\(\\lim_{\\rho\\to\\infty} G &gt; 0\\) (finite or not), then there exists a unique zero, denoted by \\(\\rho^*\\). This is reasonable, for high level of resources the growth rate should be positive.\nNext, at equilibrium we have \\(\\rho^*=C-H(N^*)\\), thus \\(H(N^*)=C-\\rho^*\\). Since \\(H(0)=0\\) and \\(H'&gt;0\\), if we further assume that \\(\\lim_{N\\to\\infty} H(N) = H_\\infty &gt; C-\\rho^*&gt; 0\\) we must have a unique zero \\(N^*\\), corresponding to the equilibrium. Therefore, if \\(\\rho^*&lt; C &lt; H_\\infty + \\rho^*\\), then there exists a unique equilibrium \\(N^*\\). Note that the second condition is superflous when \\(H_\\infty = +\\infty\\).\nFor the stability, we have \\(f'(N^*)=r(N^*)+r'(N^*)N^*=r'(N^*)N^*\\), so the sign is determined by the sign of \\(r'(N)\\). It follows: \\[\nr'(N) = G'(\\rho(N))\\rho'(N) = -G'(\\rho(N))H'(N).\n\\]\nSince \\(G'(\\rho)&lt;0\\), the stability follows from the fact that \\(H'(N)&gt;0\\).\n\n\n\n\nWhat could be other reasonable assumptions for \\(\\rho\\) instead of Equation¬†1?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe function \\(\\rho(N)\\) denotes the available resources at some level \\(N\\) of population. We expect this function to decrease with \\(N\\), and to be maximum for \\(N=0\\). Resources cannot be negative, thus it is also bounded from below by zero. In summary\n\n\\(\\rho(0)=\\rho_\\mathrm{max} &gt; 0\\),\n\\(\\rho'(N) &lt; 0\\),\n\\(\\rho(N) \\ge 0\\) for all \\(N\\ge 0\\).\n\nWe see that if \\(\\rho^*&lt; \\rho_\\mathrm{max}\\), then there exists a unique equilibrium \\(N^* &gt; 0\\). The equilibrium is asymptotically stable because of the second assumption.\n\n\n\n\nModify the equation to \\[\n\\begin{cases}\nN' = G(R)N, & \\\\\nR' = \\rho_0 ‚àíH(R,N), &\n\\end{cases}\n\\]\n\nwhere \\[\nG(R)=m\\frac{R-a}{R+1}-\\mu_0, \\quad H(R,N)=(c+bN)\\frac{R-a}{R+1}N-R.\n\\]\nHere \\(N(t)\\) represents population density, and \\(R(t)\\) available resources, and all constants are supposed to be positive.\nWith the help of Phase Plane in MATLAB, explore the solutions of the problem using parameter values such that: \\[\\rho_0 &gt; \\frac{\\mu_0 + m a}{m - \\mu_0}.\\]\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor \\(R\\) fixed, we see that the model is similar to the above in the above points. So we see that \\(G(0)=-ma-\\mu_0&lt;0\\) and \\(G'(R)=\\frac{m(1+a)}{(R+1)^2} &gt; 0\\), as assumed above. In order to have a zero, \\(G_\\infty = \\lim_{R\\to\\infty}G(R)=m-\\mu_0&gt;0\\), so we need \\(m&gt;\\mu_0\\) in order to have a non-trivial equilibrium. Note that \\(G(R)\\) is a branch of hyperbola. The condition on \\(\\rho_0\\) ensures the existence of a non-trivial equilibrium.\n\nWe see that \\(R(t)\\) and \\(N(t)\\) oscillates but converges to a non-trivial equilibrium. Therefore, the equilibrium is a focus.",
    "crumbs": [
      "Assignments",
      "Predation"
    ]
  },
  {
    "objectID": "Assignments/04_systems.html",
    "href": "Assignments/04_systems.html",
    "title": "System of ODEs",
    "section": "",
    "text": "Linear dynamics\nConsider the linear system \\(\\mathbf{v}' = \\mathbf{A}\\mathbf{v}\\) where \\[\n\\mathbf{A} = \\begin{pmatrix} -3 & 2 \\\\ -2 & 2 \\end{pmatrix}.\n\\]",
    "crumbs": [
      "Assignments",
      "System of ODEs"
    ]
  },
  {
    "objectID": "Assignments/04_systems.html#linear-dynamics",
    "href": "Assignments/04_systems.html#linear-dynamics",
    "title": "System of ODEs",
    "section": "",
    "text": "Find the eigenvalues and eigenvectors of \\(\\mathbf{A}\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe need to find \\(\\lambda\\) and \\(\\mathbf{v}\\) such that \\(\\mathbf{A}\\mathbf{v} = \\lambda\\mathbf{v}\\). First, the eigenvalues roots of \\[\n\\det(\\mathbf{A}-\\lambda\\mathbf{I}) = (-3-\\lambda)(2-\\lambda)+4 = \\lambda^2 + \\lambda - 2.\n\\]\nAlso note that \\(\\det(\\mathbf{A}-\\lambda\\mathbf{I}) = \\lambda^2 - \\operatorname{tr}\\mathbf{A}\\lambda + \\det\\mathbf{A}\\). The eigenvalues are: \\[\n\\lambda_1 = -2, \\quad \\lambda_2 = 1.\n\\]\nFor the eigenvectors we have: \\[\n\\begin{aligned}\n\\mathbf{A}\\mathbf{v}_1 &= \\lambda_1\\mathbf{v}_1 = -2 \\mathbf{v}_1, \\\\\n\\mathbf{A}\\mathbf{v}_2 &= \\lambda_2\\mathbf{v}_2 = 1 \\mathbf{v}_2.\n\\end{aligned}\n\\]\nWe see that a possible choice of eigenvectors is \\[\n\\mathbf{v}_1 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix},\n\\quad\n\\mathbf{v}_2 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}.\n\\]\n\n\n\n\nCompute the matrix exponential of \\(t\\mathbf{A}\\) and the transition matrix.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe matrix \\(\\mathbf{A}\\) is diagonalizable. Thus, the matrix exponential is: \\[\ne^{t\\mathbf{A}} = \\mathbf{V}e^{t\\Lambda}\\mathbf{V}^{-1}\n\\]\nwhere \\(\\Lambda = \\operatorname{diag}\\{\\lambda_1, \\lambda_2 \\}\\). The matrix \\(\\mathbf{V}\\) and its inverse are: \\[\n\\mathbf{V} = \\begin{bmatrix} \\mathbf{v}_1 \\;|\\; \\mathbf{v}_2 \\end{bmatrix}\n= \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix},\n\\quad\n\\mathbf{V}^{-1} = \\frac{1}{3} \\begin{bmatrix} 2 & -1 \\\\ -1 & 2 \\end{bmatrix}.\n\\]\nSo we get: \\[\n\\begin{split}\ne^{t\\mathbf{A}} &= \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}\n\\begin{bmatrix} e^{-2t} & 0 \\\\ 0 & e^t \\end{bmatrix}\n\\frac{1}{3} \\begin{bmatrix} 2 & -1 \\\\ -1 & 2 \\end{bmatrix} \\\\\n&= \\frac{1}{3} \\begin{bmatrix} 4 e^{-2t} - e^t & 2( e^t - e^{-2t}) \\\\ 2( e^{-2t} - e^t) & 4 e^t - e^{-2t} \\end{bmatrix}.\n\\end{split}\n\\]\nThe transition matrix is just \\(\\mathbf{W}(t,s) = e^{(t-s)\\mathbf{A}}\\).\n\n\n\n\nHow can we approximate the behaviour of solutions for different initial conditions?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSince we have the transition matrix, the solution for \\(\\mathbf{y}(0)=\\mathbf{y}_0\\) is: \\[\n\\mathbf{y}(t) = \\mathbf{W}(t,0)\\mathbf{y}_0 = e^{t\\mathbf{A}}\\mathbf{y}_0.\n\\]\n\n\n\n\nFind the equilibrium of the ODE and compute the stable and unstable manifold. Draw them on the phase portrait. Note, by definition the stable and unstable manifolds are \\[\n\\begin{aligned}\n\\mathcal{W}^s(\\mathbf{y}^*) &= \\bigl\\{ \\mathbf{y}_0\\in\\mathbb{R}^2\\colon \\lim_{t\\to+\\infty}\\boldsymbol{\\phi}(t,\\mathbf{y}_0) = \\mathbf{y}^* \\bigr\\}, \\\\\n\\mathcal{W}^u(\\mathbf{y}^*) &= \\bigl\\{ \\mathbf{y}_0\\in\\mathbb{R}^2\\colon \\lim_{t\\to-\\infty}\\boldsymbol{\\phi}(t,\\mathbf{y}_0) = \\mathbf{y}^* \\bigr\\}.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor this ODE we know the flow \\(\\boldsymbol{\\phi}(t,\\mathbf{y}_0)\\) explicitly: \\[\n\\boldsymbol{\\phi}(t,\\mathbf{y}_0) = e^{t\\mathbf{A}}\\mathbf{y}_0 = \\mathbf{V}e^{t\\Lambda}\\mathbf{V}^{-1}\\mathbf{y}_0.\n\\]\nThus the limit is: \\[\n\\lim_{t\\to\\pm\\infty} \\boldsymbol{\\phi}(t,\\mathbf{y}_0) = \\mathbf{V} \\Bigl( \\lim_{t\\to\\pm\\infty} e^{t\\Lambda}\\Bigr) \\mathbf{V}^{-1}\\mathbf{y}_0.\n\\]\nThe limit between parenthesis is a diagonal matrix formally equal to: \\[\n\\begin{aligned}\n\\lim_{t\\to+\\infty} e^{t\\Lambda} &= \\operatorname{diag}\\{ 0, +\\infty \\}, \\\\\n\\lim_{t\\to-\\infty} e^{t\\Lambda} &= \\operatorname{diag}\\{ +\\infty, 0 \\}.\n\\end{aligned}\n\\]\nFor the stable manifold \\(\\mathcal{W}^s\\), the limit is \\(\\mathbf{y}^* = \\mathbf{0}\\) only if \\[\n\\mathbf{V}^{-1}\\mathbf{y}_0 = \\begin{bmatrix} \\alpha \\\\ 0 \\end{bmatrix}\n\\quad\\Rightarrow\\quad\n\\mathbf{y}_0 = \\alpha \\mathbf{v}_1,\n\\]\nfor some \\(\\alpha\\in\\mathbb{R}\\), otherwise the second component of \\(\\mathbf{V}^{-1}\\mathbf{y}_0\\) diverges. Similarly for the unstable manifold, we get that \\(\\mathbf{y}_0 = \\alpha \\mathbf{v}_2\\). In conclusion, the manifolds are the linear spaces: \\[\n\\begin{aligned}\n\\mathcal{W}^s(\\mathbf{y}^*) &= \\bigl\\{ \\alpha \\mathbf{v}_1 \\colon \\alpha\\in\\mathbb{R} \\bigr\\}, \\\\\n\\mathcal{W}^u(\\mathbf{y}^*) &= \\bigl\\{ \\alpha \\mathbf{v}_2 \\colon \\alpha\\in\\mathbb{R} \\bigr\\}.\n\\end{aligned}\n\\]\nIn other words, the stable manifold is the line parallel to the first eigenvector and passing through the origin, whereas the unstable manifold is the one parallel to the second eigenvector.\n\n\n\n\nQualitatively describe or draw the orbits.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSince we know the stable and unstable manifolds, we know already 2 orbits. Specifically, one orbit is along \\(\\mathcal{W}^s\\) with dynamic \\(e^{\\lambda_1 t} = e^{-2t}\\), thus any non-zero point on \\(\\mathcal{W}^s\\) converges to zero with the above rate. Similarly, on the unstable manifold trajectories diverges with rate \\(e^{\\lambda_2 t} = e^t\\).\nFor other initial conditions, the dynamic is a linear combination of the dynamic on the stable and unstable manifold. See the picture.\n\n\n\nimage\n\n\nQ3.6. Is the equilibrium \\(\\mathbf{y}^* = \\mathbf{0}\\) stable?\nThe equilibrium is unstable, specifically a saddle point.",
    "crumbs": [
      "Assignments",
      "System of ODEs"
    ]
  },
  {
    "objectID": "Assignments/04_systems.html#phase-portraits",
    "href": "Assignments/04_systems.html#phase-portraits",
    "title": "System of ODEs",
    "section": "Phase portraits",
    "text": "Phase portraits\nTake a look at the following set of phase portraits:\n\n\n\nPhase portraits\n\n\n\nFind the phase portrait that cannot be from a linear ODE.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCase 4 cannot be from a linear system, because for a saddle the stable and unstable manifold should be linear.\n\n\n\n\nFor all the others, qualitatively describe the set of eigenvectors, eigenvalues, equilibria, and stability.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet us go case by case:\n\nHere we have \\(\\lambda_1 &lt; 0\\) (real), and \\(\\lambda_2 = 0\\). The first eigenvector \\(\\mathbf{v}_1\\) is almost vertical, whereas \\(\\mathbf{v}_2\\) is parallel to the line with equilibria. The kernel of \\(\\mathbf{A}\\) is generated by \\(\\mathbf{v}_2\\). For instance, we could take \\(\\mathbf{v}_1 = [0,1]^T\\) and \\(\\mathbf{v}_2 = [1,1]^T\\).\nThis is an asymptotically stable spiral. Thus, we have 2 complex conjugate eigenvalues, \\(\\lambda\\) and \\(\\bar{\\lambda}\\), with negative real part.\nA center, as above but the real part is zero: \\(\\lambda_{1,2} = \\pm i \\omega\\) for some \\(\\omega\\). An example is \\(\\mathbf{A} = \\left(\\begin{smallmatrix} 0 & 01 \\\\ \\omega^2 & 0 \\end{smallmatrix}\\right)\\).\nNon linear.\nTricky one. We only have one eigenvector \\(\\mathbf{v}_1\\) associated with an asymptotically stable eigenvalue (real). The eigenvalue is double, that is the algebraic multiplicity is 2 (double root of the characteristic polynomial), but the geometrical multiplicity is 1. Recall that the geometrical multiplicity is the dimension of the eigenspace \\(\\ker(\\lambda\\mathbf{I} - \\mathbf{A})\\). Along \\(\\mathbf{v}_1\\) we approach the equilibrium as usual. Components orthogonal to \\(\\mathbf{v}_1\\) will also converge to the equilibrium as \\(t e^{\\lambda t}\\). Take \\(\\mathbf{A}=\\left(\\begin{smallmatrix} a & 1 \\\\ 0 & a \\end{smallmatrix}\\right)\\) with \\(a&lt;0\\) as an example.\nAsymptotically stable equilibrium with two coincident eigenvalues, e.g., \\(\\mathbf{A} = \\lambda \\mathbf{I}\\) for some \\(\\lambda &lt; 0\\) real.",
    "crumbs": [
      "Assignments",
      "System of ODEs"
    ]
  },
  {
    "objectID": "Assignments/04_systems.html#double-well-potential",
    "href": "Assignments/04_systems.html#double-well-potential",
    "title": "System of ODEs",
    "section": "Double-well potential",
    "text": "Double-well potential\nConsider the following ODE \\[\ny'' = F(y) = y - y^3.\n\\]\n\nRecast the problem as a system of ODEs.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe set \\(y_1 = y\\) and \\(y_2 = y' = y_1'\\), so: \\[\n\\left\\{\\begin{aligned}\ny_1' &= y_2, \\\\\ny_2' &= y_1 - y_1^3 = F(y_1).\n\\end{aligned}\\right.\n\\]\n\n\n\n\nFind the Hamiltonian of the system. Hint: use the potential energy.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSince \\(U(y_1) = -\\frac{1}{2} y_1^2 + \\frac{1}{4} y_1^4\\) is such that \\(U' = -F\\), we have the Hamiltonian: \\[\nH(y_1,y_2) = \\frac{1}{2} y_2^2 + U(y_1).\n\\]\n\n\n\n\nGraphically represent the orbits in the phase plane for various choices of the initial energy. Are all orbits closed?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYes, orbits are all closed, because \\(H\\) is bounded from below and \\(H\\to+\\infty\\) as \\(\\|\\mathbf{y}\\|\\to\\infty\\). So, \\(H(y_1,y_2)=k\\) is always a closed curve if \\(k &gt; \\min_{\\mathbf{y}} H\\). Graphically:\n\n\n\nimage\n\n\n\n\n\n\nShow that the system has 3 equilibria, two stable (not asymptotically) and one unstable.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can compute them by looking at stationary points of the Hamiltonian. We have 2 equilibria at \\((\\pm1,0)\\), which are locally stable because the Hamiltonian is convex, and one saddle point at \\((0,0)\\). Note that linearization fails for this example, we can only deduce the stability from the phase portrait.\n\n\n\nimage\n\n\n\n\n\n\nThe system has 2 homoclinic orbits, one on the left and one on the right of the origin. Homoclinic orbits look like periodic orbits, but with an infinite period. Try to describe what happens if you start close to the origin, and then you take the initial condition that tends to the origin. What does it happen to the solution?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf we start close to the origin, we are on a periodic solution with some period \\(T&gt;0\\). The orbit has a shape close to the homoclinic orbit. If we start closer and closer, the orbit will tend to a drop-like shape (like the homoclinic) and the period will tend to \\(\\infty\\).",
    "crumbs": [
      "Assignments",
      "System of ODEs"
    ]
  },
  {
    "objectID": "Assignments/04_systems.html#gone-with-the-wind",
    "href": "Assignments/04_systems.html#gone-with-the-wind",
    "title": "System of ODEs",
    "section": "Gone with the Wind",
    "text": "Gone with the Wind\nBased on exercise 7.2.19 from Strogatz (2015).\nRinaldi et al. (2013) used mathematical modeling to describe the stormy love affair between Scarlett O‚ÄôHara and Rhett Butler. The system reads: \\[\n\\left\\{\\begin{aligned}\nR' &= -R + A_S + kS e^{-S}, \\\\\nS' &= -S + A_R + kR e^{-R},\n\\end{aligned}\\right.\n\\]\nwhere \\(R(t)\\) is Rhett‚Äôs love for Scarlett, and \\(S(t)\\) is Scarlett‚Äôs love for Rhett. All parameters are positive.\nNote: ‚ÄúFrankly, my dear, I don‚Äôt give a damn‚Äù is not a valid answer.\n\nInterpret the three terms on the right hand side of each equation. What do they mean, romantically speaking? In particular, what does the functional form of the exponentials terms signify about how Rhett and Scarlett react to each other‚Äôs endearments?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet us exclude for a moment the exponental terms. In this case, the equations are independent, with \\(R\\to A_S\\) and \\(S\\to A_R\\) at \\(t\\to\\infty\\). Thus, we can interpret \\(A_S\\) as the asymptotic love of Rhett for Scarlett, in the absence of a feedback from Scarlett. The same is valid for Scarlett‚Äôs love for Rhett.\nThe exponential terms models the interaction between the two lovers. The function \\(S e^{-S}\\) is as follows:\n\n\n\nimage\n\n\nSo, when \\(S=0\\) (Scarlett has no interest for Rhett), this term is also zero, providing no contribution to Rhett‚Äôs love. But as soon as \\(S\\) increases, Rhett‚Äôs love increases even more. The peak is for \\(S=1\\), after that too much love from Scarlett (\\(S\\gg 1\\)) has little effect in Rhett‚Äôs love. The situation is simmetric for Scarlett reaction to Rhett‚Äôs love.\n\n\n\n\nShow that all trajectories that begin in the first quadrant \\(R, S &gt; 0\\) stay in the first quadrant forever, and interpret that result psychologically.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe system is not in Kolmogorov form, but we can check whether the first quadrant is a trapping region. For \\(S=0\\), we have \\(S' = A_R + kR e^{-R} &gt; 0\\), so orbits can only enter. Similarly, for \\(R=0\\), we have \\(R' = A_S + kS e^{-S} &gt; 0\\). Thus, the axes of the first quadrant are inflow, thus if we start in the first quadrant, we cannot leave.\nPsychologically, we could say that once they meet, they cannot avoid love between them. But I‚Äôm open to other interpretations.\n\n\n\n\nUsing Dulac‚Äôs criterion, prove that the model has no periodic solutions. Hint: the simplest \\(h(R,S)\\) you can think of will work.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet us consider \\(h\\equiv 1\\). We have: \\[\n\\nabla\\cdot\\mathbf{f} = -2 &lt; 0.\n\\]\nThe sign is constant, thus we cannot have limit cycles.\n\n\n\n\nUsing Phase Plane in Matlab, plot the phase portrait for the system, assuming parameter values \\(A_S = 1.2\\), \\(A_R = 1\\), and \\(k=15\\). Assuming that Rhett and Scarlett are indifferent when they meet, so that \\(R(0)=S(0)=0\\), plot the predicted trajectory for what happens in the first stage of their relationship. Hint: don‚Äôt use the default range for the phase plane, increase it. If you spot a heart ‚ù§Ô∏è, it‚Äôs not on purpose.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nimage\n\n\nWe have 3 equilibria, one saddle at the center and 2 stable and attractive equilibria. The unstable manifold of the saddle connects to the stable manifold of the equilibria, effectively providing a barrier in the lower left triangle of the phase plane. The stable manifold of the saddle roughly bisect the triangle.\nStarting from \\((0,0)\\), we intially approach the saddle, thus the \\(S\\) and \\(R\\) are comparable (Scarlett loves Rhett as much as Rhett loves Scarlett), but then the orbit turns to the right, and reaches an equilibrium where Rhett‚Äôs love for Scarlett is much higher than Scarlett‚Äôs love for Rhett‚Äôs.",
    "crumbs": [
      "Assignments",
      "System of ODEs"
    ]
  },
  {
    "objectID": "Assignments/05_predators.html",
    "href": "Assignments/05_predators.html",
    "title": "Prey-predator systems",
    "section": "",
    "text": "Existence of cycles\nConsider the following competition model: \\[\n\\left\\{\\begin{aligned}\nN_1' &= r_1 N_1 (1 - N_1/K_1) - b_1 N_1 N_2, \\\\\nN_2' &= r_2 N_2 (1 - N_2/K_2) - b_2 N_1 N_2,\n\\end{aligned}\\right.\n\\]\nwhere \\(N_1(t)\\) and \\(N_2(t)\\) are two species. (The modeling details are unimportant now.) Show that the system has no periodic orbits. All parameters are positive. Hint: use the function \\(h(N_1,N_2)=1/(N_1 N_2)\\) in the Dulac‚Äôs criterium.",
    "crumbs": [
      "Assignments",
      "Prey-predator systems"
    ]
  },
  {
    "objectID": "Assignments/05_predators.html#leslie-model",
    "href": "Assignments/05_predators.html#leslie-model",
    "title": "Prey-predator systems",
    "section": "Leslie model",
    "text": "Leslie model\nLeslie (1948) proposed the following system as a model for the predator-prey dynamics: \\[\n\\left\\{\\begin{aligned}\nH' &= H (a - bH -cP), \\\\\nP' &= P (r - s P/H ).\n\\end{aligned}\\right.\n\\]\nIn other words, the predators \\(P\\) have a logistic type dynamics with carrying capacity proportional to the density of preys \\(H\\).\n\nState clearly, and in case criticize, the assumptions of the model.\n\n\nDraw the isoclines in the phase plane, and find the positive equilibrium.\n\n\nStudy its stability.",
    "crumbs": [
      "Assignments",
      "Prey-predator systems"
    ]
  },
  {
    "objectID": "Assignments/05_predators.html#optimal-hunting-strategy",
    "href": "Assignments/05_predators.html#optimal-hunting-strategy",
    "title": "Prey-predator systems",
    "section": "Optimal hunting strategy",
    "text": "Optimal hunting strategy\nConsider the general Gause model with logistic growth rate \\(r(H)\\) and numerical response \\(\\omega(H)\\) proportional to the functional response \\(\\pi(H)\\), that is \\(\\omega(H) = \\gamma \\pi(H)\\).\n\nWrite down the model.\n\n\nConsider \\(\\pi(H)\\) linear, with attack rate \\(a\\ge0\\). Compute the stable equilibrium population size of the predators.\n\n\nNow suppose that the predator species evolves over several generations, by adapting the value of \\(a\\), so to maximize its equilibrium size. Find the optimal value of \\(a\\).\n\n\nRepeat the previous point by assuming a Holling type 2 response.",
    "crumbs": [
      "Assignments",
      "Prey-predator systems"
    ]
  },
  {
    "objectID": "Assignments/05_predators.html#juvenile-predators",
    "href": "Assignments/05_predators.html#juvenile-predators",
    "title": "Prey-predator systems",
    "section": "Juvenile predators",
    "text": "Juvenile predators\nOne feature not very realistic of standard predator-prey models is that predators start catching prey as soon as they are born. One way to circumvent this problem is to introduce a class of juvenile predators, say \\(J(t)\\), that do not catch prey; juveniles become adults at constant rate, say \\(\\rho\\).\nTaking all other assumption from the first model that has been presented, and assuming that death rate is the same for juvenile and adult predators, we arrive at the system \\[\n\\begin{cases}\nH' = r (1‚àí\\frac{H}{K})H ‚àí aHP, & \\\\\nJ' = \\gamma aHP ‚àí \\rho J ‚àí \\mu J, & \\\\\nP' = \\rho J - \\mu P. &\n\\end{cases}\n\\]\n\nRead from the model, which is the probability that a newly born juvenile will still be a juvenile at time \\(t\\). And which is the probability that a newly born juvenile will still be a juvenile at time \\(t\\), conditional on being still alive at time \\(t\\)? From these computations, interpret the parameter \\(\\rho\\).\n\n\nOn the basis of the previous point discuss the realism of the assumption that juveniles become adults at constant rate (assumption necessary to frame the model in terms of ordinary differential equations). Suggest possible changes to the assumption.\n\n\nShow that using appropriate changes of variables system can be written as \\[\n\\begin{cases}\nx' = \\sigma x(1-x) ‚àí \\alpha x z, & \\\\\ny' = \\alpha x z ‚àí (\\nu+1)y, & \\\\\nz' = \\nu y - z. &\n\\end{cases}\n\\]\n\n\nShow that the new system has always the equilibria \\(E_0 = (0,0,0)\\) and \\(E_1 = (1, 0, 0)\\). Find the conditions under which it has also a positive equilibrium \\(E^+ = (x^+,y^+, z^+)\\) and compute its coordinates.\n\n\nFind the conditions for the stability of \\(E_1\\) and of \\(E^+\\). Note that there are parameter values such that all equilibria are unstable; if possible, use a computer to explore the bahaviour of solutions in such a case. Hint: use the Routh-Hurwitz criteria Theorem¬†A.7 .",
    "crumbs": [
      "Assignments",
      "Prey-predator systems"
    ]
  },
  {
    "objectID": "Assignments/05_predators.html#green-pools",
    "href": "Assignments/05_predators.html#green-pools",
    "title": "Prey-predator systems",
    "section": "Green pools",
    "text": "Green pools\nIt is often observed that, in the pools where fish have been added, water has a greenish colour, indicating a high algal biomass. Try to explain this using a prey-predator system, assuming that algae (prey)-zooplankton (predators) follow a Holling-type 2 predator-prey system, where prey grow according to the logistic equation and predators have mortality rate given by \\(d + bF\\) where \\(F\\) is fish density (assumed to be constant).\n\nWrite down the model.\n\n\nFind how equilbrium algal biomass and equilibrium stability vary with fish density \\(F\\), and comment the results.\n\n\nAssume now that the zooplankton mortality rate is \\(d + \\frac{bF}{1+cZ}\\) where \\(Z\\) is zooplankton density (Holling-type 2 functional response). Study the equilbria of the system.",
    "crumbs": [
      "Assignments",
      "Prey-predator systems"
    ]
  },
  {
    "objectID": "Assignments/05_predators.html#holling-type-3-response",
    "href": "Assignments/05_predators.html#holling-type-3-response",
    "title": "Prey-predator systems",
    "section": "Holling type 3 response",
    "text": "Holling type 3 response\nConsider the following prey-predator system: \\[\n\\left\\{\\begin{aligned}\nH' &= H \\Bigl( r(1-H/K)‚àí\\frac{a H P}{H^2 + \\beta^2} \\Bigr), \\\\\nP' &= P \\Bigl( -c + \\frac{\\gamma a H^2}{H^2 + \\beta^2} \\Bigr).\n\\end{aligned}\\right.\n\\]\nwhere all parameters are positive.\n\nGive a biological interpretation to these equations. Consider especially the functional response used in this model.\n\n\nFind all equilibria (in the first quadrant) and study their stability for \\(a\\gamma &lt; c\\).\n\n\nAssume \\(a\\gamma &gt; c\\). Find the conditions on \\(K\\) that make unstable the equilibrium without predators (but with a positive number of preys).\n\n\nShow that a positive equilibrium \\((H^*,P^*)\\), if it exists, is stable if \\[\n\\frac{r}{K} + a P^*\\phi'(H^*) &gt; 0,\\quad \\phi(H) = \\frac{H}{H^2+\\beta^2}.\n\\]\nHint: to show this, it is convenient to rewrite the system using the function \\(\\phi\\), and compute the positive equilibrium and the Jacobian through \\(\\phi(H^*)\\) and \\(\\phi'(H^*)\\), without explicitly computing these.\n\n\nShow that the previous conditions is equivalent to \\(\\Phi'(H^*) &lt; 0\\), where \\(\\Phi(H) = \\frac{r(1-H/K)(H^2+\\beta^2)}{a H}\\) is the function for the nullcline \\(H'=0\\), when \\(P=\\Phi(H)\\).\n\n\nScale the system by setting \\(x = H/\\beta\\), \\(y = P/(\\gamma\\beta)\\), \\(\\tau = ct\\). The resulting system should have only 3 parameters, that can be chosen as \\(\\eta = \\beta/K\\), \\(\\alpha = a\\gamma/c\\), \\(\\rho = r/c\\).",
    "crumbs": [
      "Assignments",
      "Prey-predator systems"
    ]
  },
  {
    "objectID": "Assignments/06_reactions.html",
    "href": "Assignments/06_reactions.html",
    "title": "Reactions",
    "section": "",
    "text": "Oscillating reaction\nLengyel, Rabai, and Epstein (1990) have proposed and analyzed a particularly elegant model of an oscillating chemical reaction, the chlorine dioxide-iodine-malonic acid reaction. (You can check out on some YouTube videos of oscillating chematical reactions.)\nAfter suitable non-dimensionalization, the model becomes \\[\n\\left\\{\\begin{aligned}\nx' &= a - x - \\frac{4xy}{1+x^2}, \\\\\ny' &= bx - \\frac{bxy}{1+x^2},\n\\end{aligned}\\right.\n\\]\nwhere \\(x=[\\mathrm{I}^-]\\) and \\(y=[\\mathrm{ClO}_2^-]\\), and \\(a\\), \\(b\\) are positive parameters.",
    "crumbs": [
      "Assignments",
      "Reactions"
    ]
  },
  {
    "objectID": "Assignments/06_reactions.html#oscillating-reaction",
    "href": "Assignments/06_reactions.html#oscillating-reaction",
    "title": "Reactions",
    "section": "",
    "text": "Find the equilibrium of the system, and prove that it is never a saddle point.\n\n\nIn the \\((a,b)\\) plane, find the region \\(\\mathcal{S}\\) such that the equilibrium is asymptotically stable.\n\n\nShow that there exists a limit cycle for any choice of the parameters \\((a,b)\\) in the complement of \\(\\mathcal{S}\\). Hint: apply the Poincar√©-Bendixson theorem, by constructing an equilibrium-free region in the phase space that traps all the orbits.\n\n\nWhat does it happen when we are on the boundary of \\(\\mathcal{S}\\)? Hint: the eigenvalues of the linearized system are purely imaginary, thus the equilibrium is actually an infinitesimally small limit cycle.\n\n\nFind the period of the (infinitesimally small) limit cycle on the boundary as a function of \\(a\\). Show that as \\(a\\to\\infty\\), \\(T\\to\\frac{2\\pi}{\\sqrt{15}}\\). Hint: since eigenvalues are purely imaginary, locally the solution is of the form \\(e^{i\\omega t}\\) for some \\(\\omega\\). Use this fact to obtain the period.",
    "crumbs": [
      "Assignments",
      "Reactions"
    ]
  },
  {
    "objectID": "Assignments/06_reactions.html#fast-slow-systems",
    "href": "Assignments/06_reactions.html#fast-slow-systems",
    "title": "Reactions",
    "section": "Fast-slow systems",
    "text": "Fast-slow systems\nConsider the following system of ODEs:\n\\[\n\\left\\{\\begin{aligned}\ny' &= z, \\\\\nz' &= \\frac{1}{\\varepsilon}(z - z^3 - y).\n\\end{aligned}\\right.\n\\]\nwith \\(\\varepsilon\\ll 1\\).\n\nExplain why the system has fast-slow dynamics, and identify the fast and the slow variable.\n\n\nWith the help of Tikhonov‚Äôs theorem, qualitatively study the dynamics of the system.",
    "crumbs": [
      "Assignments",
      "Reactions"
    ]
  },
  {
    "objectID": "Assignments/06_reactions.html#reversible-reactions",
    "href": "Assignments/06_reactions.html#reversible-reactions",
    "title": "Reactions",
    "section": "Reversible reactions",
    "text": "Reversible reactions\nIn the derivation of Michaelis-Menten equation, it is assumed that the back-reaction is negligibly slow. Consider now the full system of reactions: \\[\n\\ce{S + E &lt;=&gt;[$k_{-1}$][$k_1$] C &lt;=&gt;[$k_{-2}$][$k_2$] P + E}.\n\\]\n\nWrite down the system of equations corresponding to this scheme. Assume that at the start, there is neither complex \\(C\\) nor product \\(P\\).\n\n\nNote that two quantities are conserved: \\([E] + [C]\\) and \\([S] + [C] + [P]\\). Use this to decrease the number of equations.\n\n\nMake the equations non-dimensional, and in so doing introduce the parameter \\(\\varepsilon = \\frac{e_0}{s_0}\\), the ratio of initial values of enzyme and substrate.\n\n\nGo to the limit \\(\\varepsilon \\to 0^+\\) which amounts to make the quasi-steady-state approximation.\n\n\nConsider the resulting differential equation for \\(S(t)\\) and show that it will converge to an equilibrium \\(S_*\\) where the concentrations of substrate and product will satisfy Haldane‚Äôs relationship: \\[\n\\frac{P_*}{S_*} = \\frac{k_1k_2}{k_{-1}k_{-2}}.\n\\]",
    "crumbs": [
      "Assignments",
      "Reactions"
    ]
  },
  {
    "objectID": "Assignments/06_reactions.html#phosphorylation",
    "href": "Assignments/06_reactions.html#phosphorylation",
    "title": "Reactions",
    "section": "Phosphorylation",
    "text": "Phosphorylation\nAssume that an enzyme \\(E\\) can bind to a substrate molecule \\(S\\) allowing it to become phosphorylated, \\(S_p\\), as in the following scheme: \\[\n\\ce{S + E &lt;=&gt;[$k_1$][$k_{-1}$] C_1 -&gt;[$k_2$] S_p + E}\n\\]\nA different enzyme \\(F\\) can bind to the phosphorylated molecule \\(S_p\\) allowing it to be dephosphorylated, according to the scheme: \\[\n\\ce{S_p + F &lt;=&gt;[$k_3$][$k_{-3}$] C_2 -&gt;[$k_4$] S + F}\n\\]\n\nWrite down the system of equations corresponding to this scheme. Assume that at the start, there are no complexes \\(C_1\\) or \\(C_2\\).\n\n\nProve that two quantities are conserved: \\([E] + [C_1]\\) and \\([F] + [C_2]\\). Use this to decrease the number of equations.\n\n\nFollowing the same approach used in the derivation of Michaelis-Menten equation, arrive at a single equation for the (appropriately scaled) variable \\([S]\\).",
    "crumbs": [
      "Assignments",
      "Reactions"
    ]
  },
  {
    "objectID": "Assignments/06_reactions.html#chain-of-reactions",
    "href": "Assignments/06_reactions.html#chain-of-reactions",
    "title": "Reactions",
    "section": "Chain of reactions",
    "text": "Chain of reactions\n(Similar to Keener and Sneyd (2009), Exercise 1.5)\nConsider an enzymatic reaction in which an enzyme \\(E\\) can be activated or inactivated by the same chemical substance \\(A\\), as follows \\[\n\\begin{aligned}\n\\ce{E + A   &&lt;=&gt;[$k_1$][$k_{-1}$] E_1 } \\\\\n\\ce{E_1 + A &&lt;=&gt;[$k_2$][$k_{-2}$] E_2 } \\\\\n\\ce{S + E_1 &-&gt;[k_3] P + E } \\\\\n\\end{aligned}\n\\]\nAssume that the initial concentration of \\(E\\) is \\(e_0 \\ll a_0\\), where \\(a_0\\) is the initial concentration of \\(A\\). Further assume that \\(E_1\\), \\(E_2\\), and \\(P\\) are not present at \\(t = 0\\).\n\nWrite down the system of equations corresponding to this scheme.\n\n\nAfter having identified quantities that are conserved, rescale the variables with the initial concentrations of \\(E\\) or \\(A\\). Using the assumption \\(e_0 \\ll a_0\\) (while the values of all rates are comparable), show that the equations result in a slow-fast system.\n\n\nUsing the quasi-steady-state analysis, show that \\(x = [A]/a_0\\) satisfies an equation of the type: \\[\nx' = ‚àí \\frac{\\beta x s}{1 + \\alpha x + \\delta s + \\gamma x^2}.\n\\]\n\n\n\n\n\nKeener, James, and James Sneyd. 2009. Mathematical Physiology: I: Cellular Physiology. Second. Vol. 8/I. Springer. https://strutture-provincia.primo.exlibrisgroup.com/permalink/39SBT_INST/ghkipk/alma991012367489706186.\n\n\nLengyel, Istvan, Gyula Rabai, and Irving R Epstein. 1990. ‚ÄúExperimental and Modeling Study of Oscillations in the Chlorine Dioxide-Iodine-Malonic Acid Reaction.‚Äù Journal of the American Chemical Society 112 (25): 9104‚Äì10.",
    "crumbs": [
      "Assignments",
      "Reactions"
    ]
  },
  {
    "objectID": "Laboratories/01_ODE_Integration/lab_ODEinteg.html",
    "href": "Laboratories/01_ODE_Integration/lab_ODEinteg.html",
    "title": "Lab 01: Numerical integration",
    "section": "",
    "text": "Stability and convergence\nConsider the IVP (in numerics, this is also called test problem): \\[\n\\begin{cases}\ny' = \\lambda y, \\\\\ny(t_0) = y_0. &\n\\end{cases}\n\\]",
    "crumbs": [
      "Labs",
      "Lab 01: Numerical integration"
    ]
  },
  {
    "objectID": "Laboratories/01_ODE_Integration/lab_ODEinteg.html#stability-and-convergence",
    "href": "Laboratories/01_ODE_Integration/lab_ODEinteg.html#stability-and-convergence",
    "title": "Lab 01: Numerical integration",
    "section": "",
    "text": "Write down the forward Euler scheme and solve for the equation for various values of \\(h\\), with \\(\\lambda = -1\\). Is it always true that \\(y_n \\to 0\\) for \\(n\\to\\infty\\)? Use \\(t_0=0\\) and \\(y_0=1\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nMATLABPython\n\n\nlmbda = -1.0;\nf = @(u) lmbda*u;\nT = 8;\nu0 = 1.0;\nfig = figure; hold on;\nfor h = [0.1,0.2,0.5,1.0,2.0,2.5]\n    t = 0:h:T;\n    u = zeros(size(t));\n    u(1) = u0;\n    for n = 1:length(t)-1\n        u(n+1) = u(n) + h*f(u(n));\n    end\n    plot(t,u,'LineWidth',2.0,'DisplayName',num2str(h));\nend\nhold off; grid on; legend;\n\n\n\nimage\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nlmbda = -1.0\nf = lambda u: lmbda*u\nT = 8\nu0 = 1.0\nfig,ax = plt.subplots()\nfor h in [0.1,0.2,0.5,1.0,2.0,2.5]:\n    t = np.arange(0,T,h)\n    u = np.zeros(t.shape[0])\n    u[0] = u0\n    for n in range(t.shape[0]-1):\n        u[n+1] = u[n] + h*f(u[n])\n    ax.plot(t,u,lw=2.0,label=f'{h}')\nax.legend()\nax.grid(True)\nplt.show()\n\n\n\n\nSolution\n\n\n\n\n\n\n\nNo, for \\(h \\ge2\\) there is no convergence.\n\n\n\n\nWrite down the backward Euler scheme and repeat the previous step.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nMATLABPython\n\n\nlmbda = -1.0;\nT = 8;\nu0 = 1.0;\nfig = figure; hold on;\nfor h = [0.1,0.2,0.5,1.0,2.0,2.5]\n    t = 0:h:T;\n    u = zeros(size(t));\n    u(1) = u0;\n    for n = 1:length(t)-1\n        u(n+1) = u(n)/(1-h*lmbda);\n    end\n    plot(t,u,'LineWidth',2.0,'DisplayName',num2str(h));\nend\nhold off; grid on; legend;\n\n\n\nimage\n\n\n\n\n\nfig,ax = plt.subplots()\nfor h in [0.1,0.2,0.5,1.0,2.0,2.5]:\n    t = np.arange(0,T,h)\n    u = np.zeros(t.shape[0])\n    u[0] = u0\n    for n in range(t.shape[0]-1):\n        u[n+1] = u[n]/(1 - h*lmbda)\n    ax.plot(t,u,lw=2.0,label=f'{h}')\nax.legend()\nax.grid(True)\nplt.show()\n\n\n\n\nSolution\n\n\n\n\n\n\n\nIt is always stable.\n\n\n\n\nSolve the equation with the forward Euler scheme and compare the solution to the exact one. How does the absolute error decrease with respect to \\(h\\)? Plot the error in the logarithmic scale. Set \\(\\lambda=1\\) and \\(y(0)=1\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nMATLABPython\n\n\nlmbda = -1.0;\nf = @(u) lmbda*u;\nT = 8;\ny0 = 1.0;\nNh = 10;\nh0 = 0.1;\nerr = zeros(1,Nh);\nfor k = 1:Nh\n    h = h0/(2^(k-1));  % divide h by 2 every iteration\n    t = 0:h:T;\n    u = zeros(size(t));\n    uexact = y0*exp(lmbda*t);  % we know exact solution\n    u(1) = y0;\n    for n = 1:length(t)-1\n        % Forward Euler\n        u(n+1) = u(n) + h*f(u(n));\n    end\n    % infinity norm\n    err(k) = norm(u-uexact,inf);\nend\n\nloglog(h0./(2.^(0:Nh-1)),err,'-*','LineWidth',2.0);\ngrid on; xlabel('h'); ylabel('Error');\n\n% in loglog scale we expect a line, the slope is the order\np = polyfit((0:Nh-1)*log(2),-log(err),1);\ntitle(sprintf('Estimated order = %f\\n',p(1)));\n\n\n\nimage\n\n\n\n\n\nlmbda = -1.0\nf = lambda u: lmbda*u\nT = 8\ny0 = 1.0\nNh = 10\nh0 = 0.1\nerr = np.zeros(Nh)\nfor k in range(Nh):\n    h = h0/(2**k)  # divide h by 2 every iteration\n    t = np.arange(0,T,h)\n    u = np.zeros(t.shape[0])\n    uexact = y0*np.exp(lmbda*t)  # we know exact solution\n    u[0] = y0\n    for n in range(t.shape[0]-1):\n        # Forward Euler\n        u[n+1] = u[n] + h*f(u[n])\n    # infinity norm\n    err[k] = np.linalg.norm(u-uexact,np.inf)\n\nfig,ax = plt.subplots()\nax.loglog(h0/(2**np.arange(Nh)),err,'-*',lw=2.0)\nax.grid(True)\nax.set_xlabel('h')\nax.set_ylabel('Error')\n\n# in loglog scale we expect a line, the slope is the order\np = np.polyfit(np.arange(Nh)*np.log(2),-np.log(err),1)\nax.set_title(f'Estimated order = {p[0]:g}')\nplt.show()\n\n\n\n\nSolution\n\n\n\n\n\n\n\nThe method converges linearly.\n\n\n\n\nRepeat the previous step with the Heun scheme: \\[y_{n+1}=y_n + \\tfrac{h}{2}\\bigl( f(y_n) + f(y_n + hf(y_n)) \\bigr).\\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nMATLABPython\n\n\nWe only need to change line 14-17 above to\nfor n = 1:length(t)-1\n    fn = f(u(n));  % save, we need it twice\n    u(n+1) = u(n) + h/2*(fn + f(u(n) + h*fn));\nend\n\n\n\nimage\n\n\n\n\n\nlmbda = -1.0\nf = lambda u: lmbda*u\nT = 8\ny0 = 1.0\nNh = 10\nh0 = 0.1\nerr = np.zeros(Nh)\nfor k in range(Nh):\n    h = h0/(2**k)  # divide h by 2 every iteration\n    t = np.arange(0,T,h)\n    u = np.zeros(t.shape[0])\n    uexact = y0*np.exp(lmbda*t)  # we know exact solution\n    u[0] = y0\n    for n in range(t.shape[0]-1):\n        # Forward Euler\n        fn = f(u[n])\n        u[n+1] = u[n] + h/2*(fn + f(u[n] + h*fn))\n    # infinity norm\n    err[k] = np.linalg.norm(u-uexact,np.inf)\n\nfig,ax = plt.subplots()\nax.loglog(h0/(2**np.arange(Nh)),err,'-*',lw=2.0)\nax.grid(True)\nax.set_xlabel('h')\nax.set_ylabel('Error')\n\n# in loglog scale we expect a line, the slope is the order\np = np.polyfit(np.arange(Nh)*np.log(2),-np.log(err),1)\nax.set_title(f'Estimated order = {p[0]:g}')\nplt.show()\n\n\n\n\nSolution with Heun method\n\n\n\n\n\n\n\nNow it converges quadratically. Note that it also costs twice, as for the cost is the number of evaluations of \\(f\\).",
    "crumbs": [
      "Labs",
      "Lab 01: Numerical integration"
    ]
  },
  {
    "objectID": "Laboratories/01_ODE_Integration/lab_ODEinteg.html#nonlinear-equations",
    "href": "Laboratories/01_ODE_Integration/lab_ODEinteg.html#nonlinear-equations",
    "title": "Lab 01: Numerical integration",
    "section": "Nonlinear equations",
    "text": "Nonlinear equations\nConsider the Nagumo or bistable model: \\[\nu' = a u (1-u)(u-\\alpha),\n\\]\nwith \\(\\alpha=0.2\\) and \\(a=10\\). Implement and solve the model in \\(t=0,\\ldots,2\\) with forward Euler for \\(h=10^{-3}\\) and various values of \\(u(0)\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nMATLABPython\n\n\nalpha = 0.2;\na = 10;\nf = @(u) a*u.*(1-u).*(u-alpha);\nh = 1e-3;\nT = 2;\nt = 0:h:T;\nu = zeros(size(t));\nfig = figure; hold on;\nfor u0 = [0.1,0.2,0.25,0.3,0.6,0.8]\n    u(1) = u0;\n    for n = 1:length(t)-1\n        u(n+1) = u(n) + h*f(u(n));\n    end\n    plot(t,u,'LineWidth',2.0,'DisplayName',num2str(u0));\nend\nhold off; grid on; legend;\n\n\n\nimage\n\n\n\n\n\nalpha = 0.2\na = 10\nf = lambda u: a*u*(1-u)*(u-alpha)\nh = 1e-3\nT = 2\nt = np.arange(0,T,h)\nu = np.zeros(t.shape[0])\nfig,ax = plt.subplots()\nfor u0 in [0.1,0.2,0.25,0.3,0.6,0.8]:\n    u[0] = u0\n    for n in range(t.shape[0]-1):\n        u[n+1] = u[n] + h*f(u[n])\n    ax.plot(t,u,lw=2.0,label=f'$u_0 = {u0}$');\nax.grid(True)\nax.legend()\nplt.show()",
    "crumbs": [
      "Labs",
      "Lab 01: Numerical integration"
    ]
  },
  {
    "objectID": "Laboratories/01_ODE_Integration/lab_ODEinteg.html#van-der-pol-equation",
    "href": "Laboratories/01_ODE_Integration/lab_ODEinteg.html#van-der-pol-equation",
    "title": "Lab 01: Numerical integration",
    "section": "Van Der Pol equation",
    "text": "Van Der Pol equation\nHere a more complicate example of a system. We solve the Van Der Pol equation: \\[\ny'' - \\mu(1-y^2)y' + y = 0.\n\\]\n\nSolve the van der Pol equation with forward Euler, using \\(\\mu=5\\), \\(y(0)=2\\) and \\(y'(0)=0\\). How small \\(h\\) needs to be? Find the approximate stability limit by bisection.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe first need to recast the equation into a first-order system: \\[\n\\left\\{\\begin{aligned}\ny_1' &= y_2, \\\\\ny_2' &= \\mu (1-y_1^2)y_2 - y_1.\n\\end{aligned}\\right.\n\\]\nThen the implementation is as before, with: \\[\n\\mathbf{y}^{(n+1)} = \\mathbf{y}^{(n)} + h \\mathbf{f}(\\mathbf{y}^{(n)}), \\quad\n\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix}, \\quad\n\\mathbf{f}(\\mathbf{y}) = \\begin{bmatrix} y_2 \\\\ \\mu (1-y_1^2)y_2 - y_1 \\end{bmatrix}.\n\\]\n\nMATLABPython\n\n\nmu = 5.0;\nf = @(y) [y(2); mu*(1-y(1).^2).*y(2)-y(1)];\nT = 40;\ny0 = [2.0,0.0];\nfig = figure; hold on;\nfor h = [1e-4,1e-3,1e-2,5e-2,7.85e-2]\n    t = 0:h:T;\n    y = zeros(length(t),2);\n    y(1,:) = y0;\n    for n = 1:length(t)-1\n        y(n+1,:) = y(n,:) + h*f(y(n,:))';\n    end\n    plot(t,y(:,1),'LineWidth',2.0,'DisplayName',num2str(h));\nend\nhold off; grid on; legend;\n\n\n\nimage\n\n\n\n\n\nmu = 5.0\nf = lambda y: np.array([y[1], mu*(1-y[0]**2)*y[1]-y[0]])\nT = 40\ny0 = [2.0,0.0]\nfig,ax = plt.subplots()\nfor h in [1e-4,1e-3,1e-2,5e-2,7.85e-2]:\n    t = np.arange(0,T,h)\n    y = np.zeros((t.shape[0],2))\n    y[0,:] = y0\n    for n in range(t.shape[0]-1):\n        y[n+1,:] = y[n,:] + h*f(y[n,:])\n    ax.plot(t,y[:,0],lw=2.0,label=f'h = {h}')\nax.grid(True)\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nBeyound \\(h \\approx 0.0785\\) (found by hand) the system becomes unstable during the rapid upstroke phase (that‚Äôs the steepest part). Please note how the accuracy is significantly reduced as \\(h\\) decreases, in particular with a phase shift.\n\n\n\n\nSolve the van der Pol equation with backward Euler. Design a strategy to solve the non-linear equation. Is the scheme always stable?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe need to solve the following non-linear problem \\[\n\\mathbf{y}^{(n+1)} = \\mathbf{y}^{(n)} + h \\mathbf{f}(\\mathbf{y}^{(n+1)}),\n\\]\nin the unknown \\(\\mathbf{y}^{(n+1)}\\). We define the residual function: \\[\n\\mathbf{r}(\\mathbf{z}) \\coloneqq \\mathbf{z} - \\mathbf{y}^{(n)} - h \\mathbf{f}(\\mathbf{z}),\n\\]\nso that the above problem is to find \\(\\mathbf{y}^{(n+1)}\\) such that \\(\\mathbf{r}(\\mathbf{y}^{(n+1)})=0\\).\nWe implement the Newton‚Äôs method to solve the residual equation: starting from \\(\\mathbf{z}^{(0)} = \\mathbf{y}^{(n)}\\), \\[\n\\mathbf{z}^{(k+1)} = \\mathbf{z}^{(k)} - \\mathbf{J}^{-1}(\\mathbf{z}^{(k)}) \\mathbf{f}(\\mathbf{z}^{(k)}),\n\\]\nuntil \\(\\|\\mathbf{z}^{(k)}\\| &lt; \\text{tol}\\), and then we set \\(\\mathbf{y}^{(n+1)} = \\mathbf{z}^{(k)}\\).\nThe Jacobian is: \\[\n\\mathbf{J}(\\mathbf{z}) = \\mathbf{I} + h \\frac{\\partial\\mathbf{f}}{\\partial\\mathbf{z}}.\n\\]\nSince implicit Euler is more stable, we can use larger time steps.\n\nMATLABPython\n\n\nmu = 5.0;\nf = @(y) [y(2); mu*(1-y(1).^2).*y(2)-y(1)];\nJ = @(y) [0,1; -2*mu*y(1).*y(2)-1, mu*(1-y(1).^2)];\nT = 40;\ny0 = [2.0,0.0];\nkmax = 20;\ntol = 1e-10;\nfig = figure; hold on;\nfor h = [1e-4,1e-3,1e-2,5e-2,7.85e-2]\n    t = 0:h:T;\n    y = zeros(length(t),2);\n    y(1,:) = y0;\n    for n = 1:length(t)-1\n        % we apply Newton's iterations\n        % initial guess\n        y(n+1,:) = y(n,:);\n        for k = 1:kmax\n            % residual, we want y such that r(y)=0\n            r = - y(n+1,:) + y(n,:) + h*f(y(n+1,:))';\n            % Jacobian dr/dy\n            A = -eye(2) + h*J(y(n+1,:));\n            % correction\n            dy = -A \\ r';\n            y(n+1,:) = y(n+1,:) + dy';\n            % stopping condition\n            if norm(dy) &lt; tol, break, end\n        end\n    end\n    plot(t,y(:,1),'LineWidth',2.0,'DisplayName',num2str(h));\nend\nhold off; grid on; legend;\n\n\n\nimage\n\n\n\n\n\nmu = 5.0\nf = lambda y: np.array([y[1], mu*(1-y[0]**2)*y[1]-y[0]])\nJ = lambda y: np.array([[0,1],[-2*mu*y[0]*y[1]-1, mu*(1-y[0]**2)]])\n\nT = 40\ny0 = [2.0,0.0]\nkmax = 20\ntol  = 1e-10\nfig,ax = plt.subplots()\nfor h in [5e-2,1e-1,2e-1]:\n    t = np.arange(0,T,h)\n    y = np.zeros((t.shape[0],2))\n    y[0,:] = y0\n    for n in range(t.shape[0]-1):\n        # we apply Newton's iterations\n        # initial guess\n        y[n+1,:] = y[n,:]\n        for k in range(kmax):\n            # residual, we want y such that r(y)=0\n            r = - y[n+1,:] + y[n,:] + h*f(y[n+1,:])\n            # Jacobian dr/dy\n            A = -np.eye(2) + h*J(y[n+1,:])\n            # correction\n            dy = np.linalg.solve(-A, r)\n            y[n+1,:] += dy\n            # stopping condition\n            if np.linalg.norm(dy) &lt; tol: break\n    ax.plot(t,y[:,0],lw=2.0,label=f'h = {h}')\nax.grid(True)\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nNow it is always stable, but the computational cost is higher.",
    "crumbs": [
      "Labs",
      "Lab 01: Numerical integration"
    ]
  },
  {
    "objectID": "Laboratories/01_ODE_Integration/lab_ODEinteg.html#conservation-of-energy",
    "href": "Laboratories/01_ODE_Integration/lab_ODEinteg.html#conservation-of-energy",
    "title": "Lab 01: Numerical integration",
    "section": "Conservation of energy",
    "text": "Conservation of energy\nConsider the 2nd-order ODE \\(y'' + \\omega^2 y = 0\\) (linear oscillator).\n\nAfter recasting the equation to a first-order system, solve it with forward Euler using \\(\\omega = 1\\), \\(h=0.01\\), \\(y(0)=1\\), \\(y'(0)=0\\) and \\(t=0,\\ldots,100\\). Does the solution match the expected behavior?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe system is as follows: \\[\n\\mathbf{y}' = \\mathbf{A}\\mathbf{y},\\quad \\mathbf{A}=\\left[ \\begin{matrix} 0 & 1 \\\\ -\\omega^2 & 0 \\end{matrix} \\right].\n\\]\n\nMATLABPython\n\n\nomega = 1;\nA = [0 1; -omega^2 0];\nT = 100;\nu0 = [1; 0];\nh = 0.01;\nt = 0:h:T;\nu = zeros(2,length(t));\nu(:,1) = u0;\nfor n = 1:length(t)-1\n    u(:,n+1) = u(:,n) + h*A*u(:,n);\nend\nplot(t,u,'LineWidth',2.0);\ngrid on; legend('y','yprime');\n\n\n\nimage\n\n\n\n\n\nomega = 1\nA = np.array([[0, 1], [-omega**2, 0]])\nT = 100\nu0 = np.array([1, 0])\nh = 0.01\nt = np.arange(0,T,h)\nu = np.zeros((2,t.shape[0]))\nu[:,0] = u0\nfor n in range(t.shape[0]-1):\n    u[:,n+1] = u[:,n] + h*A @ u[:,n]\n\nplt.plot(t,u.T)\nplt.grid(True)\nplt.legend([\"$y$\",\"$y'$\"])\n\n\n\n\n\n\n\n\n\n\n\nWe notice that the amplitude of the variables increases, which shouldn‚Äôt be the case since the energy is conserved.\n\n\n\n\nTry again with backward Euler. Hint: the system is linear, you can use the backslash to solve it at each iteration.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nMATLABPython\n\n\nWe change line 10 to\nu(:,n+1) = (eye(2)-h*A) \\ u(:,n);\n\n\n\nimage\n\n\n\n\n\nu[:,0] = u0\nfor n in range(t.shape[0]-1):\n    u[:,n+1] = np.linalg.solve(np.eye(2)-h*A, u[:,n])\n\nplt.plot(t,u.T)\nplt.grid(True)\nplt.legend([\"$y$\",\"$y'$\"])\n\n\n\n\n\n\n\n\n\n\n\nWe have a similar problem, the discrete energy decreases.\n\n\n\n\nCompute the discrete energy of the system \\(E(t) = \\tfrac{1}{2}\\bigl( (y'(t))^2 + \\omega^2 y(t)^2 \\bigr)\\) and inspect the problem.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe compute the energy as\n\nMATLAB\nE = 0.5*(u(2,:).^2 + omega^2*u(1,:).^2);\n\n\nPython\n\nu[:,0] = u0\nfor n in range(t.shape[0]-1):\n    u[:,n+1] = u[:,n] + h*A @ u[:,n]\nplt.plot(t,0.5*(u[1,:]**2 + omega**2*u[0,:]**2))\n\nu[:,0] = u0\nfor n in range(t.shape[0]-1):\n    u[:,n+1] = np.linalg.solve(np.eye(2)-h*A, u[:,n])\nplt.plot(t,0.5*(u[1,:]**2 + omega**2*u[0,:]**2))\n\nplt.grid(True)\nplt.legend([\"Explicit Euler\", \"Implicit Euler\"])\nplt.title('Energy')\n\nText(0.5, 1.0, 'Energy')\n\n\n\n\n\n\n\n\n\nand we see that it increases in forward Euler and decreases in backward Euler.\n\n\n\n\n\nImplement the following strategy: first, update the velocity, then the position with the newly compute velocity. (This is called the symplectic Euler method.) Does the method conserve the (discrete) energy?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nMATLABPython\n\n\nomega = 1;\nT = 100;\nu0 = [1; 0];\nh = 0.01;\nt = 0:h:T;\nu = zeros(2,length(t));\nu(:,1) = u0;\nfor n = 1:length(t)-1\n    % new velocity\n    u(2,n+1) = u(2,n) - h*omega^2*u(1,n);\n    % new position\n    u(1,n+1) = u(1,n) + h*u(2,n+1);\nend\n\nE = 0.5*(u(2,:).^2 + omega*u(1,:).^2);\n\nplot(t,E,'LineWidth',2.0);\ngrid on; hold on;\n\n\n\nimage\n\n\n\n\n\nu[:,0] = u0\nfor n in range(t.shape[0]-1):\n    # new velocity\n    u[1,n+1] = u[1,n] - h*omega**2*u[0,n]\n    # new position\n    u[0,n+1] = u[0,n] + h*u[1,n+1]\n\nplt.plot(t,u.T)\nplt.grid(True)\nplt.legend([\"$y$\",\"$y'$\"])\n\n\n\n\n\n\n\n\n\n\n\nNow the discrete energy is bounded.",
    "crumbs": [
      "Labs",
      "Lab 01: Numerical integration"
    ]
  },
  {
    "objectID": "Laboratories/02_Predation/lab_Predation.html",
    "href": "Laboratories/02_Predation/lab_Predation.html",
    "title": "Lab 02: Predation",
    "section": "",
    "text": "Holling type predation\nHolling type functional responses can be experimentally observed with a simple game. This is inspired to the MSc thesis from Pulley (2020), where the author proposes a similar game with marshmallows.",
    "crumbs": [
      "Labs",
      "Lab 02: Predation"
    ]
  },
  {
    "objectID": "Laboratories/02_Predation/lab_Predation.html#holling-type-predation",
    "href": "Laboratories/02_Predation/lab_Predation.html#holling-type-predation",
    "title": "Lab 02: Predation",
    "section": "",
    "text": "The experiment\nWe need the following tools:\n\nA desk or a clean area where to perform the experiment;\nTwo stopwatches (there‚Äôs an app in every phone);\nA bag of small objects to be collected or eaten. In the original proposal, they were marshmallow, which can be eaten. If you‚Äôre concerned about your diet or teeth, you can use other objects (coins, beans, pens,‚Ä¶)\nA blindfold.\nA group of at least 4 students.\n\n\n\n\nDesk with a \\(0.31\\,\\text{m}^2\\) delimited area with \\(N=16\\) candies. The density here is \\(16/0.31 \\approx 52\\,\\text{candies}/\\text{m}^2\\). You can measure the area with an app.\n\n\nThe game is as follows:\n\nWe randomly place on the desk a certain number of objects. We take note of this number \\(N\\);\nStudent A is blindfolded;\nStudent B says ‚ÄúGo!‚Äù and starts the stopwatch. In the meanwhile, student A starts searching on the desk for an object.\nWhen Student A finds one object, he will collect/eat it.\nStudent C, with another stopwatch, will measure the total time spent eating/collecting. Thus, it starts the stopwatch when student A picks up the object, and stops it when student A starts searching again.\nStudent D will put a new object at random on the desk so to keep \\(N\\) constant.\n\nThe experiment can last 90 seconds. At the end of the experiment, we need to mark\n\nthe total number of preys \\(N\\),\nthe total number of preys captured \\(C\\),\nthe total handling time \\(T^*\\).\n\nTo avoid statistical fluctuations, we repeat the experiment 3 times. Data can be collected in a table. You can use the following link to copy the table: CandyLab spreadsheet.\n\n\n\nSpreadsheet for data collection\n\n\nOnce done, we compute the handling time \\(T_h = \\bar{T}^*/\\bar{C}\\).\n\n\nThe analysis\nSuppose that \\(T\\) is the total experiment time (90 seconds for us). During this time, we spend \\(T_s\\) in searching for a prey, and \\(T_h\\) in handling a prey. Thus, we have \\[\nT = T_s + C T_h,\n\\]\nwhere the total handling time is multiplied by the number of prey captured. The assumption we make is that the total number of captured preys is proportional to the searching time and the total number of prey \\(N\\). So: \\[\nC = a N T_s,\n\\]\nfor some \\(a&gt;0\\). Substituting \\(T_s\\) we obtain: \\[\nC = a N T_s = a N (T - C T_h),\n\\]\nthat we can solve for \\(C/T\\), that is the rate of predation (number of prey captured per unit time): \\[\n\\frac{C}{T} = \\frac{a N}{1 + a T_h N}\n\\]\nSo, the functional response should be exactly Holling type II.\n\n\nImport data\nBefore trying any fitting, we need to import the data into a software. For instance, this can be MATLAB or Python.\nSave and download the spreadsheet as csv format.\n\nMATLABPython\n\n\nUse the Import Data button, then select the columns of your spreadsheet. The variables will appear in your workplace.\n\nYou can access the variables with CandyLabGroup.NumberOfCandies, for instance. Just type CandyLabGroup to see all variables.\nPlot the results.\n\n\nIn Python there is no GUI for importing csv files, but you can use a module:\nimport csv\n\nimport csv\nwith open('CandyLab - Group 1.csv') as csvfile:\n    reader = csv.reader(csvfile)\n    lines = list(reader)\n\nN  = [int(l[0]) for l in lines[3:]]\nC1 = [int(l[1]) for l in lines[3:]]\nC2 = [int(l[2]) for l in lines[3:]]\nC3 = [int(l[3]) for l in lines[3:]]\n\n\n\n\n\nFitting\nThe objective is to fit the function: \\[\n\\frac{C}{T} = \\frac{a N}{1 + a T_h N}\n\\]\nSince \\(T\\) is fixed, we can fit \\(C\\) versus \\(N\\) for various values, and find \\(a\\) and \\(T_h\\).\nThere are at least 2 options you can try:\n\nOption 1: Linear fit via Lineweaver‚ÄìBurk plot\nThe Lineweaver‚ÄìBurk plot uses a simple transformation to make the above equation linear. In fact: \\[\ny = \\frac{T}{C} = \\frac{1}{a} \\frac{1}{N} + \\frac{1}{T_h} = \\alpha x + \\beta.\n\\]\nSo, the Holling type II relationship is linear with respect to the reciprocal. We can perform a linear regression to find the coefficients (use polyfit).\n\n\nOption 2: Nonlinear regression\nWe can approach the problem as follows: find \\((a,T_h)\\) such that \\[\ng(a,T_h) := \\sum_{i=1}^3 \\left( \\frac{C}{T_i} - \\frac{a N_i}{1 + a T_h N_i} \\right)^2 \\quad\\to\\quad\\min.\n\\]\nThis problem can be solved via optimization. In MATLAB, you can use nlinfit function, in Python scipy.optimize.curve_fit.\nOtherwise, you can try to use a steepest descrent approach: given an initial guess \\((a^{(0)},T_h^{(0)})\\), we update with: \\[\n(a^{(k+1)},T_h^{(k+1)}) = (a^{(k)},T_h^{(k)}) - \\eta \\nabla g((a^{(k)},T_h^{(k)})),\n\\]\nwhere \\(\\eta&gt;0\\) is the step length or learning rate.\n\n\n\n\nPulley, Melissa. 2020. ‚ÄúThe Marshmallow Lab: A Project-Based Approach to Understanding Functional Responses.‚Äù Master‚Äôs thesis, Utah State University. https://doi.org/10.26076/neyp-cc41.",
    "crumbs": [
      "Labs",
      "Lab 02: Predation"
    ]
  },
  {
    "objectID": "Laboratories/03_Bifurcations/lab_Bifurcations.html",
    "href": "Laboratories/03_Bifurcations/lab_Bifurcations.html",
    "title": "Lab 03: Bifurcations",
    "section": "",
    "text": "Installation of MatCont\nThe installation is easy, but it requires some care on macOS and Windows. In Matlab, try the following:\nIf the last command fails, then follow the instruction below and try again with the command init, only after mex -setup returns success.",
    "crumbs": [
      "Labs",
      "Lab 03: Bifurcations"
    ]
  },
  {
    "objectID": "Laboratories/03_Bifurcations/lab_Bifurcations.html#installation-of-matcont",
    "href": "Laboratories/03_Bifurcations/lab_Bifurcations.html#installation-of-matcont",
    "title": "Lab 03: Bifurcations",
    "section": "",
    "text": "unzip('https://gitlab.utwente.nl/m7686441/matcont/-/archive/main/matcont-main.zip')\ncd matcont-main\nmatcont\n\n\nmacOSWindowsGNU/Linux\n\n\nYou need to install the XCode, which is several GBs. After installation, open it once and follow the instruction to accept the license.\nIf you succeeded, then you should see in Matlab:\nmex -setup\nMEX configured to use 'Xcode with Clang' for C language compilation.\n\n\nAn easy option is to install the Matlab add-on MiniGW for Windows 10 or 11 64-bit. See the webpage. Try with mex -setup to check the compiler after installation.\nNote: if you already have Microsoft Visual C++ it should work already without MiniGW.\n\n\nIt should work out of the box. However, the compiler provided by your distribution might be too recent to work well with MEX. In this case, you might get some warnings during the compilation. On widespread distributions like Ubuntu it is usually fine.",
    "crumbs": [
      "Labs",
      "Lab 03: Bifurcations"
    ]
  },
  {
    "objectID": "Laboratories/03_Bifurcations/lab_Bifurcations.html#spruce-budworm-system",
    "href": "Laboratories/03_Bifurcations/lab_Bifurcations.html#spruce-budworm-system",
    "title": "Lab 03: Bifurcations",
    "section": "Spruce-budworm system",
    "text": "Spruce-budworm system\nWe start with a simple model to get familiar with MatCont. Open the program by typing matcont in the program folder. (only after running init at least once.) You should see\n\n\n\nThe MatCont interface.\n\n\nplus some other windows that you can ignore for now.\nIn this exercise we want to study the bifurcation diagram of the Spruce-Budworm ODE \\[\nu'=f(u,\\rho,q)=\\rho u\\Bigl(1-\\frac{u}{q}\\Bigr) - \\frac{u^2}{1+u^2},\n\\]\nwith respect to the parameters \\(\\rho\\) and \\(q\\). Remember that we rescaled the ODE assuming: \\[\nu(t) = \\frac{N(t)}{\\nu}, \\quad \\tau = \\frac{\\alpha P^*}{\\nu} t, \\quad \\rho = \\frac{r\\nu}{\\alpha P^*}, \\quad q = \\frac{K}{\\nu},\n\\]\nwhere\n\n\\(N(t)\\) is the population of budworms,\n\\(r\\) is the intrinsic growth rate,\n\\(P^*\\) is the number of predators of the budworms (e.g., birds),\n\\(\\alpha\\) is the attack rate,\n\\(K\\) is the carrying capacity,\n\\(\\nu\\) is the value of \\(N\\) yielding half of the maximum killing rate.\n\n\nCreate a new system Select-&gt;System-&gt;New. In the new window, compile as follows:\n\nName: SpruceBudworm\nCoordinates: u\nParameters: rho,q\n\nPut the first 3 derivatives to symbolically. (If symbolically is disabled, install the Symbolic Toolbox.)\nWrite the equation in the textbox at the bottom, using a natural language:\nu' = rho*u*(1-u/q)-u^2/(1+u^2)\nPress OK.\nWe should see something like this: \nNow we compute an orbit. We start at \\(t=0\\) with \\(u(0)=0.1\\). The initial parameters are: \\(\\rho=1\\) and \\(q=8\\).\nWe do as follows:\n\nType-&gt;Initial Point-&gt;Point\nIn Starter, we set the initial conditions and parameters\nIn Methods, reduce the tolerance for numerical integration\nIn the main window, Compute-&gt;Forward, then inspect View Results.\n\nTo visualize the solution,\n\nWindow/Output-&gt;Graphic-&gt;2D Plot,\nSelect in Layout time as ascissa and \\(u\\) as ordinate,\nIn the plot window, MatCont-&gt;Redraw Curve.\n\nClearly we are not yet at equilibrium. Go back and increase the integration time in Methods, then try again.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe first attempt should be like this:\n\nIncreasing the final time gives:\n\n\n\n\n\nOnce we have reached an equilibrium, we can try to continue it with respect to \\(\\rho\\).\nSelect as initial point the last integration point of the orbit: Select-&gt;Initial Point. You should notice that in the Starter window we now have different values.\nTo continue an equilibrium,\n\nType-&gt;Initial Point-&gt;Equilibrium\nType-&gt;Curve-&gt;Equilibrium\nSelect in Starter the parameter of interest, that is \\(\\rho\\) for us\nIn Continuer window, reduce the MaxStepsize to 0.02 and the MaxNumPoints to 100. (This is not strictly necessary, just to be safe.)\nIn Plot2D (if not open, open it with Window/Output-&gt;Graphic-&gt;2D Plot) change the layout to parameter rho (range \\([0,2]\\)) in the ascissa and u (range \\([0,10]\\)) on the ordinate. Clear the plot.\nIn the main window, Compute-&gt;Forward.\n\nWhat do you observe?\nTry now with Compute-&gt;Backward and then Compute-&gt;Extend. You should get a Limit Point (LP in MatCont). Continue to extend (on the left) the curve until you get another LP and a Branch Point (BP). Stop here.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe have\n\nFor the forward integration, we have no bifurcation points.\n\nAfter backward continuation, we encounter 3 bifurcation points: 2 limit points (LP) and 1 branch point (BP). Their coordinates are\n\n\n\nBifurcation\n\\(u\\)\n\\(\\rho\\)\n\n\n\n\nLP\n3.71\n0.469\n\n\nLP\n1.19\n0.579\n\n\nBP\n0\n0\n\n\n\nThe figures can be found in the Data Browser.\n\n\n\n\nIn order to visualize the stability, you need to change the Plot Properties, and set what you prefer for EP if unstable, e.g., a dashed line. Redraw the previous plot and check which branch of the equilibrium curve is unstable.\nAlternatively, try to plot in 3-D the parameter, the coordinate and the real part of the eigenvalue (navigate the Layout panel).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n Note that the intermediate branch is unstable.\n\n\n\n\nTry again with different values of \\(q\\), for instance \\(q=1\\), \\(q=5\\) and \\(q=10\\).\nOne way to compute the initial equilibrium point is to start with a new orbit (Select-&gt;Curve and then New). Then you can repeat the above process. Do not clear the plot: we can keep multiple curves.\nDo you observe limit points in all cases?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNo, this is the case for \\(q=1\\): \n\n\n\n\nIt seems that for small \\(q\\) there are no limit points, whereas for large \\(q\\) we have 2 points. In particular, as we decrease \\(q\\) the limit points get closer, thus we expect a collision. Such collision is another type of bifurcation, called cusp bifurcation, which has co-dimension 2. That is, it involves 2 parameters.\nWe can easily compute the cusp point in MatCont\n\nSelect an equilibrium curve with at least a limit point (Select-&gt;Curve)\nSelect the limit point (Select-&gt;Initial Point)\nSelect Type-&gt;Initial Point-&gt;Limit Point\nSelect Type-&gt;Curve-&gt;Limit Point\nIn the Starter window, we now have to select 2 parameters: select both rho and q (as active parameters, the box on the right.)\n\nNow we can compute the curve with Compute-&gt;Backward or Compute-&gt;Forward. Extend it a few times if necessary. Find \\(\\rho\\) and \\(q\\) corresponding to the cusp.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHere is the cusp: \nThe value of the bifurcation is \\((\\rho,q)=(0.650,5.196)\\). So, for lower values of \\(q\\) there is no folding, thus no limit points.\n\n\n\n\nTry to visualize the plot in 3-D.\nTo get an idea of the function, consider \\(\\rho\\in[0,1]\\), \\(q\\in[0,10]\\) and \\(u\\in[0,10]\\). Find the equation that implicitly define the surface of equilibria (parametrized in \\(\\rho\\) and \\(q\\)).\nUse the command isosurface as follows:\n[R Q U] = meshgrid(0.0:0.01:1,0.0:0.1:10,0:0.1:10);\nV = ...; % &lt;- EDIT!\np = patch(isosurface(R,Q,U,V,0.0));\nisonormals(R,Q,U,V,p);\np.FaceColor = 'red';\np.EdgeColor = 'none';\ndaspect([1 10 10])\nview(3)\ncamlight; lighting phong\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe function should be\nV = RHO.*(1-U./Q) - U./(1+U.^2);\nYou should see this: \nIf you don‚Äôt get it, MatCont is messing up with Matlab plotting. Try closing MatCont (no worries, the last state is saved.)\n\n\n\n\nWith the 3-D plot at hands, with a fixed value of \\(q\\), explain the effect of hysteresis by imaging a ‚Äúperson moving on the landscape‚Äù. What does the ridge of limit points represent? And the cusp?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor a fixed value of \\(\\rho\\) and \\(q\\), the equilibrium is exactly on the surface. Since we can have more than one intersection (in particular in the bistable region), there could exist multiple equilibria.\nFor a fixed value of \\(q\\), but taking \\(\\rho\\) variable, we are taking sections of the surface with the plane \\(q=\\bar{q}\\). Here, we may get the sigmoidal curve of equilibria, or just a curve with no bifurcations. In the 3D plot, the ridge of the ‚Äúcliff‚Äù corresponds to limit points (and the same for the back face).\nThe top of the cliff is the outbreak region, the bottom is the refuge. So, we get the hysteresis by first falling off the cliff, then walking in the shaded region, and go up again from behind (ok, this is perhaps far fetched as an analogy‚Ä¶)\nAnother interesting path, after falling down, is to walk towards lower values of \\(q\\), where there is no cliff. Then we can climb again without the need of undergoing the bifurcation.",
    "crumbs": [
      "Labs",
      "Lab 03: Bifurcations"
    ]
  },
  {
    "objectID": "Laboratories/03_Bifurcations/lab_Bifurcations.html#rosenzweig-macarthur-model",
    "href": "Laboratories/03_Bifurcations/lab_Bifurcations.html#rosenzweig-macarthur-model",
    "title": "Lab 03: Bifurcations",
    "section": "Rosenzweig-MacArthur model",
    "text": "Rosenzweig-MacArthur model\nA sufficiently complex prey-predator model is the following: \\[\n\\left\\{\\begin{aligned}\nu' &= u \\Bigl(\\rho (1-u) - \\frac{\\alpha\\delta v}{1+\\delta u}\\Bigr), \\\\\nv' &= v\\Bigl(-1 + \\frac{\\alpha\\delta u}{1+\\delta u}\\Bigr),\n\\end{aligned}\\right.\n\\]\nwhere \\(u(t)\\) and \\(v(t)\\) are the (non-dimensionalized) number of preys and predators, respectively.\n\nImplement the model in MatCont and integrate a trajectory \\(\\alpha = 3\\), \\(\\rho = 1\\) and with initial conditions \\(u(0)=0.9\\) and \\(v(0)=0.9\\). As integration time put a large number, say \\(T=100\\). Try different values of \\(\\delta\\in[0,5]\\), and discuss the behaviour of the system. Hint: you can plot multiple trajectories on the same graph. Try to visualize the phase portrait.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is what we get for \\(\\delta=1\\). The equilibrium is at \\((u,v) = (0.5,0.25)\\).  For larger values of \\(\\delta\\) we may converge towards the carrying capacity.\nNote: don‚Äôt call the parameter \\(\\alpha\\) with alpha, MatCont will complain. Use alph, for instance.\n\n\n\n\nApproximate the equilibrium \\(E^*\\), intersection of the two non-trivial nullcline, for \\(\\delta = 0.6\\). Then, continue the equilibrium with respect to \\(\\delta\\). How many bifurcation point do you observe? Visualize the diagram in the coordinates \\((\\delta,v^*)\\) and discuss it.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAfter integration for \\(\\delta=0.6\\), we have:  After continuing the equilibrium with respect to \\(\\delta\\) we have:  Notice that we encounter a Hopf bifurcation for \\(\\delta = 2\\).\n\n\n\n\nIs the equilibrium \\(E^*\\) always a focus? Hint: plot the imaginary part of one eigenvalue with respect to \\(\\delta\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n No, for \\(\\delta\\) small the eigenvalue is only real.\n\n\n\n\nAt some value of \\(\\delta\\) (which one?) there is a Hopf (H) bifurcation. It means that from the equilibrium we should expect the emergence of a limit cycle. Select as initial point the Hopf point. Then do:\n\nType-&gt;Initial Point-&gt;Hopf\nType-&gt;Curve-&gt;Limit Cycle\nIn Starter, select delta and period as continuation parameters\nCompute-&gt;Backward (or forward, it may vary)\n\nTry to visualize the limit cycles in 3-D, using the coordinates \\((\\rho,u^*,v^*)\\). What‚Äôs the period of the cycle at the end of the branch?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n We see that the limit cycle increases in size as \\(\\delta\\) increases.\n\n\n\n\nDefine a user function (Select-&gt;System-&gt;Manage Userfunctions) to monitor when \\(\\delta = 2.5\\) (for instance, res=delta-2.5). Once defined, activate it in the Starter window when continuing the Hopf bifurcation into a limit cycle. Find the period of the cycle for \\(\\delta = 2.5\\).\n\n\n\n\n\n\n\nSolution",
    "crumbs": [
      "Labs",
      "Lab 03: Bifurcations"
    ]
  },
  {
    "objectID": "Laboratories/04_Chaos/lab_chaos.html",
    "href": "Laboratories/04_Chaos/lab_chaos.html",
    "title": "Lab 04: Chaos",
    "section": "",
    "text": "Food chain\nThe standard prey-predator system only considers 2 species. In reality, many species interact and compete for survival. Ecological systems are typically very much interconnected. More species means more variables and parameters, thus more complexity.\nTo better appreciate the jump in complexity, we analyze here a simple system of 3 species: plant (\\(X\\)), herbivore (\\(Y\\)), and carnivores (\\(Z\\)).\nThe model reads as follows (see Hastings and Powell (1991)): \\[\n\\left\\{\\begin{aligned}\nX' &= X\\Bigl( r(1-X/K) - \\frac{a_1 Y}{1+b_1 X} \\Bigr), & \\\\\nY' &= Y\\Bigl( -d_1 + \\frac{c_1 a_1 X}{1+b_1 X} - \\frac{a_2 Z}{1+b_2 Y} \\Bigr), & \\\\\nZ' &= Z\\Bigl( -d_2 + \\frac{c_2 a_2 Y}{1+b_2 Y} \\Bigr). & \\\\\n\\end{aligned}\\right.\n\\]\nWe implement the model in a Matlab function FoodChain that can take \\(b_1\\) as free parameter.\nThe parameters are set as follows:",
    "crumbs": [
      "Labs",
      "Lab 04: Chaos"
    ]
  },
  {
    "objectID": "Laboratories/04_Chaos/lab_chaos.html#food-chain",
    "href": "Laboratories/04_Chaos/lab_chaos.html#food-chain",
    "title": "Lab 04: Chaos",
    "section": "",
    "text": "A partial food web illustrating the network of feeding relationships in a North American grassland and forest. (Encyclop√¶dia Britannica, Inc./Patrick O‚ÄôNeill Riley)\n\n\n\n\n\nfunction dy = FoodChain(~,y,b1)\n    % parameters\n    r=1;  K=1;    c1=1; c2=1;\n    a1=5; a2=0.1; b2=2; d1=0.4; d2=0.01;\n    % default for b1\n    if (nargin &lt; 3), b1 = 2.0; end\n    % rhs\n    X = y(1);\n    Y = y(2);\n    Z = y(3);\n    dX = r*X.*(1-X/K)-a1*X./(1+b1*X).*Y;\n    dY = c1*a1*X./(1+b1*X).*Y-d1*Y-a2*Y./(1+b2*Y).*Z;\n    dZ = c2*a2*Y./(1+b2*Y).*Z-d2*Z;\n    dy = [dX;dY;dZ];\nend\n\n\n\n\nParameter\nValue\n\n\n\n\n\\(r\\)\n\\(1\\)\n\n\n\\(K\\)\n\\(1\\)\n\n\n\\(a_1\\)\n\\(5\\)\n\n\n\\(a_2\\)\n\\(0.1\\)\n\n\n\\(b_1\\)\n\\(2.0 \\div 3.0\\)\n\n\n\\(b_2\\)\n\\(2\\)\n\n\n\\(c_1\\)\n\\(1\\)\n\n\n\\(c_2\\)\n\\(1\\)\n\n\n\\(d_1\\)\n\\(0.4\\)\n\n\n\\(d_2\\)\n\\(0.01\\)\n\n\n\n\nSimulate the model with ode45 for \\(b_1=2\\), initial conditions \\(X(0)=Y(0)=Z(0)=1\\), and for a final time \\(T=1000\\). Plot the solution both as a time series and in a phase portrait. Does the system converge towards a limit cycle? Hint: try to extend the final time.\nTo use ode45, please do as follows:\ny0 = [1,1,1];  % initial condition\nTT = 1000.0;\n\n% we define a function with fixed b1\nf = @(t,y) FoodChain(t,y,2.0);\nopts  = odeset('AbsTol',1e-10,'RelTol',1e-8);\n[T,Y] = ode45(f,[0,TT],y0,opts);\n\ndisp(Y(end,:));\n\nfig=figure;\nplot(T,Y,'LineWidth',1.0);\ntitle('Time series');\nlegend('X','Y','Z'); grid on;\n\nfig=figure;\nplot3(Y(:,1),Y(:,2),Y(:,3));\ntitle('Phase portrait');\ngrid on;\nxlabel('X'); ylabel('Y'); zlabel('Z');\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\nTime series for \\(b_1=2\\) up to \\(T=1000\\).\n\n\n\n\n\nPhase portrait for \\(b_1=2\\) up to \\(T=1000\\).\n\n\nNote that the variable \\(Z(t)\\) is still increasing over time. We cannot conclude that we have reached an equilibrium, although it looks like a limit cycle. We continue the integration until \\(T=10^4\\) and:\n\n\n\nTime series for \\(b_1=2\\) up to \\(T=10000\\).\n\n\n\n\n\nPhase portrait for \\(b_1=2\\) up to \\(T=10000\\).\n\n\nWe see that an equilibrium \\(E^* \\approx (0.7499, 0.1250, 13.7500)\\) has been reached.\n\n\n\n\nNow simulate the system with \\(b_1=2.2\\). Do you observe a limit cycle? Report the amplitude of the oscillations for each species.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe reuse the code above with \\(b_1 = 2.2\\). The solution looks like the following.\n\n\nThere is indeed a limit cycle. To report the amplitude and period, we continue the integration from the last point observed.\n\n\nThe amplitude we approximate by taking the difference between max and min. For the period, we find the peaks in the error function when we shift the function.\nf = @(t,y) FoodChain(t,y,2.2);\n\n% we integrate for some long time\n[~,Y] = ode45(f,[0,1e5],y0,opts);\n% then we integrate again for some time on the LC\n[T,Y] = ode45(f,[0,300],Y(end,:),opts);\n\n% amplitude\nA  = max(Y) - min(Y);\n\n% period\n[~,locs] = findpeaks(sum((Y - Y(1,:)).^2,2));\nP = T(locs(2))-T(locs(1));\nThe amplitude and period are respectively [0.1458, 0.0629, 0.4428] and 57.8563.\n\n\n\n\nVisualize the Poincar√© map (see A.7.4 Poincar√© maps) for the limit cycle. For constructing it, consider a plane orthogonal to the coordinate \\(Z\\) and placed so that passes through the midpoint of the cycle. Hopefully, the limit cycle will intersect it at some point. MATLAB ODE integrator offers a way to estimate ‚Äúevents‚Äù. Events are conditions computed along the trajectory that, once met, are returned by the integrator. For us, the condition is ‚Äúcheck if the trajectory intersects the Poincar√© plane‚Äù. If this happens, the integrator will record the position and time of the event.\nUsing the ode45 function (but it works also for other integrators), we need to define an event function as follows:\nfunction [position,isterminal,direction] = PoincareMap(~,y,c,n)\n  % C is center of the plane\n  % N is normal to the plane\n  % when the orthogonal projection is zero, we are on the plane\n  position   = dot(y(:)-c(:),n);\n  isterminal =  0;  % do not halt integration at the event\n  direction  = -1;  % only intersection in direction n\nend\nUse this code to visualize the Poincar√© map. Try to estimate the period.\nb1 = 2.2;\nf  = @(t,y) FoodChain(t,y,b1);\ny0 = [1,1,1];\n\n% we integrate for some long time\nopts  = odeset('AbsTol',1e-10,'RelTol',1e-8);\n[~,Y] = ode45(f,[0,1e5],y0,opts);\n% the last point should be on the LC\nyc = Y(end,:);\n% we integrate more to get the LC\n[T,Y] = ode45(f,[0,1e3],yc,opts);\n\n% the Poincare' map is a plane orthogonal to nn\n% and centered in pc\nnn = [0;0;1];\npc = trapz(T,Y,1) / T(end);\n\n% we can use the nullspace to get tangent vectors\nV  = null(nn');\n\n% this function is called every time 'position' is\n% zero in the right 'direction'\n% we measure the distance from the plane\nevent_fun  = @(t,y) PoincareMap(t,y,pc,nn);\nopts_event = odeset(opts,'Events',event_fun);\n\n% integration with events\n% YP is the solution; te,ye are the events\n[~,YP,te,ye,~] = ode45(f,[0,1e3],yc,opts_event);\n\nfig = figure;\nplot3(YP(:,1),YP(:,2),YP(:,3),'LineWidth',2.0);\nhold on; grid on;\nplot3(ye(:,1),ye(:,2),ye(:,3),'r.','MarkerSize',16);\ntitle(sprintf('Phase portrait (b_1 = %g)',b1));\nxlabel('X'); ylabel('Y'); zlabel('Z');\n\n% plot the Poincare plane\nrr = 0.25;\npp = [pc'+rr*V,pc'-rr*V];\np  = patch(pp(1,:),pp(2,:),pp(3,:),'r');\np.FaceAlpha = 0.4;\n\n% in-picture visualization of the Poincar√© plot\nye_proj = (ye-pc) - ((ye-pc) * nn) * nn';\naxes('Position',[.6 .6 .3 .3],'XTickLabel',[],...\n    'YTickLabel',[],'nextplot', 'add');\nplot(ye_proj(:,1),ye_proj(:,2),'r.',...\n    ye_proj(end,1),ye_proj(end,2),'b+','MarkerSize',16);\nxlim([-rr,rr]); ylim([-rr,rr]);\ngrid on; box on;\ntitle('Poincare'' map');\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nTo estimate the period, we can simply use te: the difference between two events is the period. Here, we have a period of 57.7923.\n\n\n\n\n(Optional) A limit cycle is stable if, when perturbed, attracts the orbits. Test this in the Poincar√© map, by perturbing the equilibrium within the plane, with a perturbation of \\(\\varepsilon = 10^{-2}\\):\n% ye is a point of the LC on the plane\ny0_new =  ye(end,:) + 1e-1*(V * (2*rand(2,1)-1))';\nAdapt the previous code to see the effect of the perturbation.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA possible code is the following:\nb1 = 2.2;\nf  = @(t,y) FoodChain(t,y,b1);\n\ny0_new =  ye(end,:) + 1e-1*(V * (2*rand(2,1)-1))';\n\nevent_fun  = @(t,y) PoincareMap(t,y,pc,nn);\nopts_event = odeset(opts,'Events',event_fun);\n\n[~,YP,~,ye,~] = ode45(f,[0,1e5],y0_new,opts_event);\n\nfig = figure;\nplot3(YP(:,1),YP(:,2),YP(:,3),'LineWidth',2.0);\nhold on; grid on;\nplot3(ye(:,1),ye(:,2),ye(:,3),'r.','MarkerSize',16);\ntitle(sprintf('Phase portrait (b_1 = %g)',b1));\nxlabel('X'); ylabel('Y'); zlabel('Z');\n\n% plot the Poincare plane\nrr = 0.25;\npp = [pc'+rr*V,pc'-rr*V];\np  = patch(pp(1,:),pp(2,:),pp(3,:),'r');\np.FaceAlpha = 0.4;\n\n% in-picture visualization of the Poincar√© plot\nye_proj = (ye-pc) - ((ye-pc) * nn) * nn';\naxes('Position',[.6 .6 .3 .3],'XTickLabel',[],...\n     'YTickLabel',[],'nextplot', 'add');\nplot(ye_proj(:,1),ye_proj(:,2),'r.',...\n     ye_proj(end,1),ye_proj(end,2),'b+','MarkerSize',16);\nxlim([-rr,rr]); ylim([-rr,rr]);\ngrid on; box on;\ntitle('Poincare'' map');\n\nThe equilibrium on the Poincar√© map is asymptotically stable, hence the limit cycle is asymptotically stable as well.\n\n\n\n\nSlightly increase the value of \\(b_1\\). Try with \\(b_1=2.3\\), \\(b_1=2.39\\), \\(b_1=2.392\\). What do you observe?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe \\(b_1 = 2.3\\) we have this:\n\nWe observe a so-called period-doubling bifurcation. The limit cycle splits and the period doubles. It is easier to understand the dynamics on the Poincar√© map. Here the stable equilibrium (corresponding to the limit cycle) undergoes a bifurcation for some value of \\(b_1\\in[2.2,2.3]\\). The equilibrium becomes unstable and a stable \\(2\\)-cycle appears. The \\(2\\)-cycle on the Poincar√© map corresponds to a limit cycle that requires two turns to return to the original point. We observed the same phenomenon in the discrete logistic equation.\n\nIf we increase again \\(b_1\\), there is a new period-doubling bifurcation for \\(b_1\\in[2.3,2.39]\\). Now on the Poincar√© map we have a \\(4\\)-cycle.\n\nA further increase of \\(b_1\\) yields yet another period-doubling bifurcation for \\(b_1\\in[2.39,2.392]\\), corresponding to an appearance of a \\(8\\)-cycle on the Poincar√© map.\nThe previous results closely reproduce the cascade of cycle formation in the discrete logistic equation. We expect new bifurcations with \\(16\\)-, \\(32\\)-, \\(2^n\\)-cycle formation. Please note that all the cycles are still present, but they are unstable.\nQ1.9. Keep increasing \\(b_1\\) up to \\(b_1=3\\). A strange attractor should have appeared. Visualize it in the phase portrait and as a time series. Provide some biological insights.\nWe reuse the code above, with \\(b_1 = 3.0\\).\n\n\nNote that the overall behavior looks almost periodic. However, we notice that every cycle is slighly different from the previous one. The dynamics is confined on a invariant set (the attractor).\n\n\n\n\n(Optional) One key feature of chaotic systems is the sensitivity to initial conditions. One measure of sensitivity is the Lyapunov characteristic exponent. Given two initial conditions \\(\\mathbf{y}_0\\) and \\(\\tilde{\\mathbf{y}}_0 = \\mathbf{y}_0 + \\varepsilon \\mathbf{z}\\), we visualize the distance between the trajectories over time: \\[\nd(t) = \\| \\tilde{\\mathbf{y}}(t) - \\mathbf{y}(t)\\| \\approx e^{L t}d(0).\n\\]\nWe call \\(L\\) the Lyapunov exponent. Try to visualize \\(d(t)\\) in the case of an equilibrium (\\(b_1=2\\)), a limit cycle (\\(b_1=2.2\\)), or a strange attractor (\\(b_1=3\\)). Use the semilogy command for visualizing the distance.\nThe Lyapunov exponent gives a measure stretching of the initial data. In this system there are 3 exponents: the dominant one we have estimated above (not exactly, but we got an idea about it). The others show up only with a careful selection of \\(\\mathbf{y}_0\\). In general we have:\n\n\n\nAttractor\n\\(L_1\\)\n\\(L_2\\)\n\\(L_3\\)\n\n\n\n\nEquilibrium\n\\({}-{}\\)\n\\({}-{}\\)\n\\({}-{}\\)\n\n\nCycle\n\\(0\\)\n\\({}-{}\\)\n\\({}-{}\\)\n\n\nTorus\n\\(0\\)\n\\(0\\)\n\\({}-{}\\)\n\n\nStrange attractor\n\\({}+{}\\)\n\\(0\\)\n\\({}-{}\\)\n\n\n\nDiscuss the results. (There is no torus in this system.)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe adapt the code above:\nb1 = 3.0;\nf  = @(t,y) FoodChain(t,y,b1);\ny0 = [1,1,1];\n\n% we integrate for some long time\nopts  = odeset('AbsTol',1e-10,'RelTol',1e-8);\n[~,Y] = ode45(f,[0,1e5],y0,opts);\nyc = Y(end,:);\n% we integrate more to get the attractor\n[T1,Y1] = ode45(f,0:2e3,yc,opts);\n\n% we perturb around the attractor\ny0tilde = yc + 1e-3*rand(size(Y(end,:)));\n[T2,Y2] = ode45(f,0:2e3,y0tilde,opts);\n\nf1 = figure;\nplot(T1,Y1(:,3),'LineWidth',1.0); hold on;\nset(gca,'ColorOrderIndex',1);\nplot(T2,Y2(:,3),'--','LineWidth',1.0);\ntitle('Time series');\nlegend('Z orig','Z perturb'); grid on;\n\nf2 = figure;\nsemilogy(T1,vecnorm(Y1-Y2,2,2),'LineWidth',1.0);\ntitle('Error');\ngrid on;\n\nFrom the time series (only for the variable \\(Z\\), the others are similar) we observe a sensitivity to initial data. A small initial perturbation is magnified, even though we are on the attractor. This is a characteristic of strange attractors, and makes the model unpredictible for long time.\nWe can also check the error in the log map, which should give us an indication of the Lyapunov exponent.\n\nOf course the error is bounded from above because the attractor itself is bounded.\n\n\n\n\nA positive dominant Lyapunov exponent means that the strange attractor is ‚Äústretching apart‚Äù orbits. Take several points as initial conditions by perturbing a single point from the strange attractor, similarly as done before. Plot the cloud of points in the phase portrait for various time instants, to highlight the mixing phenomenon.\nThe code samples 1000 points, then shows their position along with the attractor over time.\nb1 = 3.0;\nf  = @(t,y) FoodChain(t,y,b1);\ny0 = [1,1,1];\n\n% we integrate for some long time\nopts  = odeset('AbsTol',1e-10,'RelTol',1e-8);\n[~,Y] = ode45(f,[0,1e5],y0,opts);\nyc = Y(end,:);\n% we integrate more to get the attractor\n[T,Y] = ode45(f,[0,1e4],yc,opts);\n\n% plot the attractor\nf1 = figure;\nll = plot3(Y(:,1),Y(:,2),Y(:,3));\nll.Color = [0 0.4470 0.7410 0.2];\ngrid on; hold on;\ntitle(sprintf('Phase portrait (b_1 = %g)',b1))\n\n% we take a cloud of points around the initial condition\nNp = 1000; % number of points\nep = 1e-3; % perturbation\ny00   = yc + ep*randn(Np,3);\n[T,Y] = ode45(f,0:1:4e3,y00',opts);  % vectorized version\n\nYsol = shiftdim(reshape(Y,[],3,Np),1);\n\npp = plot3(Ysol(1,:,1),Ysol(2,:,1),Ysol(3,:,1),'.',...\n          'MarkerSize',16.0,'Color',[1,0,0,0.2]);\npause;\n\nfor tt=1:length(T)\n    pp.XData = Ysol(1,:,tt);\n    pp.YData = Ysol(2,:,tt);\n    pp.ZData = Ysol(3,:,tt);\n    pause(0.01);\nend\nhold off;\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nWe observe that the cloud of points is compact at the beginning, but then it is spread apart by the flow of the system. There is a mixing effect, due to stretching and compression.",
    "crumbs": [
      "Labs",
      "Lab 04: Chaos"
    ]
  },
  {
    "objectID": "Laboratories/04_Chaos/lab_chaos.html#transition-to-chaos",
    "href": "Laboratories/04_Chaos/lab_chaos.html#transition-to-chaos",
    "title": "Lab 04: Chaos",
    "section": "Transition to chaos",
    "text": "Transition to chaos\nA prototypical example of a chaotic system is the discrete logistic map: \\[\ny^{(k+1)} = f(y^{(k)}) = \\alpha y^{(k)}(1-y^{(k)}),\n\\]\nfor some \\(\\alpha&gt;0\\). Note that this a discrete in time equation, there is no time derivative! Still, it is a dynamical system, and we can discuss equilibria, cycles, and chaos. Surprisingly, the dynamic can be very complex even in dimension 1, whereas for ODEs we need at least 3 dimensions for chaos. The reason is that orbits in the discrete case can intersect and jump one over the other one.\n\nSet \\(\\alpha \\in [0,1)\\). Compute the trajectory of the system for \\(k\\ge 1\\) with \\(y^{(0)} \\in [0,1]\\). Orbits should converge toward the fixed point \\(y^* = 0\\). Note that fixed points (or equilibria) satisfy the equation \\[\ny^* = \\alpha y^* (1-y^*).\n\\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe trajectories tend to zero for various values of \\(\\alpha\\). We can study the stability of the fixed point via contraction theorem: if \\(f(y)\\) is a contraction in a neighborhood of \\(y^*\\), then it is locally attractive. Since the function is smooth, we can check its derivative: \\[\n|f'(y^*)| &lt; 1.\n\\]\nHere, \\(f'(y^*)=f'(0)=\\alpha\\), so \\(y^*\\) is attractive when \\(\\alpha &lt; 1\\).\n\nPythonMATLAB\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nf = lambda y,alpha: alpha*y*(1-y)\n\nN  = 31\ny  = np.zeros(N)\ny0 = 0.2\n\nfor alpha in [0.3,0.5,0.9]:\n  y[0] = y0\n  for i in range(N-1):\n    y[i+1] = f(y[i],alpha)\n  plt.plot(y,'.-',\n           linewidth=0.5,\n           label=f'$\\\\alpha = {alpha}$')\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nN = 40;\ny0 = 0.2;\ny  = zeros(1,N);\ny(1) = y0;\nfor a = [0.3,0.5,0.9]\n    for i=1:N-1\n        y(i+1) = a*y(i)*(1-y(i));\n    end\n    plot(0:N-1,y,'*-','LineWidth',2.0);\n    hold on;\nend\ngrid on;\nlegend('a=0.3','a=0.5','a=0.9');\n\n\n\n\n\n\n\n\nTry with \\(\\alpha\\in(1,3]\\). Now you should see a new equilibrium, while zero becomes unstable.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYes, zero becomes unstable and we have a new equilibrium \\[\ny^* = \\frac{\\alpha-1}{\\alpha},\n\\]\nwhich is stable since \\(|f'(y^*)| = |2-\\alpha| &lt; 1\\), so if and only if \\(\\alpha \\in (-1,3)\\). The trajectories are like the logistic curve.\nNote that for \\(\\alpha\\) close to 3 we start to observe oscillations.\n\nPythonMATLAB\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nf = lambda y,alpha: alpha*y*(1-y)\n\nN  = 31\ny  = np.zeros(N)\ny0 = 0.2\n\nfor alpha in [1.3,2.0,2.9]:\n  y[0] = y0\n  for i in range(N-1):\n    y[i+1] = f(y[i],alpha)\n  plt.plot(y,'.-',\n           linewidth=0.5,\n           label=f'$\\\\alpha = {alpha}$')\nplt.legend()\n\n\n\n\n\n\n\n\n\n\n%% Q2.2\nN = 40;\ny0 = 0.2;\ny  = zeros(1,N);\ny(1) = y0;\nfor a = [1.3,2.0,2.9]\n    for i=1:N-1\n        y(i+1) = a*y(i)*(1-y(i));\n    end\n    plot(0:N-1,y,'*-','LineWidth',2.0);\n    hold on;\nend\ngrid on;\nlegend('a=1.3','a=2.0','a=2.9');\n\n\n\n\n\n\n\n\nNow set \\(\\alpha\\in(3,1+\\sqrt{6})\\). A stable 2-cycle should appear, while the non-zero equilibrium is unstable. Note, a 2-cycle is a pair of points \\(\\{y_1,y_2\\}\\) such that \\(f(y_1) = y_2\\) and \\(f(y_2) = y_1\\), where \\(f(y) = \\alpha y(1-y)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe plot looks like:\n\nPythonMATLAB\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nf = lambda y,alpha: alpha*y*(1-y)\n\nN  = 31\ny  = np.zeros(N)\ny0 = 0.2\n\nalpha = 3.2\ny[0] = y0\nfor i in range(N-1):\n  y[i+1] = f(y[i],alpha)\nplt.plot(y,'.-',linewidth=0.5)\n\n\n\n\n\n\n\n\n\n\nN = 40;\ny0 = 0.2;\ny  = zeros(1,N);\ny(1) = y0;\na = 3.2;\nfor i=1:N-1\n    y(i+1) = a*y(i)*(1-y(i));\nend\nplot(0:N-1,y,'*-','LineWidth',2.0);\ntitle(sprintf('a=%g',a));\ngrid on;\n\n\n\n\nWe see that if \\(\\{y_1,y_2\\}\\) is a 2-cycle, then \\(f(f(y_1))=f^2(y_1)=y_1\\), so \\(y_1\\) (and also \\(y_2\\)) is a fixed point of \\(f^2\\). In particular, we have that: \\[\ny_{1,2} = \\frac{\\alpha+1\\pm\\sqrt{(1+\\alpha)(\\alpha-3)}}{2\\alpha},\n\\]\nwhich exist only for \\(\\alpha\\ge 3\\). For the stability, we require \\(|(f^2(y_{1,2}))'| = |f'(f(y_{1,2}))f'(y_{1,2})|&lt;1\\). We obtain \\(|f^2(y_{1,2})| = |4-(\\alpha-2)\\alpha|&lt;1\\) if only if \\(3 &lt; \\alpha &lt; 1+\\sqrt{6}\\).\n\n\n\n\nThe last part is to further increase \\(\\alpha\\) up to \\(4\\). We use the following Theorem (Fatou): for a fixed point iteration with \\(f(y)=ay^2+by+c\\) with a cycle, the point \\(-b/2a\\) is attracted by such orbit. So, we can fix the initial condition to \\(-b/2a\\), and then iterate until we reach the cycle. Try with various value of \\(\\alpha\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf we try with \\(\\alpha = 3.9\\) we obtain\n\n\nCode\nN  = 51\ny  = np.zeros(N)\ny0 = 0.5\n\nf = lambda y,alpha: alpha*y*(1-y)\n\nalpha = 3.9\ny[0] = y0\nfor i in range(N-1):\n  y[i+1] = f(y[i],alpha)\nplt.plot(y,'.-',linewidth=0.5)\n\n\n\n\n\nTrajectory of the discrete logistic equation for \\(\\alpha = 3.9\\).\n\n\n\n\nWe can visualize it with a web plot:\n\n\nCode\nN  = 51\ny  = np.zeros(N)\ny0 = 0.5\n\nf = lambda y,alpha: alpha*y*(1-y)\n\nalpha = 3.9\ny[0] = y0\nfor i in range(N-1):\n  y[i+1] = f(y[i],alpha)\n\nygrid = np.linspace(0,1,1000)\nplt.plot(ygrid,f(ygrid,alpha),'k')\nplt.plot(ygrid,ygrid,'k--',linewidth=1.0)\n\nzz = np.repeat(y,2)\nrr = np.roll(zz[2:],1)\nrr[0] = 0\nplt.plot(zz[:-2], rr, '-', color='gray', linewidth=0.5)\nplt.plot(zz[1:-2:2],rr[1::2],'.')\n\n\n\n\n\nWeb plot of the discrete logistic equation for \\(\\alpha = 3.9\\).\n\n\n\n\nThe dynamic is not periodic, it actually looks chaotic. In fact, for \\(\\alpha &gt; \\alpha_\\infty \\approx 3.5699...\\) we have deterministic chaos. Roughly speaking, there is a strong sensitivity to the initial datum, in the sense that if we start with \\(y^{(0)}\\) and \\(y^{(0)}+\\varepsilon\\), the two trajectories will become arbitrarily different after some iterations. This is not the case of an equilibrium or a \\(n\\)-cycle, because they are attractive for the orbits. We can see this by plotting the points visited by the map over the course of \\(1000\\) iterations:\n\n\nCode\nK  = 10_000\nN  =  1_000\ny  = np.zeros(N)\ny0 = 0.5\n\nalpha = 3.9\ny[0] = y0\n# burn-in\nfor _ in range(K):   y[0] = f(y[0],alpha)\n# iterations\nfor i in range(N-1): y[i+1] = f(y[i],alpha)\n\nfig, ax = plt.subplots()\nax.spines[[\"bottom\"]].set_position((\"data\", 0))\nax.spines[[\"top\", \"right\", \"left\"]].set_visible(False)\nax.xaxis.set_ticks(np.linspace(0,1,11))\nax.yaxis.set_ticks([])\n\nax.plot(y,np.zeros_like(y),'.',markersize=4)\n\nax.set_xlim((0,1))\nax.set_ylim((-1e-2,1e-2))\nfig.subplots_adjust(left=0, right=1, top=0.1, bottom=0.05)\n\n\n\n\n\nPoints visited by the map for \\(\\alpha = 3.9\\).\n\n\n\n\nWe see that this set is quite dense in \\([0,1]\\).\nChaos theory is fascinating, and the logistic map plays a very important role in it. It is a good model for chaos. For instance, if we slowly increase \\(\\alpha\\) and track equilibria and cycles we obtain the following diagram (very famous!):\n\n\nCode\nK  = 10_000\nN  =  1_000\ny  = np.zeros(N)\ny0 = 0.5\n\nfig, ax = plt.subplots(figsize=plt.figaspect(0.6))\nax.spines[[\"top\", \"right\", \"left\"]].set_visible(False)\nax.yaxis.set_ticks([])\nax.set_xlabel('$\\\\alpha$')\n\nfor alpha in np.linspace(2.5,4.0,1_000):\n  y[0] = y0\n  # burn-in\n  for _ in range(K):   y[0] = f(y[0],alpha)\n  # iterations\n  for i in range(N-1): y[i+1] = f(y[i],alpha)\n\n  ax.plot(alpha*np.ones_like(y),y,'k,',markersize=4)\n\n\n\n\n\nBifurcation plot for the logistic map.\n\n\n\n\nThis is a bifurcation diagram. For \\(\\alpha &lt; 3\\) we have a single equilibrium. At \\(\\alpha \\in [3,1+\\sqrt{6})\\) there is a 2-cycle, as seen above. This is visualized in the plot as 2 points. As we further increase \\(\\alpha &gt; 1+\\sqrt{6}\\), we have a 4-cycle, then 8-cycle, and so on with a \\(2^n\\)-cycle. This period doubling is the same we observed in the previous exercise in the Poincar√© map. The bifurcation points \\(a_n\\), with \\(a_1 = 3\\), \\(a_2 = 1+\\sqrt{6}\\), ‚Ä¶ converges to \\(a_\\infty\\), the barrier of chaos. On the other hand, their ratios define the Feigenbaum constant: \\[\n\\mathcal{F} = \\lim_{k\\to\\infty} \\frac{a_k-a_{k-1}}{a_{k+1}-a_k} \\approx 4.669\\,201\\,609\\,\\ldots\n\\]\nwhich is surprisingly independent from the choice of the iteration map! That is, we get this number for the logistic as well as for any other dynamical system \\(y^{(k+1)}=f(y^{(k)},\\alpha)\\) that exhibits a cascade of period doubling. This is why is an important constant.\nIn the chaotic region we also have island of ‚Äútranquility‚Äù, see below:\n\n\nCode\nK  = 10_000\nN  =  1_000\ny  = np.zeros(N)\ny0 = 0.5\n\nfig, ax = plt.subplots(figsize=plt.figaspect(0.6))\nax.spines[[\"top\", \"right\", \"left\"]].set_visible(False)\nax.yaxis.set_ticks([])\nax.set_xlabel('$\\\\alpha$')\n\nfor alpha in np.linspace(3.8,3.9,1_000):\n  y[0] = y0\n  # burn-in\n  for _ in range(K):   y[0] = f(y[0],alpha)\n  # iterations\n  for i in range(N-1): y[i+1] = f(y[i],alpha)\n\n  ax.plot(alpha*np.ones_like(y),y,'k,',markersize=4)\n\n\n\n\n\nBifurcation plot for the logistic map.\n\n\n\n\nFor instance we have a 3-cycle for \\(\\alpha &gt; 1+\\sqrt{8}\\). Orbits of period 3 are also important, because they imply chaos. (There is a famous article titled ‚ÄúPeriod three implies chaos‚Äù.) The order of appearance of \\(k\\)-cycles has a specific ordering called ‚ÄúSharkovsky ordering‚Äù.\nThe final note is the link to fractals. Well, first of all famous fractals like the Mandelbrot set are obtained by fixed point iterations (in the case of the Mandelbrot set, it is a map in the complex plane.) For the logistic, it is possible to show that in the chaotic regime the absorbing invariant set \\(\\mathfrak{I}\\subset[0,1]\\), that is such that \\(f(\\mathfrak{I})=\\mathfrak{I}\\) and attractive, has fractal dimension around \\(0.5\\) (the value depends on the way we define the fractal dimension.)\nThe MATLAB code for generating the bifurcation diagram is:\nN  = 1e4;\nNe = 1e2;\nY  = zeros(N,1);\ny0 = 0.5; % Fatou\n\nfigure; hold on;\nfor a = 2.5:0.001:3.8\n    % compute iterations\n    Y(1) = y0;\n    for i=1:N-1\n        Y(i+1) = a*Y(i)*(1-Y(i));\n    end\n    % get the equilibrium points\n    Yeq = uniquetol(Y(end-Ne:end));\n    plot(a,Yeq,'r.','MarkerSize',4.0);\n    hold on;\nend\nxlabel('a');\nhold off; grid on;\n\n\n\n\n\n\n\nHastings, Alan, and Thomas Powell. 1991. ‚ÄúChaos in a Three-Species Food Chain.‚Äù Ecology 72 (3): 896‚Äì903.",
    "crumbs": [
      "Labs",
      "Lab 04: Chaos"
    ]
  },
  {
    "objectID": "Laboratories/05_Feedbacks/lab_feedback.html",
    "href": "Laboratories/05_Feedbacks/lab_feedback.html",
    "title": "Lab 05: Feedback loops",
    "section": "",
    "text": "Circadian Clock\nThe circadian clock is an internal physiological mechanism that shows oscillations of 24 hours. The existence of such internal cycle is apparent when we travel or right after daylight saving time switch.\nAn established mechanism for producing limit cycles in molecular networks is negative feedback. For instance, we can have an enzyme \\(X_1\\) that produces \\(X_2\\) with some rate, then \\(X_2\\) produces \\(X_3\\), and so on until \\(X_n\\), but then \\(X_n\\) has a negative influence of \\(X_1\\). The chain of reaction is necessary to create a delay.\nWe consider here a model for the Drosophila. We have the protein PER (for period), denoted by \\(P_0\\), that is phosphorylated into \\(P_1\\) and then \\(P_2\\): \\[\n\\ce{P_0 &lt;=&gt; P_1 &lt;=&gt; P_2}.\n\\]\nThe protein \\(P_2\\) is then transported into the nucleus, where it inhibits the production of PER mRNA (and thus the protein PER).\nAfter a few steps and simplifications, Tyson et al. (1999) showed that the model can be reduced to \\[\n\\left\\{\\begin{aligned}\nM' &= \\frac{\\nu_m}{1+\\left(\\frac{P(1-q)}{2P_\\mathrm{crit}}\\right)^2} - k_m M, \\\\\nP' &= \\nu_p M - \\frac{k_{p_1}P q + k_{p_2}P}{J_p + P} - k_{p_3} P, \\\\\nq &= \\frac{2}{1+\\sqrt{1 + 8 K_\\mathrm{eq} P}},\n\\end{aligned}\\right.\n\\]\nwhere \\(P\\) is the total PER protein and \\(M\\) is the mRNA concentration. The parameters are:",
    "crumbs": [
      "Labs",
      "Lab 05: Feedback loops"
    ]
  },
  {
    "objectID": "Laboratories/05_Feedbacks/lab_feedback.html#circadian-clock",
    "href": "Laboratories/05_Feedbacks/lab_feedback.html#circadian-clock",
    "title": "Lab 05: Feedback loops",
    "section": "",
    "text": "Parameter\nValue\n\n\n\n\n\\(\\nu_m\\)\n1\n\n\n\\(k_m\\)\n0.1\n\n\n\\(\\nu_p\\)\n0.5\n\n\n\\(k_{p_1}\\)\n10\n\n\n\\(k_{p_2}\\)\n0.03\n\n\n\\(k_{p_3}\\)\n0.1\n\n\n\\(K_\\mathrm{eq}\\)\n200\n\n\n\\(P_\\mathrm{crit}\\)\n0.1\n\n\n\\(J_p\\)\n0.05\n\n\n\n\nImplement the model in MatCont and simulate an orbit. (The time unit is hours.) The initial condition is \\(M(0)=0.1\\) and \\(P(0)=0.1\\). You can copy and paste this code for the equations:\nq=(2/(1+sqrt(1+8*Keq*PP)))\nMM'=vm/(1+( PP*(1-q)/(2*Pcrit) )^2) - km*MM\nPP'=vp*MM-(kp1*PP*q+kp2*PP)/(Jp+PP)-kp3*PP \n\n\nManually find the period. To do so, start from any point on the cycle and then integrate for enough time so to close the cycle. To get the exact value, do Select-&gt;Initial Point and select LC Select Cycle. It will ask for tolerance and number of discretization points.\n\n\nA limit cycle should contain an (unstable) equilibrium or another unstable limit cycle (why?) Integrate the system backward in time to approximate it.\n\n\nContinue the unstable equilibrium with respect to \\(k_{p_1}\\), both backward and forward. Report all bifurcations.\n\n\nContinue the first Hopf point with respect to both \\(k_{p_1}\\) and \\(K_\\mathrm{eq}\\), and find a region in the \\((k_{p_1},K_\\mathrm{eq})\\) plane that allows for a limit cycle to exist. Hint: if it is too slow, increase Maxstepsize in the Continuer.\n\n\nGo back to the original Hopf point, and now continue it with respect to the period and \\(k_{p_1}\\). Plot how the period varies with respect to \\(k_{p_1}\\).\n\n\nSelect the original LC of 24 hours. Then, try to continue it with respect to \\(k_{p_1}\\) and \\(K_\\mathrm{eq}\\). The curve should represent all possible combinations of the parameters giving a cycle of period 24 hours. Notice that you will see some limit point of cycles (LPC).",
    "crumbs": [
      "Labs",
      "Lab 05: Feedback loops"
    ]
  },
  {
    "objectID": "Laboratories/05_Feedbacks/lab_feedback.html#activator-inhibitor-oscillations",
    "href": "Laboratories/05_Feedbacks/lab_feedback.html#activator-inhibitor-oscillations",
    "title": "Lab 05: Feedback loops",
    "section": "Activator-inhibitor oscillations",
    "text": "Activator-inhibitor oscillations\nConsider the following network loop from Tyson, Chen, and Novak (2003):\n\n(The activation for the enzyme \\(\\ce{E}\\) are of Michaelis-Menten type.)\nThe corresponding system of equations is: \\[\n\\begin{aligned}{}\n[\\ce{R}]' &= k_0 [\\ce{E_p}] + k_1[\\ce{S}] - k_2[\\ce{R}]-k_2'[\\ce{X}][\\ce{R}], \\\\\n[\\ce{X}]' &= k_5 [\\ce{R}] - k_6[\\ce{X}], \\\\\n[\\ce{E_p}]' &= \\frac{k_3[\\ce{R}](1-[\\ce{E_p}])}{J_3 + 1-[\\ce{E_p}]} - \\frac{k_4[\\ce{E_p}]}{J_4 + [\\ce{E_p}]}.\n\\end{aligned}\n\\]\n\nSimulate an orbit for \\([\\ce{R}](0)=[\\ce{X}](0)=[\\ce{E_p}](0)=0.1\\) and parameters:\n\n\n\n\nParameter\nValue\n\n\n\n\n\\([\\ce{S}]\\)\n0.2\n\n\n\\(k_0\\)\n4\n\n\n\\(k_1\\), \\(k_2\\), \\(k_2'\\), \\(k_3\\), \\(k_4\\)\n1\n\n\n\\(k_5\\)\n0.1\n\n\n\\(k_6\\)\n0.075\n\n\n\\(J_3\\), \\(J_4\\)\n0.3\n\n\n\n\nTry to approximate the period of the limit cycle.\n\n\nFor larger \\([\\ce{S}]\\), say \\([\\ce{S}] = 0.5\\), there is no limit cycle. Approximate the equilibrium (it should be a spiral), then continue the curve backward with respect to \\([\\ce{S}]\\). Stop at \\([\\ce{S}]\\approx 0\\). Do you observe bifurcations?\n\n\nSelect the Hopf bifurcation, and continue the limit cycle with respect to \\([\\ce{S}]\\). You should obtain a limit point of cycle. Find the corresponding value of it.\n\n\nPlot in 3D the limit cycle family with respect to \\([\\ce{S}]\\). Notice that for a range of \\({\\ce[S]}\\) (which one?) the system is bistable. The leftmost Hopf is subcritical, whereas the other one is supercrical. Which one is a catastrophic bifurcation?\n\n\n\n\n\nTyson, John J, Katherine C Chen, and Bela Novak. 2003. ‚ÄúSniffers, Buzzers, Toggles and Blinkers: Dynamics of Regulatory and Signaling Pathways in the Cell.‚Äù Current Opinion in Cell Biology 15 (2): 221‚Äì31.\n\n\nTyson, John J, Christian I Hong, C Dennis Thron, and Bela Novak. 1999. ‚ÄúA Simple Model of Circadian Rhythms Based on Dimerization and Proteolysis of PER and TIM.‚Äù Biophysical Journal 77 (5): 2411‚Äì17.",
    "crumbs": [
      "Labs",
      "Lab 05: Feedback loops"
    ]
  },
  {
    "objectID": "IntroODE.html",
    "href": "IntroODE.html",
    "title": "Appendix A ‚Äî Introduction to ODEs",
    "section": "",
    "text": "A.1 Basic definitions\nLet us start with some simple definitions about Ordinary Differential Equations (ODEs).\nLet‚Äôs have a look to some of the equations we are going to deal with during this course. We will analyze the equations for a purely mathematical point of view, with no biological insights: this will come later.\nIn general, an ODE does not tell anything about a specific solution; it is just the law of motion. We can have many solutions. As a rule of thumb, an \\(n\\)-order scalar ODE requires \\(n\\) constants for uniquely defining a solution. An Initial Value Problem (IVP) is an ODE supplemented wuth additional conditions. It is called ‚Äúinitial condition‚Äù because it sets the value of the solution at some initial time \\(t_0\\). (Please note that an IVP is also called ‚ÄúCauchy problem‚Äù on some books.)\nIn all the above cases, the ODEs were scalar. A system of ODEs is the natural generalization to \\(\\mathbb{R}^n\\). We will only consider first-order system of ODEs, because their extension to higher-order ODEs is superfluous (we will see why in a moment.)\nConsider the system of first-order ODEs: \\[\n\\left\\{\\begin{aligned}\ny_1' &= f_1(t,y_1,y_2,\\ldots,y_n), \\\\\ny_2' &= f_2(t,y_1,y_2,\\ldots,y_n), \\\\\n& \\cdots  \\\\\ny_n' &= f_n(t,y_1,y_2,\\ldots,y_n). \\\\\n\\end{aligned}\\right.\n\\]\nWe set \\(\\boldsymbol{y}(t) = (y_1,\\ldots,y_n)^\\intercal\\) and \\(\\boldsymbol{f}(t,\\boldsymbol{y})=(f_1,\\ldots,f_n)^\\intercal\\) so the system rewrites to \\[\n\\boldsymbol{y}' = \\boldsymbol{f}(t,\\mathbf{y}).\n\\]\n(Please note that the time derivative of the vector is the vector of the derivatives.) In compact form, the IVP reads as: \\[\n\\left\\{\\begin{aligned}\n\\boldsymbol{y}' = \\boldsymbol{f}(t,\\mathbf{y}), \\\\\n\\boldsymbol{y}(t_0) = \\boldsymbol{y}_0. \\\\\n\\end{aligned}\\right.\n\\tag{A.1}\\]\nThe solution \\(\\boldsymbol{\\phi}\\in\\mathcal{C}^1(I,\\mathbb{R}^n)\\) of the system is a curve in \\(\\mathbb{R}^n\\) such that: \\[\n\\boldsymbol{\\phi}'(t) = \\boldsymbol{f}(t,\\boldsymbol{\\phi}(t)),\\quad\\text{for all $t\\in I$}.\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Introduction to ODEs</span>"
    ]
  },
  {
    "objectID": "IntroODE.html#basic-definitions",
    "href": "IntroODE.html#basic-definitions",
    "title": "Appendix A ‚Äî Introduction to ODEs",
    "section": "",
    "text": "Definition A.1 (ODE) An scalar ODE is an equation (generally nonlinear) involving a function \\(y(t)\\) and its derivatives. Let \\(f\\colon D\\to\\mathbb{R}\\), \\(D\\subseteq\\mathbb{R}^{n+1}\\) open set not empty. Then an ODE of order \\(n\\) is an equation of the type: \\[y^{(n)} = f(t,y,y',\\ldots,y^{(n-1)}).\\]\n\n\n\n\n\n\n\n\nExample: Malthusian growth model\n\n\n\n\\[N'(t) = r\\,N(t).\\]\nThis is a simple model of population growth where \\(N(t)\\) represents the population size at time \\(t\\). We have that \\(f(t,N) = rN\\). It is a linear ODE (\\(f\\) is linear with respect to \\(N\\)), with constant coefficients (\\(r\\) is a constant), first-order, and autonomous (\\(f\\) does not explictly depend on \\(t\\)).\n\n\n\n\n\n\n\n\nExample: Logistic Growth Model\n\n\n\n\\[\nN'(t) = r N\\left(1 - \\frac{N}{K}\\right)\n\\] The logistic model modifies the Malthusian growth by introducing a carrying capacity. We have that \\(f(t,N) = rN(1-N/K)\\). It is a nonlinear ODE (\\(f\\) is non-linear with respect to \\(N\\)), first-order, and autonomous.\n\n\n\n\n\n\n\n\nExample: Damped Harmonic Oscillator\n\n\n\n\\[\ny''(t) + 2\\beta y'(t) + \\omega_0^2 y(t) = F(t).\n\\] This is a second-order ODE, linear, constant coefficients, non-autonomous (because of the presence of \\(F(t)\\)).\n\n\n\n\nDefinition A.2 (Initial Value Problem) An initial value problem (IVP) is a system of the form: \\[\n\\left\\{\\begin{aligned}\ny^{(n)} &= f(t,y,y',\\ldots,y^{(n-1)}), &\\\\\ny(t_0) &= y_0, \\\\\ny'(t_0) &= y_1, \\\\\n&\\vdots \\\\\ny^{(n-1)}(t_0) &= y_{n-1}. \\\\\n\\end{aligned}\\right.\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Introduction to ODEs</span>"
    ]
  },
  {
    "objectID": "IntroODE.html#separation-of-variables",
    "href": "IntroODE.html#separation-of-variables",
    "title": "Appendix A ‚Äî Introduction to ODEs",
    "section": "A.2 Separation of variables",
    "text": "A.2 Separation of variables\nSolving ODE analytically is not always possible. As for integration, sometimes we cannot find a solution in closed form. However, we have alternatives to rescue us:\n\nQualitative analysis: we don‚Äôt solve the ODE explicitly, we rather analyse the behaviour of the trajectories as we would do for a function study. Very often we proceed graphically (in 1-D and 2-D). It is very helpful for parametric studies.\nNumerical integration: we solve the IVP on a computer. We have no limits, but we always need to provide all parameters and initial values. Parametric studies are more difficult.\n\nA combination of the above approaches in always wise. On the other hand, it is good to know how to integrate an ODE when it is possible.\nA simple yet effective technique to solve IVPs is the method of separation of variables. It applies to IVPs of the form: \\[\n\\begin{cases}\ny' = f(t)g(y), \\\\\ny'(t_0) = y_0.\n\\end{cases}\n\\]\nfor some functions \\(f\\) and \\(g\\). Using the Leibniz notation, we write the ODE as follows: \\[\n\\newcommand{\\dd}{\\mathrm{d}}\n\\frac{\\dd y}{\\dd t} = f(t)g(y),\n\\]\nwhich suggests to separate the variables \\(t\\) and \\(y\\): \\[\n\\newcommand{\\dd}{\\mathrm{d}}\n\\frac{\\dd y}{g(y)} = f(t)\\,\\dd t.\n\\]\nHere we consider \\(y\\) and \\(t\\) as independent variables. The advantage is that we can integrate both sides to get rid of differentials: \\[\n\\newcommand{\\dd}{\\mathrm{d}}\n\\int_{y_0}^y \\frac{\\dd \\tilde y}{g(\\tilde y)} = \\int_{t_0}^t f(\\tilde t)\\,\\dd \\tilde t.\n\\]\nPlease note that amongst all anti-derivatives, we select those satisfying the initial condition \\(y(t_0)=y_0\\). Suppose that \\(F(t)\\) and \\(G(y)\\) are respectively anti-derivatives of \\(f(t)\\) and \\(\\frac{1}{g(y)}\\), i.e.¬†\\(F'(t)=f(t)\\) and \\(G'(y)=\\frac{1}{g(y)}\\). Then, according to the Fundamental Theorem of Calculus, we have: \\[\nG(y) - G(y_0) = F(t) - F(t_0),\n\\]\nwhich gives an implicit expression for \\(y(t)\\): \\[\nG\\bigl(y(t)\\bigr) = F(t) + G(y_0) - F(t_0).\n\\]\nPlease note that the expression on the right hand side only contains the variable \\(t\\). When \\(G\\) is invertible, the expression of \\(y(t)\\) can be made explicit.\n\n\n\n\n\n\nExample (Malthusian growth model)\n\n\n\nWe aim at solving the following IVP: \\[\n\\begin{cases}\nN' = r N, \\\\\nN(t_0) = N_0.\n\\end{cases}\n\\tag{A.2}\\]\nWe apply the method of separation of variables. We have: \\[\n\\newcommand{\\dd}{\\mathrm{d}}\n\\int_{N_0}^N \\frac{\\dd\\tilde N}{\\tilde N} = \\int_{t_0}^t r\\,\\dd\\tilde{t}.\n\\]\nAfter integration (recall that \\(\\ln'(y) = 1/y\\)): \\[\n\\ln\\Bigl(\\tfrac{N}{N_0}\\Bigr) = r (t-t_0),\n\\]\nor, by inverting the logarithm, \\[\nN(t) = N_0 e^{r (t-t_0)}.\n\\]\n\n\n\n\n\n\n\n\nExercise (Time-dependent Malthus equation)\n\n\n\nCheck that \\(N(t) = N_0 e^{r (t-t_0)}\\) solves the problem (Equation¬†A.2).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Introduction to ODEs</span>"
    ]
  },
  {
    "objectID": "IntroODE.html#well-posedness",
    "href": "IntroODE.html#well-posedness",
    "title": "Appendix A ‚Äî Introduction to ODEs",
    "section": "A.3 Well-posedness",
    "text": "A.3 Well-posedness\nLet us recall this (simplistic) view of mathematical modeling:\n\n\n\n\n\nflowchart LR\n    id1[(Physical world)] ---|Approximation| id2[(Mathematical model)] ---|Approximation| id3[(Numerical model)]\n\n\n\n\n\n\nA good mathematical model shall approximate well the physical world. For instance, it is able to fit experimental data in a validation test. However, a validation test requires that we are able to solve the mathematical problem. For that, we require that the problem should satisfy well-posedness properties.\nWe are dealing with IVPs. Here, well-posedness translates into:\n\nExistence of a solution \\(\\boldsymbol{\\phi} \\in \\mathcal{C}^1([0,T],\\mathbb{R}^n)\\) for every choice of the initial data;\nUniqueness of the solution, in the sense that if \\(\\boldsymbol{\\phi}_1(t)\\) and \\(\\boldsymbol{\\phi}_2(t)\\) are both solutions of the IVP for the same initial data, then \\(\\boldsymbol{\\phi}_1(t)=\\boldsymbol{\\phi}_2(t)\\) for all \\(t\\).\nStability of the solution to perturbations, that is if \\(\\tilde{\\boldsymbol{\\phi}}(t)\\) is the solution of the IVP with some perturbation, e.g., applied to the initial datum or to the right hand side, then \\(\\|\\tilde{\\boldsymbol{\\phi}}-\\boldsymbol{\\phi}\\|\\to 0\\) as the perturbation goes to zero.\n\nFor IVPs, stability to perturbation is also called zero stability. The zero stability for initial data follows from the Cauchy-Lipschitz theorem, as shown below.\nFor the numerical realm, we have a similar definition of well-posedness. Additionally, we typically need to show that the numerical solution converges (in some sense) to the analytical solution. That is, the approximation error goes to zero as we refine the numerical problem. For an IVP, convergence means that that the error between the numerical solution and the true solution goes to zero as the time step goes to zero.\nWe start with a definition of a solution for the IVP.\n\nDefinition A.3 (Classic solution) A classic solution of the IVP (Equation¬†A.1) is a function \\(\\boldsymbol{\\phi}\\in\\mathcal{C}^1(I;\\mathbb{R}^n)\\), where \\(I\\subset\\mathbb{R}\\) is a closed interval, such that\n\n\\(\\boldsymbol{\\phi}'(t) = \\boldsymbol{f}(t,\\boldsymbol{\\phi}(t))\\) for all \\(t\\in I\\);\n\\(\\boldsymbol{\\phi}(t_0) = \\boldsymbol{y}_0\\);\n\\((t,\\boldsymbol{\\phi}(t))\\subset D\\) for all \\(t\\in I\\), where \\(D\\subset\\mathbb{R}^{n+1}\\) is the domain of \\(\\boldsymbol{f}\\).\n\n\nThe first 2 conditions are simply the IVP (equation and initial condition). The third condition concern the domain of definition of the right hand side of the ODE.\n\n\n\n\n\n\nExample (lack of existence)\n\n\n\nWhen the \\(\\boldsymbol{f}\\) is not continuous, we cannot expect existence in general. In fact, consider the IVP \\[\n\\begin{cases} y' = \\mathcal{H}(t) = \\begin{cases} 0, & t\\le 0,\\\\ 1, & t &gt; 0,\\end{cases} \\\\\ny(0) = y_0.\n\\end{cases}\n\\]\nA possible and intuitive solution is \\[\n\\phi(t) = y_0 + \\max\\{0, t\\} = \\begin{cases} y_0, & t\\le 0,\\\\ y_0 + t, & t&gt;0.\\end{cases}\n\\]\nThe function \\(\\phi(t)\\) is, however, not \\(\\mathcal{C}^1(I)\\) for any neighborhood \\(I\\) of the initial time \\(t_0=0\\). Therefore, the function cannot be a solution, at least according to the above definition of solution. Note that with a different initial condition, say \\(y(1) = y_0\\), we can find a \\(\\mathcal{C}^1(I)\\) solution, for \\(I\\) away from \\(t=0\\).\n\n\n\nA.3.1 Local well-posedness\nA general result for the local existence of a solution is due to Giuseppe Peano:\n\nTheorem A.1 (Peano) Let \\(\\mathbf{f}\\colon D \\to\\mathbb{R}^n\\), \\(D\\subseteq\\mathbb{R}^{n+1}\\) open set, be continuous. Then, for all \\((t_0,\\mathbf{y}_0)\\in D\\), there exists a neighborhood of \\(t_0\\), denoted by \\(I_\\delta = [t_0-\\delta,t_0+\\delta]\\) with \\(\\delta &gt; 0\\), on which is possible to construct a solution \\(\\boldsymbol{\\phi}\\in\\mathcal{C}^1( I_\\delta; \\mathbb{R}^n)\\) to the IVP.\n\nPeano‚Äôs theorem ensures the existence but not the uniqueness. He also provided a simple counterexample to uniqueness.\n\n\n\n\n\n\nPeano‚Äôs counterexample on uniqueness\n\n\n\nConsider the ODE \\[\n\\begin{cases}\ny' = \\sqrt[3]{y}, \\\\\ny(1) = 0.\n\\end{cases}\n\\]\nThe function \\(f(t,y) = \\sqrt[3]{y}\\) is continuous with respect to \\(t\\) and \\(y\\) (note that it does not explicitly depends on time), thus Peano‚Äôs Theorem ensures the existence of at least one solution.\nWe can find a solution by separation of variables. We have (please check!): \\[\n\\phi_1(t) = \\begin{cases}\n\\bigl(\\tfrac{2}{3} (t-1)\\bigr)^{3/2}, & t \\ge 1, \\\\\n0, & t &lt; 1.\n\\end{cases}\n\\]\nThis solution is however not the only one. Also the following function is a solution (again, please check!) \\[\n\\phi_2(t) = \\begin{cases}\n-\\bigl(\\tfrac{2}{3} (t-1)\\bigr)^{3/2}, & t \\ge 1, \\\\\n0, & t &lt; 1.\n\\end{cases}\n\\]\nYet another solution is \\(\\phi_3(t)\\equiv 0\\). Actually, the IVP is so subtle that we can build an infinite number of solutions with the power of continuum! In fact, for any \\(\\alpha\\ge 1\\), we have that \\[\n\\phi_{\\alpha^\\pm}(t) = \\begin{cases}\n\\pm\\bigl(\\tfrac{2}{3} (t-\\alpha)\\bigr)^{3/2}, & t \\ge \\alpha, \\\\\n0, & t &lt; \\alpha,\n\\end{cases}\n\\]\nare all solutions of the IVP. That‚Äôs a lot to deal with.\n\n\n\nPeano‚Äôs brush\n\n\n\n\nThe example shows that the lack of uniqueness is somehow related to a lack of stability of the solution. Consider a particle located, at \\(t=1\\), in \\(y=0\\). The velocity at some generic time \\(t\\) is \\(y'(t) = \\sqrt[3]{y(t)}\\). Then, the trajectory of the particle is exactly the solution of Peano‚Äôs example.\nThe example shows that we need some control on the growth rate of \\(\\boldsymbol{f}\\). The correct concept is the Lipschitz continuity:\n\nDefinition A.4 (Lipschitz continuity) Let \\(\\boldsymbol{f}\\colon \\Omega \\to \\mathbb{R}^n\\), \\(\\Omega\\subseteq\\mathbb{R}^n\\), a continuous function. We say that \\(\\boldsymbol{f}\\) is Lipschitz continuous if there exists a constant \\(L&gt;0\\) such that \\[\n\\| \\boldsymbol{f}(\\boldsymbol{y}) - \\boldsymbol{f}(\\boldsymbol{z}) \\| \\le L\\|\\boldsymbol{y}-\\boldsymbol{z}\\|,\n\\]\nfor all \\(\\boldsymbol{y},\\boldsymbol{z}\\in\\Omega\\).\n\n\nDefinition A.5 (Local Lipschitz continuity) Let \\(\\mathbf{f}\\colon \\Omega \\to \\mathbb{R}^n\\) a continuous function, with \\(\\Omega\\subseteq\\mathbb{R}^n\\) open domain. We say that \\(\\boldsymbol{f}\\) is locally Lipschitz continuous in \\(\\Omega\\) if for every \\(\\boldsymbol{y}_0\\in\\Omega\\) there exists a neighborhood of \\(\\boldsymbol{y}_0\\) in which \\(\\boldsymbol{f}\\) is Lipschitz continuous, with a constant \\(L\\) possibly depending on the neighborhood.\n\n\n\n\n\n\n\nExercise\n\n\n\nShow that \\(\\mathcal{C}^1\\) functions are always locally Lipchitz. For simplicity consider functions on \\(\\mathbb{R}\\).\n\n\n\nTheorem A.2 (Cauchy-Lipschitz) Let \\(\\boldsymbol{f}\\in\\mathcal{C}(D;\\mathbb{R}^n)\\), \\(D\\subseteq\\mathbb{R}^{n+1}\\) open domain. If \\(\\boldsymbol{f}\\) is locally Lipschitz in \\(D\\), with respect to \\(\\boldsymbol{y}\\) and uniformly in \\(t\\), then for every \\((t_0,\\boldsymbol{y}_0)\\in D\\) there exists an interval \\(I_\\delta = [t_0-\\delta,t_0+\\delta]\\), with \\(\\delta&gt;0\\), on which is possible to construct a solution \\(\\boldsymbol{\\phi}\\in\\mathcal{C}^1( I_\\delta; \\mathbb{R}^n)\\) to the IVP (Equation¬†A.1). Such solution is also unique, in the sense that every other solution of the IVP coincides with \\(\\boldsymbol{\\phi}(t)\\) on \\(I_\\delta\\).\n\nThe Cauchy-Lipschitz theorem has very important consequences on the study of ODEs. We mention here the following\n\nCorollary A.1 (Zero stability) Consider the following IVPs: \\[\n\\left\\{\\begin{aligned}\n\\boldsymbol{y}' &= \\boldsymbol{f}(t,\\mathbf{y}), \\\\\n\\boldsymbol{y}(t_0) &= \\boldsymbol{y}_0, \\\\\n\\end{aligned}\\right. \\quad\\text{and}\\quad\n\\left\\{\\begin{aligned}\n\\tilde{\\boldsymbol{y}}' &= \\boldsymbol{f}(t,\\tilde{\\mathbf{y}}), \\\\\n\\tilde{\\boldsymbol{y}}(\\tilde{t}_0) &= \\tilde{\\boldsymbol{y}}_0. \\\\\n\\end{aligned}\\right.\n\\]\nSuppose we are under the hypotheses of the Cauchy-Lipschitz theorem. Then, we have the following stability estimate: \\[\n\\| \\tilde{\\boldsymbol{y}}(t) - \\boldsymbol{y}(t) \\| \\le M |\\tilde{t}_0 - t_0| + e^{L|t-t_0|} \\| \\tilde{\\boldsymbol{y}}_0 - \\boldsymbol{y}_0 \\|,\n\\] where \\(L\\) is the Lipschitz constant of \\(\\boldsymbol{f}\\).\n\nIn other words, if we perturb the initial value of the IVP, the perturbed solution remains close to the unperturbed solution as long as the perturbation is small and small time. For large time, however, the initial perturbation may grow exponentially, with a rate proportional to the Lipschitz constant.\n\n\nA.3.2 Global well-posedness\nThe solution provided by the Cauchy-Lipschitz Theorem is only defined in a local interval, that is up to \\(t=t_0+\\delta\\), with \\(\\delta &gt; 0\\). Unfortunately, \\(\\delta\\) can be small, thus there is no guarantee that we can integrate the IVP for an arbitrarily long time.\nWe can try to circumvent the problem as follows. Let us call \\(\\boldsymbol{\\phi}_1(t)\\) the solution and \\(\\delta_1 = \\delta\\). Now we set up a new problem of the form: \\[\n\\begin{cases}\n\\boldsymbol{y}' = \\boldsymbol{f}(t,\\boldsymbol{y}), & \\\\\n\\boldsymbol{y}(t_1) = \\boldsymbol{y}_1,  & \\\\\n\\end{cases}\n\\]\nwhere \\(t_1 = t_0 + \\delta_1\\) and \\(\\boldsymbol{y}_1 = \\boldsymbol{\\phi}_1(t_1)\\). Since also the above problem is well-posed, we get a new solution \\(\\boldsymbol{\\phi}_2(t)\\) up to \\(t_2=t_1+\\delta_2\\), for some \\(\\delta_2&gt;0\\).\nSo, we can define a new solution on the interval \\([t_0,t_0 + \\delta_1+\\delta_2]\\) by gluing the solutions around \\(t_1\\): \\[\n\\boldsymbol{\\phi}(t) = \\begin{cases}\n\\boldsymbol{\\phi}_1(t), & t \\in [t_0,t_0+\\delta_1], \\\\\n\\boldsymbol{\\phi}_2(t), & t \\in [t_0+\\delta_1,t_0+\\delta_1+\\delta_2].\n\\end{cases}\n\\]\nPlease note that \\(\\boldsymbol{\\phi}_1(t_1)=\\boldsymbol{\\phi}_2(t_1)\\) by construction, and the same applies to the derivative, so the full solution is still \\(\\mathcal{C}^1([t_0, t_0+\\delta_1+\\delta_2])\\).\nBy iterating the process, we obtain a sequence \\(\\{ \\delta_n \\}\\) of extensions. The hope is that \\(\\sum_n \\delta_n \\to \\infty\\), that is we can extend the solution indefinitely. However, it is certainly possible that \\(t_0 + \\sum_n \\delta_n \\to T_\\text{max} &lt; \\infty\\).\n\nDefinition A.6 (Maximal interval) The maximal right interval \\([t_0,T_\\text{max})\\) is simply the maximum time \\(T_\\text{max}\\) up to which we can extend the solution to the right. In general the interval is open. Similarly, the left maximal interval is \\((T_\\text{min},t_0]\\). Finally, the maximal interval is just \\((T_\\text{min},T_\\text{max})\\). We denote the maximal interval of the solution with \\(J_\\phi\\).\n\n\n\n\n\n\n\nExample (blow-up of solutions)\n\n\n\nConsider the problem \\[\n\\begin{cases} y' = y^2, \\\\ y(0) = 1.\\end{cases}\n\\]\nWe solve it by separation of variables. So, \\[\n\\begin{aligned}\n& \\int_1^y \\frac{\\mathrm{d}\\tilde{y}}{\\tilde{y}^2} = \\int_0^t \\mathrm{d}\\tilde{t} \\\\\n\\Rightarrow\\; & \\left[ -\\frac{1}{\\tilde{y}} \\right]_1^y = t \\\\\n\\Rightarrow\\; & -\\frac{1}{y} + 1 = t \\\\\n\\Rightarrow\\; & y(t) = \\frac{1}{1-t}.\n\\end{aligned}\n\\]\nWe know that the problem is locally well-posed, because \\(f(t,y) = y^2\\) is continuous and locally Lipschitz. Thus, the above solution is locally unique. However, the interval of integration cannot be arbitrarily large: starting at \\(t=0\\), we can go forward in time only up to \\(t=1-\\varepsilon\\), with \\(\\varepsilon &gt; 0\\). That is, we cannot reach \\(t=1\\). The reason is clear: the solution exhibit a vertical asymptote at \\(t=1\\), thus the solution blows up. In mathematics, this event is indeed called blow-up in finite time of the solution.\nConversely, backward integration exhibit no problem. Therefore, the maximal interval of well-posedness is \\(J_\\phi = (-\\infty,1)\\).\n\n\nComputing the maximal interval for a solution by hand is clearly not practical. We would like to estimate the interval without having to solve the problem analytically.\n\nTheorem A.3 Let \\(\\|\\boldsymbol{\\phi}(t)\\|\\le M\\) for all \\(t\\in(a,b)\\). Then \\(J_\\phi=(a,b)\\).\n\nIn the theorem, we can also take \\(a=-\\infty\\) and \\(b=+\\infty\\). In this case the maximal interval would be \\(\\mathbb{R}\\).\nIf the right hand side of the IVP grows almost linearly, then we have global existence as well. Indeed, the following theorem holds.\n\nTheorem A.4 Let \\(\\boldsymbol{f}\\colon (a,b)\\times\\mathbb{R}^n\\to\\mathbb{R}^n\\). Suppose we are under the assumptions of the Cauchy-Lipschitz theorem. If there exist 2 non-negative constants \\(k_1\\), \\(k_2\\) such that\n\\[\n\\| \\boldsymbol{f}(t,\\boldsymbol{y}) \\| \\le k_1 + k_2 \\| \\boldsymbol{y} \\|\n\\]\nfor every \\((t,\\boldsymbol{y})\\in [a,b] \\times \\mathbb{R}^n\\), then for all \\((t_0,\\boldsymbol{y}_0)\\) the unique solution \\(\\boldsymbol{\\phi}(t)\\) is defined in \\([a,b]\\).\n\nIn general, we use a priori information on the solution to apply the above Theorems. For that, we may use energy estimates (common for problems in physics), or we can study the qualitative behavior of the solutions for various initial conditions, e.g., in the phase space. Let us try that with a simple example.\n\n\n\n\n\n\nExample (logistic equation)\n\n\n\nConsider the ODE \\[\ny' = y(1-y),\n\\]\nwhere \\(y(t)\\) may represent, for instance, the density of a population. We would like to show that \\(J = \\mathbb{R}\\) for every choice of the initial data \\(y(0)=y_0\\). Note that the right hand side \\(f(t,y)=y(1-y)\\) is quadratic: we cannot exclude a blow up as in the above example!\nWe have the following cases:\n\nif \\(y_0 = 0\\) or \\(y_0 = 1\\), the solution is constant in time, \\(y(t)=y_0\\). In fact, \\(f(t,y_0) = 0\\), thus \\(y'(t)=0\\) and we have a constant solution. We are going to call this type of solutions fixed points or equilibria (see the next section.)\nif \\(y_0 \\in (0,1)\\), then \\(y(t) \\to 1^-\\) as \\(t\\to+\\infty\\) and \\(y(t) \\to 0^+\\) as \\(t\\to-\\infty\\). Thus, \\(y(t)\\in(0,1)\\) for all \\(t\\). The limit follows from an analysis of the sign of \\(f(t,y)\\). For \\(y\\in(0,1)\\), \\(f(t,y) &gt; 0\\), thus \\(y' &gt; 0\\) for all \\(t\\) and \\(y(t)\\) is monotonically increasing. But \\(y = 1\\) is an equilibrium that cannot be crossed, and there are no other equilibria in \\((0,1)\\). Thus, it must be that \\(y\\to 1\\) from below as \\(t\\to\\infty\\).\nif \\(y_0 &gt; 1\\), then \\(y(t)\\to 1^+\\) as \\(t\\to+\\infty\\). Again, the statement follows from \\(f(t,y)&lt;0\\) for \\(y &gt; 1\\), thus \\(y'&lt;0\\) and \\(y(t)\\) is monotonically decreasing. Since it is bounded from below by \\(y(t)=1\\), we have \\(y(t)\\to 1\\). However, for \\(t\\to-\\infty\\) we have a blow up in finite time, specifically for \\(t = \\bar{t} &lt; 0\\) where (show this by explicit integration of the IVP!) \\[\n\\bar{t} = \\ln(1-y_0) - \\ln(y_0).\n\\]\n\nWe conclude that the solution \\(y(t)\\) is bounded in \\([y_0,0]\\) for any choice of the initial condition \\(y_0 \\ge 0\\) and \\(t\\ge 0\\). (Why we exclude \\(y_0 &lt;0?\\)) Thus, we can apply the above theorem with \\(a=0\\) and \\(b=\\infty\\), concluding that \\(J = (0,\\infty)\\). For \\(t &lt; 0\\), the solution is bounded for \\(y_0 \\in [0,1]\\), thus \\(J = \\mathbb{R}\\). However, for \\(y_0 &gt; 1\\), we have \\(J = (\\bar{t},\\infty)\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Introduction to ODEs</span>"
    ]
  },
  {
    "objectID": "IntroODE.html#linear-odes",
    "href": "IntroODE.html#linear-odes",
    "title": "Appendix A ‚Äî Introduction to ODEs",
    "section": "A.4 Linear ODEs",
    "text": "A.4 Linear ODEs\nA general linear ODE is an equation of the form: \\[\n\\mathbf{y}' = \\mathbf{A}(t)\\mathbf{y} + \\mathbf{b}(t)\n\\]\nwith \\[\n\\mathbf{y} = \\begin{pmatrix}\ny_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix},\\quad\n\\mathbf{A}(t) = \\begin{pmatrix}\n0 & 1 & 0 & \\cdots & 0 \\\\\n0 & 0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & 0 & 1 \\\\\na_n(t) & a_{n-1}(t) & \\cdots & a_2(t) & a_1(t)\n\\end{pmatrix},\\quad\n\\mathbf{b}(t) = \\begin{pmatrix}\n0 \\\\ 0 \\\\ \\vdots \\\\ b(t)\n\\end{pmatrix}.\n\\]\nNote that \\(\\mathbf{f}(t,\\mathbf{y}) = \\mathbf{A}(t)\\mathbf{y} + \\mathbf{b}(t)\\) is only linear in \\(\\mathbf{y}\\), that is \\[\n\\mathbf{f}(t,\\alpha\\mathbf{y}+\\mathbf{z}) = \\alpha \\mathbf{f}(t,\\mathbf{y}) + \\mathbf{f}(t,\\mathbf{z}),\n\\]\nfor all choices of \\(\\alpha\\in\\mathbb{R}\\) and \\(\\mathbf{y}, \\mathbf{z}\\in\\mathbb{R}^n\\).\n\nA.4.1 Well-posedness\n\nTheorem A.5 Assuming that \\(\\mathbf{A} \\in \\mathcal{C}^0([\\alpha,\\beta];\\mathbb{R}^{n\\times n})\\) and \\(\\mathbf{b}\\in \\mathcal{C}^0([\\alpha,\\beta];\\mathbb{R}^n)\\), that is all coefficients are continuous function of \\(t\\in[\\alpha,\\beta]\\), then we have a unique global solution \\(\\boldsymbol{\\phi}\\in\\mathcal{C}^1((\\alpha,\\beta);\\mathbb{R}^n)\\) for each choice of the initial data.\n\nThe proof of the local well-posedness follows for the Cauchy-Lipschitz theorem, because \\(\\mathbf{f}(t,\\mathbf{y})\\) is locally Lipschitz uniformly in \\(t\\). In fact, since \\(\\mathbf{A}(t)\\) is continous on a closed interval, it is also bounded. Thus: \\[\n\\| \\mathbf{f}(t,\\mathbf{y}) - \\mathbf{f}(t,\\mathbf{z}) \\| =\n\\| \\mathbf{A}(t) (\\mathbf{y}-\\mathbf{z}) \\|\n\\le \\| \\mathbf{A}(t) \\| \\| \\mathbf{y}-\\mathbf{z} \\|\n\\le \\underbrace{\\max_{t\\in[\\alpha,\\beta]}\\| \\mathbf{A}(t) \\|}_{L} \\| \\mathbf{y}-\\mathbf{z} \\|.\n\\]\nThe global existence follows from Theorem¬†A.4. \\[\n\\| \\mathbf{f}(t,\\mathbf{y}) \\| = \\| \\mathbf{A}(t)\\mathbf{y} + \\mathbf{b}(t) \\|\n\\le \\| \\mathbf{A}(t)\\| \\|\\mathbf{y}\\| + \\| \\mathbf{b}(t) \\|\n\\le \\underbrace{\\max_{t\\in[\\alpha,\\beta]}\\| \\mathbf{A}(t) \\|}_{k_1} \\|\\mathbf{y}\\| + \\underbrace{\\max_{t\\in[\\alpha,\\beta]}\\| \\mathbf{b}(t) \\|}_{k_2}.\n\\]\n\n\nA.4.2 Solution of the homogeneous problem\nAn interesting aspect of linear ODEs is that we can represent explicitly all the solutions of the ODE. Obviously, a linear IVP has a unique solution. The ODE, on the other hand, can have many solutions. We start with the homogeneous problem: \\[\n\\mathbf{y}' = \\mathbf{A}(t)\\mathbf{y}.\n\\]\n\nTheorem A.6 Let \\(\\mathcal{U} = \\bigl\\{ \\boldsymbol{\\phi} \\in \\mathcal{C}^1((\\alpha,\\beta);\\mathbb{R}^n) : \\boldsymbol{\\phi}'(t) = \\mathbf{A}(t)\\boldsymbol{\\phi}(t) \\: \\forall t\\in(\\alpha,\\beta) \\bigr\\}\\) be the set of solutions of the linear ODE. Then \\(\\dim\\mathcal{U} = n\\).\n\nTherefore, there exists a basis \\(\\{\\boldsymbol{\\phi}_1(t),\\ldots,\\boldsymbol{\\phi}_n(t)\\}\\) of \\(\\mathcal{U}\\) such that each solution \\(\\boldsymbol{\\phi}(t) \\in \\mathcal{U}\\) reads as follows: \\[\n\\boldsymbol{\\phi}(t) = \\sum_{i=1}^n c_i \\boldsymbol{\\phi}_i(t),\n\\]\nfor some choice of \\(\\mathbf{c} = [c_1, \\ldots, c_n]^T \\in \\mathbb{R}^n\\).\n\nDefinition A.7 (Wronskian matrix) We define the Wronskian matrix \\(\\mathbf{W}(t)\\) as the column-matrix of the basis of \\(\\mathcal{U}\\):\n\\[\n\\mathbf{W}(t)=\\Bigl[\\begin{matrix}\n& \\boldsymbol\\phi_1(t) &|& \\boldsymbol\\phi_2(t) &|& \\cdots &|& \\boldsymbol\\phi_n(t) &\n\\end{matrix}\\Bigr].\n\\]\n\nNote that from its definition it follows that: \\[\n\\mathbf{W}(t)' = \\mathbf{A}(t)\\mathbf{W}(t).\n\\]\nThe general solution of the linear ODE is: \\[\n\\boldsymbol{\\phi}(t) = \\mathbf{W}(t)\\mathbf{c},\n\\]\nfor some \\(\\mathbf{c}\\in\\mathbb{R}^n\\). For an IVP with initial condition \\(\\mathbf{y}(t_0)=\\mathbf{y}_0\\), we have that: \\[\n\\boldsymbol{\\phi}(t_0) = \\mathbf{W}(t_0)\\mathbf{c} = \\mathbf{y}_0,\n\\quad\\Rightarrow\\quad\n\\mathbf{c} = \\mathbf{W}^{-1}(t_0)\\mathbf{y}_0,\n\\]\nthus the unique solution is: \\[\n\\boldsymbol{\\phi}(t) = \\mathbf{W}(t)\\mathbf{c} = \\mathbf{W}(t)\\mathbf{W}^{-1}(t_0)\\mathbf{y}_0\n= \\mathbf{W}(t,t_0)\\mathbf{y}_0,\n\\]\nwhere \\(\\mathbf{W}(t,t_0)\\) is called transition matrix. The transition matrix bears this name because it transfers the initial condition \\(\\mathbf y_0\\) at time \\(t_0\\) to the solution at time \\(t\\). It is a particular case of flow of an ODE. We have the following very useful properties, all straightforward to prove using the definition above:\n\n\\(\\mathbf W(t_0,t_0) = \\mathbf I\\),\n\\(\\mathbf W(t,s)\\mathbf W(s,t_0) = \\mathbf W(t,t_0)\\),\n\\((\\mathbf W(t,t_0))^{-1}=\\mathbf W(t_0,t)\\).\n\nNote that we haven‚Äôt proved the invertibility of \\(\\mathbf{W}(t_0)\\). We are going to do it later for the case of \\(\\mathbf{A}\\) with constant coefficients.\n\n\n\n\n\n\nExercise\n\n\n\nProve the properties of the transfer matrix.\n\n\n\n\nA.4.3 Solution of the general problem\nThe general solution is useful for building a particular solution for the non-homogeneous problem \\[\n\\mathbf{y}' = \\mathbf{A}(t)\\mathbf{y} + \\mathbf{b}(t).\n\\tag{A.3}\\]\nStarting from the solution \\(\\boldsymbol{\\phi}(t) = \\mathbf{W}(t)\\mathbf{c}\\), we can apply the method of variation of constants to find a solution of the form \\(\\boldsymbol{\\phi}(t) = \\mathbf{W}(t)\\mathbf{c}(t)\\), for some choice of \\(\\mathbf{c}(t)\\). It turns out that the particular solution is: \\[\n\\boldsymbol\\phi(t) = \\mathbf{W}(t) \\int^t \\mathbf{W}^{-1}(s)\\mathbf b(s) \\mathrm{d}s\n= \\int^t \\mathbf{W}(t,s)\\mathbf b(s) \\mathrm{d}s.\n\\]\nSupplemented with an initial condition \\(\\mathbf{y}(t_0)=\\mathbf{y}_0\\), the solution of the IVP is \\[\n\\boldsymbol\\phi(t) = \\mathbf{W}(t,t_0)\\mathbf{y}_0 + \\int_{t_0}^t \\mathbf{W}(t,s)\\mathbf{b}(s) \\mathrm{d}s.\n\\tag{A.4}\\]\n\n\n\n\n\n\nExercise\n\n\n\nShow that (Equation¬†A.4) is the solution of the IVP (Equation¬†A.3).\n\n\n\n\nA.4.4 Matrix exponential\nWe focus now on the following case: \\[\\begin{cases}\n\\mathbf y'=\\mathbf A\\mathbf y + \\mathbf b(t), & \\\\\n\\mathbf y(t_0)=\\mathbf y_0. &\n\\end{cases}\\]\nWe need the Wronskian matrix. For computing it, we use the fact that \\(\\mathbf{W}' = \\mathbf{A}\\mathbf{W}\\) and \\(\\mathbf{W}(0)=\\mathbf{I}\\) (identity matrix). Formally, notice that \\[\n\\begin{aligned}\n\\mathbf{W}(0) &= \\mathbf{I}, \\\\\n\\mathbf{W}'(0) &= \\mathbf{A}\\mathbf{W} = \\mathbf{A}, \\\\\n\\mathbf{W}''(0) &= \\mathbf{A}\\mathbf{W}' = \\mathbf{A}^2, \\\\\n&\\cdots \\\\\n\\mathbf{W}^{(k)}(0) &= \\mathbf{A}^n.\n\\end{aligned}\n\\]\nThus we can construct the solution as a Taylor expansion: \\[\n\\mathbf{W}(t) = \\sum_{n=0}^\\infty \\frac{\\mathbf{W}^{(n)}(0)}{n!} t^k\n= \\sum_{n=0}^\\infty \\frac{\\mathbf{A}^n}{n!} t^k\n= \\mathbf{I} + t\\mathbf{A} + \\frac{t^2}{2} \\mathbf{A}^2 + \\ldots\n\\]\n\nDefinition A.8 (Matrix exponential) We define the matrix exponential as: \\[\ne^{\\mathbf{A}} := \\sum_{n=0}^\\infty \\frac{\\mathbf{A}^n(t)}{n!}.\n\\]\n\nThus, the Wronskian is: \\[\n\\mathbf{W}(t) = e^{t\\mathbf{A}}.\n\\]\nThe series converges for each \\(\\mathbf{A}\\in\\mathbb{R}^{n\\times n}\\). Below some useful properties:\n\nIf \\(\\mathbf A = \\operatorname{diag}\\{ a_1, \\dots, a_n \\}\\), then \\(e^{\\mathbf A} = \\operatorname{diag}\\{ e^{a_1}, \\dots, e^{a_n} \\}\\). The matrix exponential of diagonal matrices is then straightforward to compute. The proof is simple: just plug the diagonal matrix in the definition and note that \\(\\mathbf A^k = \\operatorname{diag}\\{ a_1^k, \\dots, a_n^k \\}\\). Hence: \\[\\sum_{k=0}^\\infty \\frac{\\mathbf{A}^k}{k!} = \\operatorname{diag}\\biggl\\{ \\sum_{k=0}^\\infty \\frac{a_1^k}{k!}, \\ldots, \\sum_{k=0}^\\infty \\frac{a_n^k}{k!} \\biggr\\} = \\operatorname{diag}\\{ e^{a_1}, \\dots, e^{a_n} \\}.\\]\n\\(e^{\\mathbf 0} = \\mathbf{I}\\). If \\(\\mathbf A\\) is the matrix with all zero entries \\(\\mathbf{0}\\), then the matrix exponential is the identity matrix \\(\\mathbf{I}\\). This is trivial because all the entries of the sum are zero except for \\(k=0\\).\n\\(\\det(e^{\\mathbf A})=e^{\\operatorname{tr}\\mathbf A}\\). This is reminescent of the Wronskian (determinant of Wronskian matrix), and the proof is similar, based on the Taylor expansion \\(|\\mathbf I + \\varepsilon\\mathbf A| = 1+\\varepsilon\\operatorname{tr}\\mathbf A + \\mathcal{O}(\\varepsilon^2)\\). We use the alternative definition of matrix exponential, and the fact that the determinant is a continuous function with respect to the coefficients of the matrix: \\[\\begin{aligned}\n|e^{\\mathbf A}| &= \\biggl| \\lim_{n\\to\\infty} \\Bigl(\\mathbf I + \\tfrac{1}{n}\\mathbf A \\Bigr)^n \\biggr| \\\\\n\\text{\\small(continuity)} &= \\lim_{n\\to\\infty} \\biggl| \\Bigl( \\mathbf I + \\tfrac{1}{n}\\mathbf A \\Bigr)^n \\biggr| \\\\\n\\text{\\small(det of product)} &= \\lim_{n\\to\\infty} \\Bigl| \\mathbf I + \\tfrac{1}{n}\\mathbf A \\Bigr|^n \\\\\n\\text{\\small(det expansion)} &= \\lim_{n\\to\\infty} \\Bigl( 1 + \\tfrac{1}{n}\\operatorname{tr}\\mathbf A + \\mathcal{O}(\\tfrac{1}{n^2}) \\Bigr)^n = e^{\\operatorname{tr}\\mathbf A}.\n\\end{aligned}\\]\n\\(e^{\\mathbf A}\\) is invertible. This is a consequence of the previous property, since the determinant is always strictly positive.\nIf \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) commute, that is \\(\\mathbf{A}\\mathbf{B} = \\mathbf{B}\\mathbf{A}\\), then \\(e^{\\mathbf{A}+\\mathbf{B}}=e^{\\mathbf{A}} e^{\\mathbf{B}}\\). We are not proving this fact.\n\\(e^{0\\mathbf A}=\\mathbf{I}\\). This follows immediately from property 2. of the matrix exponential.\n\\(e^{t \\mathbf A} e^{s \\mathbf A} = e^{(t+s)\\mathbf A}\\). In this case we could use property 5., but instead we use the formula for the product of two series and Newton‚Äôs binomial formula: \\[\\begin{aligned}\ne^{t \\mathbf A} e^{s \\mathbf A}\n&= \\biggl(\\sum_{k=0}^\\infty \\frac{t^k \\mathbf A^k}{k!}\\biggr)\n\\cdot\\biggl(\\sum_{j=0}^\\infty \\frac{s^j \\mathbf A^j}{j!}\\biggr) \\\\\n&= \\sum_{k=0}^\\infty \\sum_{l=0}^k \\frac{t^l \\mathbf A^l}{l!} \\frac{s^{k-l} \\mathbf A^{k-l}}{(k-l)!} \\\\\n&= \\sum_{k=0}^\\infty \\frac{\\mathbf A^k}{k!} \\sum_{l=0}^k \\frac{k!}{l!(k-l)!} t^l s^{k-l} \\\\\n&= \\sum_{k=0}^\\infty \\frac{(t+s)^k\\mathbf A^k}{k!}\n= e^{(t+s)\\mathbf A}.\n\\end{aligned}\\]\n\\(\\frac{\\mathrm d}{\\mathrm d t} e^{t\\mathbf A} = \\mathbf A e^{t\\mathbf A}\\). It follows from the definition: \\[\\tfrac{\\mathrm d}{\\mathrm d t} e^{t\\mathbf A}\n%= \\frac{\\mathrm d}{\\mathrm d t} \\sum_{k=0}^\\infty \\frac{t^k \\mathbf A^k}{k!}\n= \\sum_{k=0}^\\infty \\frac{\\mathrm d}{\\mathrm d t} \\frac{t^k \\mathbf A^k}{k!}\n= \\sum_{k=1}^\\infty \\frac{kt^{k-1} \\mathbf A^k}{k!}\n= \\mathbf A \\sum_{k=1}^\\infty \\frac{t^{k-1} \\mathbf A^{k-1}}{(k-1)!}\n= \\mathbf A e^{t\\mathbf A}.\\]\n\nThis last property in particular shows rigorously that the matrix exponential is the Wronskian matrix of the ODE \\(\\mathbf y' = \\mathbf A\\mathbf y\\), while with the other properties we define the transition matrix.\n\n\nA.4.5 Solution of the case with constant coefficients\nThe solution of \\[\\begin{cases}\n\\mathbf y'=\\mathbf A\\mathbf y + \\mathbf b(t), & \\\\\n\\mathbf y(t_0)=\\mathbf y_0, &\n\\end{cases}\\]\nfollows now trivially from the matrix exponential. We have: \\[\n\\boldsymbol\\phi(t) = e^{(t-t_0)\\mathbf{A}}\\mathbf{y}_0 + \\int_{t_0}^t e^{(t-s)\\mathbf{A}}\\mathbf{b}(s) \\mathrm{d}s.\n\\]\n\n\n\n\n\n\nExample (scalar case)\n\n\n\nWhen \\(n = 1\\), with \\(\\mathbf{A}=a\\in\\mathbb{R}\\), the matrix exponential is the usual exponential function. The solution is \\[\n\\phi(t) = e^{a(t-t_0)}y_0 + \\int_{t_0}^t e^{a(t-s)}b(s) \\mathrm{d}s.\n\\]\nFor instance, with \\(b = e^{\\omega t}\\), we have: \\[\n\\int_{t_0}^t e^{a(t-s)}b(s) \\mathrm{d}s =\n\\int_{t_0}^t e^{a(t-s)}e^{\\omega s} \\mathrm{d}s\n= \\frac{e^{a t} - e^{\\omega t}}{a-\\omega}.\n\\]\nSo the solution to the problem: \\[\n\\begin{cases}\ny' = ay + e^{\\omega t}, & \\\\\ny(t_0) = y_0, &\n\\end{cases}\n\\]\nis as follows: \\[\n\\phi(t) = e^{a(t-s)} + \\frac{e^{a t} - e^{\\omega t}}{a-\\omega}.\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Introduction to ODEs</span>"
    ]
  },
  {
    "objectID": "IntroODE.html#sec-dyn",
    "href": "IntroODE.html#sec-dyn",
    "title": "Appendix A ‚Äî Introduction to ODEs",
    "section": "A.5 Dynamical systems",
    "text": "A.5 Dynamical systems\nIntuitively, a dynamical system is a state or a set of variables evolving over time according to some rule. The concept is very general. Markov chains, cellular automata and large language models are examples of dynamical systems. Roughly speaking, a dynamical system is composed by\n\nA state space, the set of all admissible states of the system, and\nA rule for moving from the current state to the next one.\n\nWe can associate to a system of ODEs a dynamical system. The state space (called phase space) is the subset of \\(\\mathbb{R}^n\\) of all possible states. The update rule is the solution of the ODE (called flow). For ODEs, one is not interested in studying a specific instance (or trajectory) of the system, but rather the system as a whole, for all possible initial states.\n\nDefinition A.9 A dynamical system is well-posed autonomous ODE (autonomous means that \\(\\mathbf{f}\\) is not an explicit function of \\(t\\)). Specifically, given \\(\\mathbf{f}\\in\\mathcal{C}^1(\\Omega;\\mathbb{R}^n)\\), with \\(\\Omega\\neq\\emptyset\\) an open set of \\(\\mathbb{R}^n\\), we suppose that for every initial condition \\(\\mathbf{y}_0\\in\\mathbb{R}^n\\), the IVP \\[\\begin{cases}\n\\mathbf{y}' = \\mathbf{f}(\\mathbf{y}), \\\\\n\\mathbf{y}(0) = \\mathbf{y}_0,\n\\end{cases}\\]\nhas a unique solution \\(\\boldsymbol{\\phi}(t)\\) defined for \\(t\\in[0,\\infty)\\), and such that \\(\\boldsymbol{\\phi}(t)\\in\\Omega\\) for all \\(t\\ge0\\). We call \\(\\mathbf{y}' = \\mathbf{f}(\\mathbf{y})\\) dynamical system on the phase space \\(\\Omega\\). Succintly, the dynamical system is fully qualified by the pair \\((\\mathbf{f},\\Omega)\\).\n\n\nDefinition A.10 The flow of an ODE is a function \\(\\boldsymbol{\\Phi}\\colon\\mathbb{R}\\times\\Omega\\to \\Omega\\) such that \\[\n\\boldsymbol{\\Phi}(t,\\mathbf{y}_0) = \\boldsymbol{\\phi}(t),\n\\]\nwhere \\(\\boldsymbol{\\phi}(t)\\) is the solution of the ODE with initial condition \\(\\mathbf{y}_0\\).\n\nFor example, for a linear dynamical system \\(\\mathbf{y}' = \\mathbf{A}\\mathbf{y}\\) the flow is given by the transition matrix: \\[\n\\boldsymbol{\\Phi}(t,\\mathbf{y}_0) = \\mathbf{W}(t,0)\\mathbf{y}_0 = e^{t\\mathbf{A}}\\mathbf{y}_0.\n\\]\nNote. It is worth to mention that also the general, non-autonomous ODEs \\(\\mathbf{y}' = \\mathbf{f}(t,\\mathbf{y})\\) in \\(\\mathbb{R}^n\\) can be recast to the autonomous form \\(\\mathbf{z}' = \\mathbf{g}(\\mathbf{z})\\) in \\(\\mathbb{R}^{n+1}\\) by setting \\[\\mathbf{z}(t) = \\begin{bmatrix} \\tau(t) \\\\ \\mathbf{y}(t) \\end{bmatrix},\\quad \\mathbf{g}(\\mathbf{z}) = \\begin{bmatrix} 1 \\\\ \\mathbf{f}(\\tau,\\mathbf{y}) \\end{bmatrix},\\]\nin fact, \\(\\tau(t)=t\\).\n\nA.5.1 Orbits and trajectories\n\nDefinition A.11 For a given solution (or trajectory) \\(\\boldsymbol{\\phi}(t)\\), we consider the associated orbit \\(\\mathcal{O}(\\boldsymbol{\\phi})\\) defined as follows \\[\\mathcal{O}(\\boldsymbol{\\phi}) = \\bigcup_{t\\ge 0} \\boldsymbol{\\phi}(t),\\]\nthat is, the orbit is the ‚Äúshadow‚Äù of a trajectory onto the phase space \\(\\Omega\\).\n\nIn practice, the study of a dynamical system is not limited to a particular solution (defined by some initial condition), but rather it covers the entire domain. In some sense, there is a strong geometrical interpretation: the r.h.s. of the ODE determines a vectorial field in the phase space \\(\\Omega\\), so that a given orbit is tangent at every point to such field.\n\nProposition A.1 If \\(\\boldsymbol{\\phi}(t)\\) is a trajectory of the dynamical system for all \\(t\\ge 0\\), also \\(\\boldsymbol{\\psi}(t) = \\boldsymbol{\\phi}(t+c)\\) is a solution for all \\(t\\ge -c\\), with \\(c\\in\\mathbb{R}\\).\n\nThe proposition is a simple consequence of the fact that a dynamical system is an autonomous ODE, that is it \\(\\mathbf{f}\\) is not a function of time.\n\nProposition A.2 Two distinct orbits of a dynamical system either coincide or they never intersect.\n\nThis proposition is extremely important when analyzing the phase portrait of a system, because some orbits may act as ‚Äúbarriers‚Äù for other orbits. Please note that the statement refers to orbits, not simply to the trajectories.\nTo prove the proposition, suppose there are two distinct orbits associated to the trajectories \\(\\boldsymbol{\\phi}\\) and \\(\\boldsymbol{\\psi}\\), and such that \\(\\boldsymbol{\\phi}(t_1) = \\boldsymbol{\\psi}(t_2)\\) for some \\(t_1,t_2\\ge 0\\). That is, the orbits intersect. Now consider the function \\(\\tilde{\\boldsymbol{\\psi}}(t) = \\boldsymbol{\\psi}(t+t_2-t_1)\\). This function is still a solution, and \\(\\tilde{\\boldsymbol{\\psi}}(t_1) = \\boldsymbol{\\psi}(t_2) = \\boldsymbol{\\phi}(t_1)\\). Thus, \\(\\tilde{\\boldsymbol{\\psi}}\\) and \\(\\boldsymbol{\\phi}\\) have the same initial condition, and they must coincide for all \\(t\\), that is \\(\\tilde{\\boldsymbol{\\psi}}(t) = \\boldsymbol{\\phi}(t)\\). In other words, \\(\\tilde{\\boldsymbol{\\psi}}\\) and \\(\\boldsymbol{\\phi}\\) have the same orbit. But also \\(\\tilde{\\boldsymbol{\\psi}}\\) and \\(\\boldsymbol{\\psi}\\) have the same orbit, because the system is autonomous. In conclusion, if two orbits intersect at a least one point, they must concide at all points.\n\n\n\n\n\n\nExample (Van der Pol)\n\n\n\nBelow an example of the phase space for the equation \\(y'' - \\mu(1-y^2)y' + y = 0\\), which models a stiff oscillating system.\n\n\n\nTrajectory\n\n\nIn the figure, in blue we have the orbit associated to the trajectory in orange.\n\n\n\nPhase space\n\n\nThis second figure represents the phase portrait of the system, with the orbit and the limit cycle.\n\n\n\n\n\n\n\n\nExample (positive systems)\n\n\n\nAn example are dynamical systems of the form: \\[\\begin{cases}\ny_1' = y_1 \\cdot f(y_1,y_2), \\\\\ny_2' = y_2 \\cdot g(y_1,y_2).\n\\end{cases}\\]\nIn fact, the line \\((y_1,y_2) = (0,y_2)\\) (the ordinate) is an orbit of the system, because if we start from \\((0,y_2)\\) we never leave the line, since \\(y_1'=0\\). Similarly, the line \\((y_1,y_2) = (y_1,0)\\) (the abscissa) is also an orbit. Therefore, any other orbit starting in the positive quadrant will stay in the positive quadrant for all times. That is, the two orbits are barriers that ensures the positivity of the system for all times.\n\n\n\n\nA.5.2 Types of orbits\nSome orbits of a dynamical system are special. Here, we only mention the two most important ones: equilibria and periodic orbits.\n\nDefinition A.12 We say that \\(\\mathbf{y}_0\\in\\Omega\\) is an equilibrium of the system if \\(\\mathbf{f}(\\mathbf{y}_0)=0\\). In particular, the trajectory \\(\\boldsymbol{\\phi}(t)=\\mathbf{y}_0\\) for all \\(t\\in\\mathbb{R}\\) is called solution of the equilibrium. The orbit is simply the point \\(\\{ y_0 \\}\\).\n\nWhat if an orbit self-intersect at some finite time? We have a periodic orbit.\n\nProposition A.3 If there exist \\(\\tau_1,\\tau_2\\ge 0\\) such that \\(\\boldsymbol{\\phi}(\\tau_1) = \\boldsymbol{\\phi}(\\tau_2)\\), then \\(\\boldsymbol{\\phi}(t)\\) is a periodic solution.\n\nIn fact, as above, consider the trajectory \\(\\boldsymbol{\\psi}(t) = \\boldsymbol{\\phi}(t-\\tau_1+\\tau_2)\\). Then \\(\\boldsymbol{\\psi}(\\tau_1)=\\boldsymbol{\\phi}(\\tau_2)=\\boldsymbol{\\phi}(\\tau_1)\\). So we conclude that \\(\\boldsymbol{\\psi}(t)\\) and \\(\\boldsymbol{\\phi}(t)\\) coincide for all time. In particular, \\(\\boldsymbol{\\phi}(t)=\\boldsymbol{\\phi}(t+\\tau_2-\\tau_1)\\), that is, \\(\\boldsymbol{\\phi}(t)\\) is periodic with period \\(\\tau_2-\\tau_1\\).\nThe kind of orbits that we may find in a dynamical system is actually limited to\n\nOrbits consisting of a single point (equilibria);\nOrbits corresponding to closed regular curves (periodic solutions);\nOrbits corresponding to open regular curves, with no self-intersections.\n\nThe dimension of the phase space plays a key role in the type of attractor. For \\(\\Omega \\subset \\mathbb{R}\\), we can only have equilibria. In fact:\n\nProposition A.4 For \\(n=1\\) a dynamical system cannot have periodic solutions.\n\nFor the proof, suppose that \\(\\phi(t)\\) is periodic with period \\(T&gt;0\\). Then, we multiply by \\(\\phi'\\) both sides of the ODE and integrate in \\([t,t+T]\\): \\[\n\\int_t^{t+T} \\bigl( \\phi'(s)\\bigr)^2 \\:\\mathrm{d}s = \\int_t^{t+T} f(\\phi(s))\\phi'(s)\\:\\mathrm{d}s\n= \\int_{\\phi(t)}^{\\phi(t+T)} f(u)\\:\\mathrm{d}u = 0,\n\\]\nwhere we applied the change of variables \\(u = \\phi(s)\\). The last integral is zero because \\(\\phi(t)=\\phi(t+T)\\). But the first integral is strictly positive, so we have a contradiction.\nNote. For \\(n=1\\), a more general non-autonomuous ODE \\(y'=f(t,y)\\) can have periodic solutions. But this is not a dynamical system, unless we recast it as a system, thus \\(n=2\\) and periodic solutions are possible.\n\n\n\n\n\n\nExample (test problem)\n\n\n\nThe dynamical system \\(y'=\\lambda y\\) has only one equilibrium, \\(y^*=0\\).\n\n\n\n\n\n\n\n\nExample (logistic equation)\n\n\n\nThe dynamical system \\(y'=y(1-y)\\) has two equilibria, \\(y_1=0\\) and \\(y_2=1\\).\n\n\n\n\n\n\n\n\nExample (linear system)\n\n\n\nGiven \\(\\mathbf{A}\\in\\mathbb{R}^{n\\times n}\\), the dynamical system \\(\\mathbf{y}'=\\mathbf{A}\\mathbf{y}\\) has equilibria such that \\(\\mathbf{A}\\mathbf{y}^*=0\\), that is equilibria belong to the kernel of \\(\\mathbf{A}\\), \\(\\mathbf{y}^*\\in\\ker\\mathbf{A}\\). If \\(\\mathbf{A}\\) is invertible, then we only have \\(\\mathbf{y}^*=0\\).\n\n\n\n\n\n\n\n\nExample (FitzHugh-Nagumo model)\n\n\n\nWe consider the dynamical system defined by the ODE \\[\\begin{cases}\nu' = f(u,r) = u(1-u)(u-\\alpha) - r, \\\\\nr' = g(u,r) = \\beta(u-\\gamma r),\n\\end{cases}\\]\nwhere \\(\\alpha,\\beta,\\gamma\\in\\mathbb{R}\\) are parameters of the model. This system is an important phenomenological model of excitability of cells and for modeling the action potential.\nWe define as nullclines associated to \\(u'\\) (resp. \\(r'\\)) the implicit curve defined by the equation \\(f(u,r)=0\\) (resp. \\(g(u,r)=0\\)). Equilibria are found as intersections between nullclines, because at those points all right hand sides simultaneously cancel. For the FitzHugh-Nagumo system, nullclines are: \\[\\begin{cases}\nf(u,r) = 0, \\Leftrightarrow r = u(1-u)(u-\\alpha), \\\\\ng(u,r) = 0, \\Leftrightarrow r = \\gamma^{-1} u.\n\\end{cases}\\]\nThe first nullcline is a cubic function with zeros at \\(0\\), \\(\\alpha\\), and \\(1\\), and such that \\(r\\to-\\infty\\) for \\(u\\to\\infty\\). The second nullcline is a line through the origin and with slope \\(\\gamma^{-1}\\). One equilibrium is certainly the point \\((u_1,r_1)=(0,0)\\), for all choices of the parameters. The other equilibria depend on the choice of the parameters, specifically on whether the cubic function and the line intersect outside the origin. Those are such that \\[u(1-u)(u-\\alpha) = \\gamma^{-1} u,\\]\nso, besides \\(u=0\\), the number of zeros depends on the sign of the discriminant: \\[\\Delta = (\\alpha+1)^2 - 4 \\gamma^{-1}.\\]\nIn conclusion, we have three equilibria for \\(\\Delta &gt; 0\\), two equilibria for \\(\\Delta = 0\\) and only one for \\(\\Delta &lt; 0\\).\n\n\n\n\nA.5.3 Lyapunov stability and attractors\nConsider a dynamical system \\((\\mathbf{f},\\Omega)\\) with \\(\\Omega\\subseteq \\mathbb{R}^n\\), with an equilibrium \\(\\mathbf{y}_0 \\in\\Omega\\), that is \\(\\mathbf{f}(\\mathbf{y}_0)=0\\). How does the system behave in a neighborhood of the equilibrium? Say, if we start close to \\(\\mathbf{y}_0\\), do the stay close to it for long? Please note that the interest is purely qualitative: we are not interested in the specific form of the trajectory, but rather its behavior for \\(t\\to\\infty\\).\n\nDefinition A.13 (Lyapunov stability) We say that \\(\\mathbf{y}_0\\) is a locally stable equilibrium if for each \\(\\varepsilon&gt;0\\) there exists \\(\\delta = \\delta(\\varepsilon,\\mathbf{y}_0)\\) such that for all \\(\\mathbf{y}_1\\in\\Omega\\) with \\(\\| \\mathbf{y}_1 - \\mathbf{y}_0 \\| &lt; \\delta\\) we have that \\(\\| \\boldsymbol{\\Phi}(t,\\mathbf{y}_1) - \\mathbf{y}_0 \\| &lt; \\varepsilon\\) for all \\(t\\ge 0\\), where \\(\\boldsymbol{\\Phi}(t,\\mathbf{y})\\) is the flow of the system.\nMoreover, we say that \\(\\mathbf{y}_0\\) is asymptotically stable if in the above definition we have \\(\\| \\boldsymbol{\\phi}(t;\\mathbf{y}_1) - \\mathbf{y}_0 \\| \\to 0\\) for \\(t\\to\\infty\\).\n\nThe difference between simple stability and asymptotic stability is that in the former case orbits stay close to the equilibrium without necessarily approaching it for \\(t\\to\\infty\\). For instance, the vertical downward position of the frictionless pendulum is only stable, because the orbits of the systems (oscillations at given amplitude) stay close to it with distance equal to the amplitude but never approach it. On the other hand, in the presence of friction, the equilibrium becomes asymptotically stable. (Thermodynamically speaking, asymptotically stable equilibria are quite boring: it often means extinction, death.)\nFinally, an unstable equilibrium is defined by (logically) negating the above definition. That is, \\(\\mathbf{y}_0\\in\\Omega\\) is an unstable equilibrium if there exists \\(\\varepsilon&gt;0\\) such that for all \\(\\delta &gt; 0\\) there exists \\(\\mathbf{y}_1\\in\\Omega\\) with the property that \\(\\|\\mathbf{y}_1 - \\mathbf{y}_0\\| &lt; \\delta\\) but \\(\\|\\boldsymbol{\\phi}(t_n,\\mathbf{y}_1) - \\mathbf{y}_0 \\| \\ge \\varepsilon\\) for a sequence \\(\\{ t_n \\}_n\\to\\infty\\) and all \\(n\\in\\mathbb{N}\\). In other words, there exists at least one initial condition that brings the associated trajectory arbitrarily far from the equilibrium at some time points \\(t_n\\) approaching infinity. (It would be too much to ask the same for all \\(t\\ge M\\), because we could have a diverging trajectory that periodically comes very close to the equilibrium, without really approaching it. Take for instance the function \\(\\phi(t)=t\\sin^2 t + \\varepsilon/2\\), where for \\(t=t_n=n\\pi\\) is equal to \\(\\varepsilon/2\\), but for \\(t_n=n\\pi/2\\) is diverging: it is clearly unstable.)\nAn interesting concept associated with equilibria is that of basin of attraction.\n\nDefinition A.14 (Basin of attraction) If an equilibrium is only locally asymptotically stable, then we have the basin of attraction defined as \\[\\mathcal{B}(\\mathbf{y}_0) := \\Bigl\\{ \\tilde{\\mathbf{y}}\\in\\Omega\\colon \\lim_{t\\to+\\infty} \\boldsymbol{\\Phi}(t,\\mathbf{y}_0) = \\mathbf{y}_0 \\Bigr\\}.\\]\n\nAn equilibrium is globally asymptotically stable when \\(\\mathcal{B}(\\mathbf{y}_0)=\\Omega\\).\nRemark. Lyapunov stability is different from the concept of stability to perturbation, or zero stability (see Corollary¬†A.1). In the latter, the aim is check whether we are able to recover the original solution (equilibrium or not) as the perturbation in the dynamical system goes to zero. The original and perturbed solutions may diverge from each other for long time, but we can always reduce the gap between them by reducing the initial error (the perturbation): when that‚Äôs not possible, the system is not (zero) stable. Lyapunov stability, on the other hand, concerns the structural stability properties of the system, that is we study the long-term behavior of solutions without controlling the initial perturbation. Actually, all initial conditions in the basin of attraction of an equilibrium yield solutions that converge to the same equilibrium, irrespective of the gap between them.\n\n\n\n\n\n\nExample (stability of the test problem)\n\n\n\nThe stability of equilibrium \\(y=0\\) of \\(y'=\\lambda y\\) depends clearly on \\(\\lambda\\). Suppose that \\(\\lambda\\in\\mathbb{C}\\). Then, the full solution is of the form \\[\\phi(t) = e^{\\lambda t} y_0.\\]\nWe can expand the exponential to get more insights: \\[\\phi(t) = e^{\\lambda t} y_0 = e^{t \\operatorname{Re}\\lambda}e^{t \\operatorname{Im}\\lambda} y_0 =\ne^{t \\operatorname{Re}\\lambda} \\bigl(\\cos(t\\operatorname{Im}\\lambda) + i \\sin(t\\operatorname{Im}\\lambda)\\bigr) y_0.\\]\nSince the equilibrium is 0, we just need to check whether this trajectory, in modulus, stays close (or even approaches) zero over time. That is, \\[|\\phi(t)|^2 = \\phi^*(t)\\phi(t) = e^{2 t \\operatorname{Re}\\lambda} \\bigl(\\cos(t\\operatorname{Im}\\lambda)^2 + \\sin(t\\operatorname{Im}\\lambda)\\bigr)^2 |y_0|^2,\\]\nso we have: \\[|\\phi(t)| = e^{t \\operatorname{Re}\\lambda}|y_0|.\\]\nFrom this expression, we easily deduce that\n\nIf \\(\\operatorname{Re}\\lambda &lt; 0\\), then \\(|\\phi(t)|\\to 0\\) for all \\(y_0\\in\\mathbb{C}\\). The equilibrium is therefore globally asymptotically stable.\nIf \\(\\operatorname{Re}\\lambda &gt; 0\\), then \\(|\\phi(t)|\\to\\infty\\) for at least one non trivial \\(y_0\\in\\mathbb{C}\\). The equilibrium is therefore unstable.\nIf \\(\\operatorname{Re}\\lambda = 0\\), then \\(|\\phi(t)| = |y_0|\\). The equilibrium is stable.\n\n\n\n\n\n\n\n\n\nExample (stability of logistic equation)\n\n\n\nThe logistic equation \\(y'=y(1-y)\\) has two equilibria, \\(y_1=0\\) and \\(y_2=1\\). We could study their stability by taking advantage of the analytical solution, available in this case, but we will not. We will proceed more generally. The dynamics is determined by the sign of \\(f(y)=y(1-y)\\).\n\nIf \\(y_0 = y_1\\) or \\(y_0 = y_2\\), there is no dynamics over time.\nIf \\(y_0\\in(0,1)\\) (boundaries excluded), then the solution \\(\\phi(t;y_0)\\) will never leave the interval \\((0,1)\\), because \\(0\\) and \\(1\\) are barriers (equilibria are also orbits, and orbits cannot intersect). Moreover, in this region \\(f(y)&gt;0\\). Therefore \\(y'&gt;0\\), that the solution \\(\\phi(t;y_0)\\) increases over time. Since the equilibrium \\(y_2=1\\) cannot be crossed, \\(\\phi(t;y_0)\\) indefinitely approaches \\(y_2\\) (from the left) without crossing it.\nIf \\(y_0 &gt; 1\\), with the same reasoning as above we conclude that \\(\\phi(t;y_0)&gt;1\\) indefinitely. But here \\(f(y)&lt;0\\), so \\(y'&lt;0\\) and again \\(\\phi(t;y_0)\\) approaches \\(y_2\\) (from the right).\nIf \\(y_0 &lt; 0\\), \\(y' = f(y)&lt;0\\) and \\(\\phi(t;y_0)\\to-\\infty\\).\n\nWith the above analysis, we can easily conclude that\n\n\\(y_1=0\\) is unstable, because orbits diverge from it.\n\\(y_2=0\\) is asymptotically stable, but only locally, with \\(\\mathcal{B}(y_2) = \\{ y &gt; 0 \\}\\).\n\nWhen restricting the dynamical system to \\(\\Omega=\\mathbb{R}^+\\), as in the case of population dynamics, the equilibrium \\(y_2\\) is almost globally attractive, except when we start from \\(y_1=0\\).\n\n\nPlease note that the above argument is very general, as it can be straightforwardly applied to any 1-D dynamical system: it is enough to study the sign and the zeros of \\(f(y)\\).\n\n\nA.5.4 Stability of linear ODEs\n\nA.5.4.1 Computation of the matrix exponential\nThe computation of the matrix exponential is not a trivial task, in general. In some cases, however, it is practical. When \\(\\mathbf A\\) is diagonal we have seen that the matrix exponential is trivially the element-wise exponential.\nWhen \\(\\mathbf A\\) is diagonalizable, that is there exists a matrix \\(\\mathbf S\\) such that \\[\\mathbf S^{-1}\\mathbf A\\mathbf S = \\boldsymbol\\Lambda,\\]\nwith \\(\\boldsymbol\\Lambda\\) diagonal matrix, the exponential matrix follows immediately from the following observation: \\[(\\mathbf S\\boldsymbol\\Lambda\\mathbf S^{-1})^k = (\\mathbf S\\boldsymbol\\Lambda\\mathbf S^{-1})(\\mathbf S\\boldsymbol\\Lambda\\mathbf S^{-1})\\cdots(\\mathbf S\\boldsymbol\\Lambda\\mathbf S^{-1})=\\mathbf S\\boldsymbol\\Lambda^k \\mathbf S^{-1}.\\]\nIn fact, we have \\[e^{\\mathbf A} = \\sum_{k=0}^\\infty \\frac{\\mathbf A^k}{k!}\n= \\sum_{k=0}^\\infty \\frac{(\\mathbf S\\boldsymbol\\Lambda\\mathbf S^{-1})^k}{k!}\n= \\mathbf S \\sum_{k=0}^\\infty \\frac{\\boldsymbol\\Lambda^k}{k!} \\mathbf S^{-1}\n= \\mathbf S e^{\\boldsymbol\\Lambda} \\mathbf S^{-1}.\\]\nThe matrix \\(e^{\\boldsymbol{\\Lambda}}\\) is easy to compute, using the above formula (at least when it is easy to compute eigenvalues and eigenvectors.)\nIn the general case, when \\(\\mathbf A\\) is not diagonalizable, the computation is not straightforward. It is based on the so-called Jordan canonical form. In practice, it is always possible to find a matrix \\(\\mathbf S\\) such that \\(\\mathbf S^{-1}\\mathbf A\\mathbf S = \\mathbf J\\) is in the Jordan canonical form, that is \\(\\mathbf J\\) is a block diagonal matrix, each block with a specific structure. The matrix exponential of the Jordan canonical form is again a block diagonal matrix. The matrix exponential of each block can be computed explicitly.\nThe Jordan canonical form is as follows: \\[\\mathbf J = \\begin{pmatrix} \\mathbf{J}_1 & & \\\\ & \\ddots & \\\\ & & \\mathbf{J}_r \\end{pmatrix}\\]\nwhere \\(J_i\\) is a matrix of the form: \\[\\mathbf{J}_i = \\begin{pmatrix} \\lambda_i & 1 & & \\\\ & \\lambda_i & \\ddots & \\\\ & & \\ddots & 1 \\\\ & & & \\lambda_i \\end{pmatrix}\\]\nThe eigenvalues \\(\\lambda_i\\) of the matrix \\(\\mathbf A\\) appears on the diagonal of the Jordan block \\(\\mathbf{J}_i\\). If the matrix is diagonalizable, then there are exactly \\(n\\) Jordan blocks each of dimension 1. In fact, the Jordan canonical form is diagonal. If some eigenvalues have geometric multiplicity strictly less than algebraic multiplicity, then the matrix is not diagonalizable. The Jordan blocks compensate for the difference in multiplicity. For instance, consider the matrix \\[\\mathbf A = \\begin{pmatrix} 0 & 1 \\\\ -1 & 2 \\end{pmatrix}.\\]\nThe eigenvalues are \\(\\lambda_1 = \\lambda_2 = 1\\). The algebraic multiplicity is 2, but the geometric multiplicity, that is the dimension of the eigenspace \\(E_\\lambda = \\operatorname{ker}(\\mathbf A - \\lambda\\mathbf I)\\) associated to \\(\\lambda=1\\), is only 1. The Jordan canonical form is \\[\\mathbf J = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}.\\]\nThe matrix exponential of the Jordan block follows from the property (for a \\(2\\times 2\\) block): \\[\\begin{pmatrix} \\lambda_i & 1 \\\\ 0 & \\lambda_i \\end{pmatrix}^k\n= \\begin{pmatrix} \\lambda_i^k & k \\lambda_i^{k-1} \\\\ 0 & \\lambda_i^k \\end{pmatrix}.\\]\nSo putting everything together we have \\[e^{t\\mathbf J} = \\begin{pmatrix} e^{\\lambda_i t} & t e^{\\lambda_i t} \\\\ 0 & e^{\\lambda_i t} \\end{pmatrix}.\\]\nPlease note that the appearance of the term \\(t e^{\\lambda_i t}\\).\nFor the sake of completeness, we just recall that the algebraic multiplicity is associated with the characteristic polynomial \\(\\mathcal{P}(\\lambda) = \\det(\\lambda\\mathbf{I}-\\mathbf{A})\\), and corresponds to the number of times a zero of \\(\\mathcal{P}(\\lambda)\\) appears. More precisely, we can always write \\(\\mathcal{P}(\\lambda) = (\\lambda-\\lambda_1)^{\\mu_1}\\cdot (\\lambda-\\lambda_2)^{\\mu_2}\\cdots (\\lambda-\\lambda_r)^{\\mu_r}\\), \\(\\{ \\lambda_i \\}_{i=1}^r\\), \\(r\\le n\\), are the eigenvalues, and \\(1\\le \\mu_i\\le n\\) the algebraic multiplicity of \\(\\lambda_i\\). The geometrical multiplicity \\(\\mu_i\\) is the dimension of the eigenspace \\(V_i = \\ker(\\lambda_i\\mathbf{I}-\\mathbf{A}) = \\{ \\mathbf{v}\\in\\mathbb{R}^n\\colon \\lambda_i\\mathbf{v}=\\mathbf{A}\\mathbf{v} \\}\\). Since \\(\\nu_i \\le \\mu_i\\), when at least one eigenvalue has \\(\\nu_i &lt; \\mu_i\\), the direct sum of all \\(V_i\\)s does not fill the whole \\(\\mathbb{R}^n\\), and additional (generalized) eigenvectors are required. These are taken from \\(\\mathbf{A}^k\\), for some \\(k=2,3,\\ldots\\), and leads to terms of the form \\(t^{k-1}e^{t\\lambda_i}\\) in the matrix exponential.\n\n\nA.5.4.2 Stability\nFor the linear ODEs \\(\\mathbf{y}' = \\mathbf{A}\\mathbf{y}\\), with \\(\\mathbf{A}\\in\\mathbb{R}^{n\\times n}\\) invertible, we only have the equilibrium \\(\\mathbf{y}^* = \\mathbf{0}\\). In the more general case \\(\\mathbf{y}' = \\mathbf{A}\\mathbf{y} + \\mathbf{b}\\), the equilibrium is \\(\\mathbf{y}^* = \\mathbf{A}^{-1}\\mathbf{b}\\). Note that in this case we can define \\(\\mathbf{z}(t) = \\mathbf{y}(t) - \\mathbf{y}^*\\) that satisfies the ODE \\(\\mathbf{z}' = \\mathbf{A}\\mathbf{z}\\) with equilibrium \\(\\mathbf{z}^* = \\mathbf{0}\\). Thus, we can focus on the homogeneous case with no loss of generality.\nWe now try to characterize the stability of the equilibrium \\(\\mathbf{y}^* = \\mathbf{A}\\) for \\(\\mathbf{y}' = \\mathbf{A}\\mathbf{y}\\).\nLet‚Äôs first consider the case of \\(\\mathbf{A}\\) diagonalizible. Here, there exists an invertible matrix \\(\\mathbf{V}\\in\\mathbb{R}^{n\\times n}\\) such that \\(\\mathbf{V}^{-1}\\mathbf{A}\\mathbf{V} = \\boldsymbol{\\Lambda}\\) is a diagonal matrix. The entries of this matrix are the eigenvalues of \\(\\mathbf{A}\\). The general solution of the ODE reads: \\[\\boldsymbol{\\phi}(t) = e^{t\\mathbf{A}}\\mathbf{y}_0 = \\mathbf{V} e^{t\\boldsymbol{\\Lambda}}\\mathbf{V}^{-1}\\mathbf{y}_0.\\]\nWe need to compute \\[\n\\lim_{t\\to\\infty} \\| \\boldsymbol{\\phi}(t) - \\mathbf{y}^* \\|,\n\\]\nfor an arbitrary initial condition \\(\\mathbf{y}_0\\).\nSince the matrix exponential of a diagonal matrix is just the component-wise exponentiation, we have that the components of \\(\\mathbf{V}^{-1}\\boldsymbol{\\phi}\\) are a linear combination of terms of the form \\(e^{\\lambda_i t}\\), being \\(\\lambda_i\\) the \\(i\\)-th eigenvalue of \\(\\mathbf{A}\\). Thus, we have that\n\nIf \\(\\operatorname{Re}\\lambda_i &lt; 0\\) for all \\(i=1,\\ldots,n\\), then \\(0\\) is the only globally attractive equilibrium.\nIf there exists at least one eigenvalue such that \\(\\operatorname{Re}\\lambda_i &gt; 0\\), the equilibrium \\(0\\) is unstable.\nIf \\(\\operatorname{Re}\\lambda_i \\le 0\\) for all \\(i=1,\\ldots,n\\), then \\(0\\) is stable.\n\nFor a generic \\(\\mathbf{A}\\in\\mathbb{R}^{n\\times n}\\) (diagonalizable or not), given the set of eigenvalues \\(\\lambda_i\\), the solution is some linear combination of terms of the form: \\[e^{\\lambda t}, t e^{\\lambda t}, \\ldots, t^m e^{\\lambda t},\\]\ndepending on the geometric multiplicity of \\(\\lambda\\). So we have that 1. and 2. above still applies, because the exponential is stronger than any polynomial. The non-trivial case is when \\(\\operatorname{Re}\\lambda_i = 0\\) for some \\(\\lambda_i\\). If \\(\\lambda_i\\) is such that we need extra terms of the form \\(t^j e^{\\lambda_i t}\\), \\(j\\ge 1\\), to complete the solution space, then the equilibrium is clearly unstable, because: \\[t^j e^{\\lambda_i t} = t^j \\bigl( \\cos(t\\operatorname{Im}\\lambda_i) + i \\sin(t\\operatorname{Im}\\lambda_i) \\bigr) \\to \\infty\\]\nas \\(t\\to\\infty\\). Otherwise, the equilibrium is stable (not asymptotically). The extra polynomial terms are required when the geometrical multiplicity of \\(\\lambda_i\\) is strictly lower than its algebraic multiplicity. So, in the general case we replace 3. above with\n\nIf \\(\\operatorname{Re}\\lambda_i \\le 0\\) for all \\(i=1,\\ldots,n\\), and for those with \\(\\operatorname{Re}\\lambda_i = 0\\) the algebraic and geometrical multiplicity coincide, then the equilibrium is stable.\n\nConcerning asymptotic stability, it is possible to find algebraic conditions on the coefficients of \\(\\mathbf{A}\\) such that \\(\\operatorname{Re}\\lambda_i &lt; 0\\) for all eigenvalues, without actually computing them.\n\n\nA.5.4.3 The \\(n=2\\) case\nThe characteristic polynomial of the matrix \\[\n\\mathbf{A} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\n\\]\nis as follows \\[\n\\mathcal{P}(\\lambda) = \\lambda^2 - (a+b)\\lambda + (ad-bc) = \\lambda^2 - \\operatorname{tr}(\\mathbf{A})\\lambda + \\det(\\mathbf{A}).\n\\]\nwhere we introduced the trace \\(\\operatorname{tr}(\\mathbf{A}) = a + b\\) and the determinant \\(\\det(\\mathbf{A}) = ad - bc\\).\n\nProposition A.5 The condition \\(\\operatorname{Re}\\lambda_i &lt; 0\\) for all \\(i=1,\\ldots,n\\) is equivalent to \\[\n\\operatorname{tr}(\\mathbf{A}) &lt; 0, \\quad\\text{and}\\quad\n\\det(\\mathbf{A}) &gt; 0.\n\\]\n\nThe proof is simple. We also know that: \\[\n\\begin{aligned}\n\\operatorname{tr}(\\mathbf{A}) &= \\lambda_1 + \\lambda_2, \\\\\n\\det(\\mathbf{A}) &= \\lambda_1 \\lambda_2,\n\\end{aligned}\n\\]\nbecause the above polynomial has always 2 roots on \\(\\mathbb{C}\\), so it admits a factorization \\(\\mathcal{P}(\\lambda) = (\\lambda - \\lambda_1)(\\lambda - \\lambda_2)\\) that expanded gives the equality.\nProposition¬†A.5 is one of the most important results for the course, and we will use it very often.\nThe case \\(n=2\\) is also very common in applications, thus it is worth studying in depth the equilibria in the phase space. We suppose that the equilibrium is the origin, that is we study the equation \\(\\mathbf{y}'=\\mathbf{A}\\mathbf{y}\\).\n\n\n\n\n\n\nExample (real and distinct eigenvalues)\n\n\n\nSuppose that the eigenvalues \\(\\lambda_1\\) and \\(\\lambda_2\\) of the matrix \\(\\mathbf{A}\\) are real, with eigenvectors \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\). Then the solution is: \\[\n\\mathbf{y}(t) = c_1 e^{t\\lambda_1}\\mathbf{v}_1 + c_2 e^{t\\lambda_2}\\mathbf{v}_2,\n\\] where \\(\\mathbf{c} = \\begin{bmatrix} c_1 \\\\ c_2 \\end{bmatrix} = \\mathbf{V}^{-1}\\mathbf{y}_0\\) and \\(\\mathbf{V}\\) is matrix with the eigenvectors. Thus, the solution is a linear combination of the eigenvectors.\nWhen \\(\\mathbf{y}_0 = \\alpha \\mathbf{v}_1\\), then \\(c_1=\\alpha\\) and \\(c_2=0\\), so we stay along the line with direction \\(\\mathbf{v}_1\\).Along this line, the solution is \\[\n\\mathbf{y}(t) = \\alpha e^{t\\lambda_1} \\mathbf{v}_1,\n\\]\nthus \\(\\mathbf{y}(t)\\) stays along the line and, when \\(\\lambda_1 &lt;0\\), approaches the equilibrium for \\(t\\to \\infty\\). In this case, we call the space generated by \\(\\mathbf{v}_1\\), the stable manifold of the equilibrium. Viceversa, for \\(\\lambda_1 &gt; 0\\), the trajectory diverges from the equilibrium; in this case, \\(\\mathbf{v}_1\\), is the unstable manifold.\nThe same applies to \\(\\mathbf{v}_2\\). So, when \\(\\lambda_1&lt;0\\) and \\(\\lambda_2 &lt; 0\\), the equilibrium is globally stable, with stable manifold is simply \\(\\mathbf{R}^2\\). The equilibrium is a stable node. When the eigenvalues are both positive, we have the opposite behavior, and the equilibrium is an unstable node. Finally, when they have opposite sign, the equilibrium is a saddle.\n\n\n\n\nA.5.4.4 The \\(n &gt; 2\\) case: Routh-Hurwitz criteria\nIn general, for \\(n&gt;2\\), we can find a set of algebraic conditions ensuring the asymptotic stability, without computing the eigenvalues.\nWe have the following\n\nTheorem A.7 Given the polynomial \\[\n\\mathcal{P}(\\lambda) = \\lambda^n + a_1 \\lambda^{n-1} + \\cdots + a_{n-1}\\lambda + a_n,\n\\]\nwhere the coefficients \\(a_i\\) are real constants, \\(i=1,\\ldots,n\\), define the \\(n\\) Hurwitz matrices using the coefficients \\(a_i\\) of the characteristic polynomial: \\[\n\\mathbf{H}_1 = \\begin{pmatrix} a_1 \\end{pmatrix},\n\\quad\n\\mathbf{H}_2 = \\begin{pmatrix} a_1 & 1 \\\\ a_3 & a_2 \\end{pmatrix},\n\\quad\n\\mathbf{H}_3 = \\begin{pmatrix} a_1 & 1 & 0 \\\\ a_3 & a_2 & a_1 \\\\ a_5 & a_4 & a_3 \\end{pmatrix},\n\\]\nand in general \\[\n\\mathbf{H}_n = \\begin{pmatrix}\na_1 &   1 &   0 &   0 & \\cdots & 0 \\\\\na_3 & a_2 & a_1 &   1 & \\cdots & 0 \\\\\na_5 & a_4 & a_3 & a_2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  0 &   0 &   0 &   0 & \\cdots & a_n\n\\end{pmatrix},\n\\]\nwhere \\(a_j = 0\\) if \\(j&gt;n\\). All the roots of the polynomial \\(\\mathcal{P}(\\lambda)\\) are negative or have negative real part if and only if the determinants of all Hurwitz matrices are positive: \\[\n\\det\\mathbf{H}_j &gt; 0, \\quad j=1,2,\\ldots,n.\n\\]\n\nWe can specialize the Theorem for low \\(n\\). We have\n\n\\(n=2\\): \\(\\det\\mathbf{H}_1 = a_1 &gt; 0\\) and \\(\\det\\mathbf{H}_2 = a_1a_2 &gt; 0\\). This is equivalent to \\(a_1&gt;0\\) and \\(a_2&gt;0\\). Note that \\(a_1 = -\\operatorname{tr}\\mathbf{A}\\) and \\(a_2 = \\det\\mathbf{A}\\).\n\\(n=3\\): as above, \\(a_1&gt;0\\), \\(a_1 a_2 - a_3 &gt; 0\\), and \\(\\det\\mathbf{H}_3 = a_3\\det\\mathbf{H}_2 &gt; 0\\). So, this is equivalent to \\(a_i &gt; 0\\) and \\(a_1a_2 - a_3 &gt; 0\\).\n\\(n=4\\): it is possible to show that we need \\(a_1&gt;0\\), \\(a_3 &gt; 0\\), \\(a_4 &gt; 0\\) and \\(a_1 a_2 a_3 &gt; a_3^2 + a_1^2 a_4\\).\n\nWe will make use of the case \\(n=3\\).\n\n\nA.5.4.5 Other useful cases\nSometimes we are going to deal with \\(4\\times 4\\) or larger matrices. Very often, though, they have a block structure. For instance: \\[\n\\mathbf{A} = \\begin{pmatrix}\n\\mathbf{A}_1 & \\mathbf{C} \\\\\n\\mathbf{0}   & \\mathbf{A}_2\n\\end{pmatrix},\n\\]\nwhere \\(\\mathbf{A}_1\\) and \\(\\mathbf{A}_2\\) can differ in dimension. Then, the eigenvalues of \\(\\mathbf{A}\\) are the union of the eigenvalues of \\(\\mathbf{A}_1\\) and \\(\\mathbf{A}_2\\). A specific case is the triangular matrix, where eigenvalues are on the diagonal.\n\n\n\nA.5.5 Linearization method\nThe analysis of stability quickly becomes impractical for complex non-linearities. The study of local stability properties, however, can be carried out fairly easily. Suppose that \\(\\mathbf{y}^*\\in\\mathbb{R}^n\\) is an equilibrium of the dynamical system \\((\\mathbf{f},\\Omega)\\), with \\(\\mathbf{f}\\in\\mathcal{C}^1(\\Omega)\\). Then \\[\\mathbf{f}(\\mathbf{y}) = \\mathbf{f}(\\mathbf{y}^*) + D\\mathbf{f}(\\mathbf{y}^*)(\\mathbf{y}-\\mathbf{y}^*) + \\ldots,\\] where \\(D\\mathbf{f}(\\mathbf{y}^*)\\) is the Jacobian of \\(\\mathbf{f}\\). Since \\(\\mathbf{f}(\\mathbf{y}^*) = 0\\), we have the following linear approximation of the dynamical system: \\[\\mathbf{y}' = D\\mathbf{f}(\\mathbf{y}^*)(\\mathbf{y}-\\mathbf{y}^*) + \\ldots\\] We now consider \\(\\mathbf{z}(t) = \\mathbf{y}(t) - \\mathbf{y}^*\\), and notice that \\(\\mathbf{z}' = \\mathbf{y}'\\), to obtain: \\[\\mathbf{z}' = \\mathbf{A}\\mathbf{z},\\] where we set \\(\\mathbf{A}=D\\mathbf{f}(\\mathbf{y}^*)\\). The dynamical system in \\(\\mathbf{z}\\) is linear, and we know how to analyze the stability of the equilibrium \\(\\mathbf{z}=0\\). (This corresponds to the equilibrium \\(\\mathbf{y}^*\\) in the original variables.) In fact, let \\(\\{\\lambda_i \\}\\) be the eigenvalues of \\(D\\mathbf{f}(\\mathbf{y}^*)\\). Then:\n\nIf \\(\\operatorname{Re}\\lambda_i &lt; 0\\) for all \\(i=1,\\ldots,r\\), then \\(\\mathbf{y}^*\\) is locally asymptotically stable.\nIf there exists \\(i\\in\\{1,\\ldots,r\\}\\) such that \\(\\operatorname{Re}\\lambda_i &gt; 0\\), the equilibrium is locally unstable.\n\nWe cannot conclude anything regarding the case \\(\\operatorname{Re}\\lambda_i = 0\\), because in this case higher order terms in the Taylor expansion dictates the local dynamics of the system.\n\n\n\n\n\n\nExample (logistic equation, alternative analysis)\n\n\n\nWe consider again the ODE \\(y'=y(1-y)\\). Given \\(f(y)=y(1-y)\\), we have \\(f'(y)=1-2y\\). At the first equilibrium, \\(f'(0)=1 &gt; 0\\), so it is locally unstable. For the second one, \\(f'(1)=-1&lt;0\\), so it is locally asympotically stable.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Introduction to ODEs</span>"
    ]
  },
  {
    "objectID": "IntroODE.html#periodic-orbits",
    "href": "IntroODE.html#periodic-orbits",
    "title": "Appendix A ‚Äî Introduction to ODEs",
    "section": "A.6 Periodic orbits",
    "text": "A.6 Periodic orbits\nSo far, we went is more great detail in the study of equilibria of a dynamical system. Equilibria as ‚Äúsimple‚Äù, in the sense that we can find all of them as solution of the nonlinear system \\(\\mathbf{f}(\\mathbf{y}) = \\mathbf{0}\\). Their local stability follows from the linearization of the ODE around the equilibrium.\nPeriodic orbits are more difficult to characterize, but they are extremely important in applications.\n\nA.6.1 Hamiltonian systems\nConsider a system of 2 ODEs, written in general as \\[\n\\left\\{\\begin{aligned}\ny_1' &= f_1(y_1,y_2), \\\\\ny_2' &= f_2(y_1,y_2).\n\\end{aligned}\\right.\n\\]\nThe phase space is planar. Let us introduce the vector field \\[\n\\mathbf{f}(y_1,y_2) = \\begin{pmatrix} f_1(y_1,y_2) \\\\ f_2(y_1,y_2) \\end{pmatrix},\n\\]\nand suppose that given \\(\\Omega\\subset\\mathbb{R}^2\\) we have a dynamical system \\((\\mathbf{f},\\Omega)\\). We introduce another vector field: \\[\n\\mathbf{G}(y_1,y_2) = \\begin{pmatrix} -f_2(y_1,y_2) \\\\ f_1(y_1,y_2) \\\\ 0 \\end{pmatrix},\n\\]\nwith the following associated differential form \\[\n\\omega(y_1,y_2) = -f_2(y_1,y_2)\\mathrm{d}y_1 + f_1(y_1,y_2)\\mathrm{d}y_2.\n\\]\nRecall the following interpretation of a differential form: given an infinitesimal displacement \\(\\mathrm{d}\\mathbf{y}\\) in \\(\\mathbb{R}^3\\), the quantity \\(\\omega = \\langle\\mathbf{G},\\mathrm{d}\\mathbf{y}\\rangle\\) is the infinitesimal work done by \\(\\mathbf{G}\\) along \\(\\mathrm{d}\\mathbf{y}\\). A \\(\\mathcal{C}^1(\\Omega)\\) differential form is exact when there exists a \\(\\mathcal{C}^2(\\Omega)\\) function \\(H: \\Omega\\to\\mathbb{R}\\) such that \\(\\mathrm{d}H = \\omega\\) in \\(\\Omega\\). In particular, \\[\n\\mathrm{d}H = \\langle\\nabla H, \\mathrm{d}\\mathbf{y}\\rangle = \\langle\\mathbf{G},\\mathrm{d}\\mathbf{y}\\rangle = \\omega,\n\\]\nso we conclude that \\(\\nabla H = \\mathbf{G}\\). We call \\(H\\) potential or Hamiltonian function.\nA simple necessary condition for exactness is that the curl of \\(\\mathbf{G}\\) cancels, since \\(\\nabla\\times\\mathbf{G} = \\nabla\\times\\nabla H = \\mathbf{0}\\), so \\[\n\\nabla\\times \\mathbf{G}(y_1,y_2) = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\frac{\\partial f_1(y_1,y_2)}{\\partial y_1} + \\frac{\\partial f_2(y_1,y_2)}{\\partial y_2} \\end{pmatrix} = \\mathbf{0}.\n\\]\nThe condition \\[\n\\frac{\\partial f_1(y_1,y_2)}{\\partial y_1} + \\frac{\\partial f_2(y_1,y_2)}{\\partial y_2} = 0,\n\\]\nis also sufficient for \\(\\omega\\) being exact when \\(\\Omega\\) is simply connected. In conclusion, if \\(\\nabla\\times\\mathbf{G}=\\mathbf{0}\\) and \\(\\Omega\\) is simply connected, then there exists a Hamiltonian function \\(H(y_1,y_2)\\) such that: \\[\n\\left\\{\\begin{aligned}\ny_1' &= \\frac{\\partial H}{\\partial y_2}(y_1,y_2), \\\\\ny_2' &= -\\frac{\\partial H}{\\partial y_1}(y_1,y_2).\n\\end{aligned}\\right.\n\\]\nODEs of the above form are called Hamiltonian systems. They are very common in mechanics.\nFor a Hamiltonian system, orbits are level sets of the function \\(H\\). Indeed, given a trajectory \\(\\mathbf{y}(t)\\) of the ODE, we have that the Hamiltonian function along the trajectory is constant: \\[\n\\frac{\\mathrm{d}}{\\mathrm{d}t} H\\bigl(y_1(t),y_2(t)\\bigr) =\n\\frac{\\partial H}{\\partial y_1} y_1'(t) + \\frac{\\partial H}{\\partial y_2} y_2'(t) =\n\\frac{\\partial H}{\\partial y_1} \\frac{\\partial H}{\\partial y_2} - \\frac{\\partial H}{\\partial y_2} \\frac{\\partial H}{\\partial y_1} = 0.\n\\]\nThus, \\(H\\bigl(y_1(t),y_2(t)\\bigr) = \\text{constant}\\) for \\(t\\ge 0\\). Given \\(H(y_1,y_2)\\), we can easily draw the phase portrait.\n\n\n\n\n\n\nExample (conservative systems)\n\n\n\nConsider the ODE \\[\nm x'' = F(x),\n\\]\nwhere \\(m\\) is the mass of a particle and \\(F(x)\\) is a force. We can recast this into a system: \\[\n\\left\\{\\begin{aligned}\ny_1' &= \\tfrac{1}{m} y_2, \\\\\ny_2' &= F(y_1),\n\\end{aligned}\\right.\n\\]\nwhere \\(y_1 = x\\) is the position and \\(y_2 = m x'\\) is the momentum.\nWhen the force is conservative, there exists a potential energy \\(U(x)\\) such that \\(F = -U'\\). This implies the existence of a Hamiltonian function \\[\nH(y_1,y_2) = \\frac{1}{2m} y_2^2 + \\frac{1}{m} U(y_1),\n\\]\nwhich is exactly the total energy, the sum of kinetic and potential energy. This quantity is conserved and the orbits are level sets of \\(H = H_0\\), where \\(H_0 = H(y_1(0),y_2(0))\\) is the initial energy of the particle.\nTake for instance the Lennard-Jones potential below:\n\n\n\nLennard-Jones potential\n\n\nWe observe that if the initial energy is between \\(-1\\) and \\(0\\), then we have closed orbits. These corresponds to periodic solutions. When the initial energy is positive, then orbits are open.\n\n\n\n\n\n\n\n\nExercise (Double-well potential)\n\n\n\nFind a Hamiltonian for the ODE \\[\nx'' = x - x^3,\n\\]\nand show how orbits vary as \\(H_0\\) increases.\n\n\n\n\n\n\n\n\nExample (Lotka-Volterra system)\n\n\n\nConsider the system: \\[\n\\left\\{\\begin{aligned}\ny_1' &= y_1(a-by_2), \\\\\ny_2' &= y_2(-c + dy_1).\n\\end{aligned}\\right.\n\\]\nwhere \\(y_1(t)\\) and \\(y_2(t)\\) are respectively the prey and the predator density. We will study this system more in depth. We can find the Hamiltonian in the positive quadrant as follows. Note that \\[\n\\frac{\\mathrm{d}y_1}{\\mathrm{d}y_2} = \\frac{y_1(a-by_2)}{y_2(-c + dy_1)},\n\\]\nso we can separate the variables: \\[\n\\frac{(-c + dy_1)\\mathrm{d}y_1}{y_1} = \\frac{(a-by_2)\\mathrm{d}y_2}{y_2},\n\\]\nand integrate: \\[\n\\int_{y_1(0)}^{y_1(t)} \\Bigl(-\\frac{c}{y} + d\\Bigr)\\mathrm{d}y = \\int_{y_2(0)}^{y_2(t)} \\Bigl( \\frac{a}{y_2} - b \\Bigr) \\mathrm{d}y_2,\n\\]\nwe obtain that \\(H(y_1(t),y_2(t)) = H(y_1(0),y_2(0))\\) with \\[\nH(y_1,y_2) = -c \\ln y_1 + dy_1 - a\\ln y_2 + by_2.\n\\]\nThis function has a minimum at \\((y_1^*,y_2^*) = (\\frac{a}{b}, \\frac{c}{d})\\), since \\(\\nabla H(y_1^*,y_2^*) = 0\\) and the Hessian is positive definite. For \\(y_1\\) or \\(y_2\\to 0\\) we have \\(H\\to+\\infty\\), as well for \\(y_{1,2}\\to+\\infty\\). So, the function has level set that are closed curves, and those are associated with periodic orbits.\n\n\n\n\nA.6.2 Isolated periodic orbits\nWe have seen that periodic orbits are possible in planar systems. In all the above cases, however, periodic orbits are packed, that is if we have an orbit for an energy level \\(H_0\\), then we can have another periodic orbit infinitesimally close to the original one. Is it possible to have isolated periodic orbits? In other words, is it possible that there exists an open set \\(\\Omega'\\subseteq\\Omega\\) such that \\(\\Omega'\\) contains only one periodic orbit? If so, we call this orbit a limit cycle.\n\n\n\n\n\n\nExample\n\n\n\nThe ODE \\[\n\\left\\{\\begin{aligned}\ny_1' &= y_2, \\\\\ny_2' &= y_2(1-y_1^2+y_2^2) - y_1.\n\\end{aligned}\\right.\n\\]\nhas an isolated limit cycle for \\(y_1(t)^2 + y_2(t)^2 = 1\\). To see this, first note that \\((y_1(t),y_2(t)) = (\\cos t,\\sin t)\\) is a (periodic) solution. Now we check the stability.\nTake the function \\[\nE(y_1,y_2) = \\frac{1}{2}(y_1^2 + y_2^2),\n\\]\nand note that \\[\n\\frac{\\mathrm{d}}{\\mathrm{d}t} E\\bigl(y_1(t),y_2(t)\\bigr) =\n\\bigl(1- y_1(t)^2 - y_2(t)^2\\bigr)y_2(t)^2.\n\\]\nThus, if \\(y_1^2 + y_2^2 \\le 1\\) we have \\(E'(t) \\ge 0\\), whereas if \\(y_1^2 + y_2^2 \\ge 1\\) we have \\(E'(t)\\le 0\\). Hence, the limit cycle is attracting orbits.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Introduction to ODEs</span>"
    ]
  },
  {
    "objectID": "IntroODE.html#limit-cycles",
    "href": "IntroODE.html#limit-cycles",
    "title": "Appendix A ‚Äî Introduction to ODEs",
    "section": "A.7 Limit cycles",
    "text": "A.7 Limit cycles\nWe have seen that periodic orbits are possible in planar systems. In all the above cases, however, periodic orbits are packed, that is if we have an orbit for an energy level \\(H_0\\), then we can have another periodic orbit infinitesimally close to the original one. Is it possible to have isolated periodic orbits? In other words, is it possible that there exists an open set \\(\\Omega'\\subseteq\\Omega\\) such that \\(\\Omega'\\) contains only one periodic orbit? If so, we call this orbit a limit cycle.\n\nA.7.1 Dulac‚Äôs criterium\nWe have the following result of non-existence of limit cycles:\n\nTheorem A.8 (Dulac‚Äôs criterium) Let \\(\\mathbf{y}' = \\mathbf{f}(\\mathbf{y})\\) a planar dynamical system with \\(\\mathbf{f}\\in\\mathcal{C}^1(\\Omega)\\), \\(\\Omega\\subset\\mathbb{R}^2\\) open. If there exists a function \\(h\\in\\mathcal{C}^1(\\Omega)\\) such that \\(\\operatorname{div}(h\\mathbf{f})\\) does not change sign in \\(\\Omega'\\subseteq\\Omega\\) simply connected, then there exists no limit cycle in \\(\\Omega'\\).\n\nNote that the theorem applies also with \\(h\\equiv 1\\). The proof is simple. Take a closed orbit entirely lying in the region \\(\\Omega'\\). We denote by \\(C\\subset\\Omega'\\) the interior of the orbit whose boundary \\(\\partial C\\) is the orbit itself. Then \\[\n\\int_{C} \\operatorname{div}(h\\mathbf{f})\\mathrm{d}\\mathbb{y} = \\int_{\\partial C} h\\langle \\mathbf{f},\\mathbf{n}\\rangle \\mathrm{d}s.\n\\]\nSince \\(\\partial C\\) is an orbit, \\(\\langle \\mathbf{f},\\mathbf{n}\\rangle = 0\\), so the second integral is zero, no matter of the choice of \\(h\\). But the first integral cannot be zero, because \\(\\operatorname{div}(h\\mathbf{f})\\) has a constant sign in \\(C\\). The contraction yields to the non-existence of such a periodic orbit.\nUnfortunately there is no algorithm for finding the function \\(h\\). Usually is of the form \\((y_1y_2)^{-1}\\), \\(e^{y_1}\\), \\(e^{y_2}\\), ‚Ä¶\n\n\n\n\n\n\nExample\n\n\n\nConsider the ODE: \\[\n\\left\\{\\begin{aligned}\ny_1' &= y_1(2-y_1-y_2), \\\\\ny_2' &= y_2(4y_1-y_1^2-3).\n\\end{aligned}\\right.\n\\]\nWe can show that the system has no closed orbits in the positive quadrant. We take \\(h(y_1,y_2) = 1/(y_1 y_2)\\). Then: \\[\n\\nabla\\cdot(h\\mathbf{f}) = \\frac{\\partial}{\\partial y_1}\\left(\\frac{2-y_1-y_2}{y_2}\\right) + \\frac{\\partial}{\\partial y_2}\\left( \\frac{4y_1-y_1^2-3}{y_1}\\right) = -\\frac{1}{y_2} &lt; 0.\n\\]\nThe region \\(\\mathrm{R}^2_+\\) is simply connected, and the functions are smooth, so there are no limit cycles in it.\n\n\n\n\nA.7.2 Poincar√©-Bendixson existence theorems\n\nTheorem A.9 (Poincar√©-Bendixson) Suppose that \\((\\mathbf{f},\\Omega)\\) is a planar dynamical system, \\(\\Omega\\subset\\mathbb{R}^2\\), with \\(\\mathbf{f}\\in\\mathcal{C}^1(\\Omega)\\). If there exists a closed and bounded region \\(R\\subseteq\\Omega\\) that does not contain any equilibrium and there exists a trajectory \\(C\\) that is ‚Äúconfined‚Äù in \\(D\\), in the sense that it starts in \\(D\\) and stays in \\(D\\) for all future time, then either \\(C\\) is a closed orbit, or it spirals toward a closed orbit at \\(t\\to\\infty\\). In either case, \\(D\\) contains a closed orbit.\n\nIn order to apply the theorem, we need to find a ‚Äútrapped‚Äù orbit in \\(D\\). How to proceed? The idea is the following: we can construct a region \\(D\\) that is closed and connected, and such that its boundary is inflow for the dynamical system. That is, the vector field \\(\\mathbf{f}\\) only enters into \\(D\\). So, orbits cannot leave.\n\n\n\nTrapping region\n\n\nSome comments:\n\nThe requirement for no equilibria is important. Very often we do have an equilibrium inside (as we are going to see in a moment, a limit cycle always has an equilibrium inside of it), but we can remove it by ‚Äúdigging‚Äù a hole around it.\nEquilibria cannot be on the border of the region. Consider a homoclinic orbit as in the figure below. We see that the ‚Äúpunctured‚Äù region traps an orbit \\(C\\), but there are no limit cycles inside. The orbit tends to the homoclinic.\n\n\n\n\n\n\n\nFigure¬†A.1: Homoclinic orbit\n\n\n\nWe can be more precise. Let us define\n\nDefinition A.15 (\\(\\alpha\\)-limit and \\(\\omega\\)-limit sets) We define the \\(\\alpha\\)-limit and the \\(\\omega\\)-limit of an initial condition \\(\\mathbf{y}_0\\) as the set of \\(\\Omega\\) that the trajectory approches as \\(t\\to-\\infty\\) and \\(t\\to\\infty\\), respectively: \\[\n\\begin{aligned}\n\\alpha(\\mathbf{y}_0) &= \\lim_{t\\to-\\infty} \\boldsymbol{\\Phi}(t,\\mathbf{y}_0), \\\\\n\\omega(\\mathbf{y}_0) &= \\lim_{t\\to+\\infty} \\boldsymbol{\\Phi}(t,\\mathbf{y}_0).\n\\end{aligned}\n\\]\n\nThen, we have the following\n\nTheorem A.10 (Poincar√©-Bendixson Trichotomy) Suppose that \\((\\mathbf{f},\\Omega)\\) is a planar dynamical system, \\(\\Omega\\subset\\mathbb{R}^2\\), with \\(\\mathbf{f}\\in\\mathcal{C}^1(\\Omega)\\). Let \\(C^+(\\mathbf{y}_0) = \\{ \\boldsymbol{\\Phi}(t,\\mathbf{y}_0), t\\ge 0\\}\\) a positive orbit that remains in a closed, bounded region \\(R\\subseteq\\Omega\\) that contains only a finite number of equilibria. Then the \\(\\omega\\)-limit set takes one of the following three forms:\n\n\\(\\omega(\\mathbf{y}_0)\\) is an equilibrium,\n\\(\\omega(\\mathbf{y}_0)\\) is a periodic orbits,\n\\(\\omega(\\mathbf{y}_0)\\) is a singular cycle, that is \\(\\omega(\\mathbf{y}_0)\\) contains a finite number of equilibria and a set of orbits whose \\(\\alpha\\)- and \\(\\omega\\)-limit sets consist of one these equilibria for each orbit.\n\n\nThe point 3. is for instance a heteroclinic, which contains two saddle equilibria connected by the stable and unstable manifolds, or a homoclinic orbit, which is a single saddle equilibrium where the stable and unstable manifolds correspond (for \\(t\\ge 0\\)).\nThe proofs of these theorems heavily rely on concepts from topology. Here, we only recall index theory for a planar dynamical system.\n\n\n\n\n\n\nExample (glycolysis)\n\n\n\nWe consider a model for glycolysis as proposed by Sel‚Äôkov (1968). In yeast cells glycolysis can proceed in a oscillatory fashion. Here is a model: \\[\n\\left\\{\\begin{aligned}\nx' &= -x + ay + x^2 y, \\\\\ny' &= b - ay - x^2 y,\n\\end{aligned}\\right.\n\\]\nwhere \\(x(t)\\) is the concentration of ADP (adenosine diphosphate) and \\(y(t)\\) is the concentration of F6P (fructose-6-phosphate). The parameters \\(a&gt;0\\) and \\(b&gt;0\\) comes from the kinetic of the reaction.\nThe phase portrait is as follows:\n\n\n\nTrapping region\n\n\nBy looking at the arrows, it looks like we could have a limit cycle or some stable equilibrium. We construct the trapping region as follows:\n\n\n\nTrapping region\n\n\nNow we check. Graphically, we note that\n\nAbove the nullcline \\(x'=0\\) we have \\(x'&gt;0\\), below \\(x'&lt;0\\).\nOn the left of the nullcline \\(y'=0\\) we have \\(y'&gt;0\\), on the right \\(y'&lt;0\\).\n\nThus, the horizontal and vertical parts in of the region are easy to check, just by looking at the arrows. The diagonal part is more subtle. Intuitively, when \\(x\\) and \\(y\\) are large, the ODE is approximately \\(x' \\approx x^2 y\\) and \\(y' \\approx -x^2 y\\). So, \\(y'/x' \\approx -1\\), which is tangent to the trajectories. This suggests to take: \\[\nx' - (-y') = -x +ay + x^2y + (b - ay -x^2y) = b - x.\n\\]\nHence, for \\(x&gt;b\\) we have \\(-y' &gt; x'\\), and trajectories are entering into the diagonal part, because their slope is more negative than \\(-1\\).\nWe conclude that \\(R\\) is a trapping region. However, we cannot apply the Theorem, because we still need to rule out equilibria. We have only one, at the intersection between nullclines. We can do so just by digging a hole around it, obtaining a ‚Äúpunctured‚Äù region.\n\n\n\nPunctured region\n\n\nFor a sufficiently small hole, we can ensure that orbits are leaving or entering the region just by looking at the stability of the equilibrium. If it is unstable, then orbits are diverging, and thus entering into \\(R\\). That is, \\(R\\) traps orbits and it does not contain equilibria. Therefore, there must exist a closed orbit in it!\nThe stability of the equilibrium is easy to check. First, the equilibrium is \\[\n(x^*,y^*) =  \\Bigl( b, \\frac{b}{a+b^2} \\Bigr).\n\\]\nThe Jacobian is: \\[\nJ_\\mathbf{f}(x,y) = \\begin{pmatrix} -1 + 2xy & a + x^2 \\\\ -2xy & -a-x^2 \\end{pmatrix}.\n\\]\nThe trace and the determinant at \\((x,y)=(x^*,y^*)\\): \\[\n\\det J=a+b^2&gt;0, \\quad\n\\operatorname{tr}J = -\\frac{b^4 + (2a-1)b^2 + (a+a^2)}{a+b^2}.\n\\]\nDepending on \\(a\\) and \\(b\\), the trace can be either positive or negative. Solving the numerator for \\(b^2\\) we have that the trace is zero for \\[\nb^2 = \\frac{1}{2}\\Bigl( 1 - 2a \\pm \\sqrt{1-8a} \\Bigr).\n\\]\nFor choice of the parameters such that the trace is positive, the equilibrium is unstable and the region contains a closed orbit.\n\n\n\n\nA.7.3 Index theory\n\nDefinition A.16 (index of a closed curve) We define the index of a closed curve \\(C\\), denoted by \\(i_C\\), as the net number of counterclockwise revolutions made by the vector field \\(\\mathbf{f}\\) around \\(C\\). More precisely, take the angle \\(\\phi(t)\\) between the tangent to the curve \\(C\\) and the vector field \\(\\mathbf{f}\\). Then, \\(\\phi(t)\\) has changed by an integer multiple of \\(2\\pi\\). Such integer is the index.\n\n\n\n\nIndex of a curve\n\n\nThe index closely resembles residue theory in complex analysis. For instance, we have that\n\nIf \\(C\\) can be continuously deformed into \\(C'\\) without passing through an equilibrium, then \\(i_C = i_{C'}\\). (Note that \\(i_C\\) is an integer-valued function of \\(C\\). The only way \\(i_C\\) can be a continuous function of \\(C\\) is being constant!)\nIf \\(C\\) does not enclose any equilibrium, then \\(i_C = 0\\). (Since \\(C\\) does not contain any zero of \\(\\mathbf{f}\\), we can shrink \\(C\\) to a tiny circle where \\(\\mathbf{f}\\) is almost constant, so the index is zero.)\nIf we reverse the arrows of the vector field, that is \\(t\\mapsto -t\\), the index is unchanged.\nSuppose that \\(C\\) is a closed orbit. Then \\(i_C = +1\\).\n\nMathematically, given the vector field \\(\\mathbf{f}(x,y) = (f(x,t), g(x,y))\\) in \\(\\mathbb{R}^2\\), the angle given by \\[\n\\phi = \\tan^{-1}(g/f),\n\\]\nso in terms of differentials we have: \\[\n\\mathrm{d}\\phi = \\mathrm{d}\\bigl(\\tan^{-1}(g/f)\\bigr) = \\frac{f\\mathrm{d}g - g\\mathrm{d}f}{f^2+g^2}.\n\\]\nThe index is, therefore, \\[\ni_C = \\frac{1}{2\\pi}\\int_C \\mathrm{d}\\phi = \\frac{1}{2\\pi}\\int_C \\frac{f\\mathrm{d}g - g\\mathrm{d}f}{f^2+g^2}.\n\\]\n\nDefinition A.17 (Index of an equilibrium) The index of an equilibrium \\(\\mathbf{y}^*\\), denoted by \\(i_{\\mathbf{y}^*}\\) is the index of a curve containing the equilibrium.\n\nNote that thanks to 1. the definition of \\(i_{\\mathbf{y}^*}\\) does not depend on the choice of \\(C\\).\n\nTheorem A.11 All hyperbolic equilibria, that is the Jacobian of \\(\\mathbf{f}\\) is non-singular at the equilibrium, have \\(i_{\\mathbf{y}^*} = \\pm 1\\). In particular, only the saddle has \\(i_{\\mathbf{y}^*} = -1\\).\n\nA non-hyperbolic equilibrium can also have index 0, for instance a saddle-node.\n\nTheorem A.12 If a closed curve \\(C\\) surrounds \\(n\\) isolated equilibria \\(\\mathbf{y}_1^*, \\mathbf{y}_2^*, \\ldots,\\mathbf{y}_n^*\\), then \\[\ni_C = \\sum_{k=1}^n i_{\\mathbf{y}_n^*}.\n\\]\n\nThe proof is familiar from multivariate calculus. We shrink the curve \\(C\\) so to surround the equilibria with tiny circles and corridors connecting them. The index does not change. Then we show that the contribution of corridors cancels out, because of symmetry reasons. So, we are only left with circles around the equilibria, which is the sum of the indices of each equilibrium.\n\nTheorem A.13 (Poincar√©) Any closed periodic orbit in the phase plane must enclose equilibria whose indices sum to \\(+1\\). If all equilibria are hyperbolic, then the limit cycle contains exactly \\(2s + 1\\) equilibria, \\(s\\) saddles and \\(s+1\\) non saddles, with \\(s\\ge 0\\) integer.\n\n\n\n\n\n\n\nExample\n\n\n\nConsider the system: \\[\n\\left\\{\\begin{aligned}\ny_1' &= -y_1 - y_1^2 y_2^2, \\\\\ny_2' &= y_1 + y_2.\n\\end{aligned}\\right.\n\\]\nWe show now that there exists no limit cycle around the origin.\nFirst, notice that the divergence of \\(\\mathbf{f}\\) is: \\[\n\\operatorname{div}\\mathbf{f} = -2y_1 y_2^2,\n\\]\nwhich changes sign in \\(\\mathbb{R}^2\\), so we cannot exclude the existence of a limit cycle.\nWe have 2 equilibria, \\(\\mathbf{y}_1^* = (0,0)\\) and \\(\\mathbf{y}_2^* = (-1,1)\\). In particular, \\(\\mathbf{y}_1^*\\) is a saddle. A limit cycle around the origin must contain the saddle, but the index of the cycle is \\(+1\\), whereas the index of the saddle is \\(-1\\). It cannot be. Even the case of a limit cycle surrounding both equilibria is not possible, because the sum of the indices is \\(0\\). We conclude that we cnnot have a limit cycle around the origin.\n\n\n\n\nA.7.4 Poincar√© maps\nWe still need to discuss the stability of limit cycles. Since they are isolated by definition, they can attract or repulse nearby orbits.\nConsider a periodic orbit \\(\\Gamma\\subset\\mathbb{R}^n\\), and take any point \\(\\mathbf{y}_0\\in\\Gamma\\). We introduce a cross-section \\(\\Sigma\\) to the cycle at this point as a smooth hypersurface of dimension \\(n-1\\) that crosses \\(\\Gamma\\) at non-zero angle. A non-zero angle means that the normal of the hypersurface at \\(\\mathbf{y}_0\\) is not orthogonal to the tangent of \\(\\Gamma\\) at \\(\\mathbf{y}_0\\).\nNote that the limit cycle reduces to a point on the cross-section \\(\\Sigma\\). Other closeby orbits will also intersect the cross-section \\(\\Sigma\\) at some angle (by continuity the angle remains non-zero in a neighborhood of \\(\\mathbf{y}_0\\)). A point \\(\\mathbf{z}\\in\\Sigma\\), close to \\(\\mathbf{y}_0\\), will give some orbit through the flow of the ODE, and eventually return to \\(\\Sigma\\) after some time at a point \\(\\tilde{\\mathbf{z}}\\). This procedure is encoded in a map \\(\\mathbf{z}\\mapsto \\tilde{\\mathbf{z}} = \\mathcal{P}(\\mathbf{z})\\).\nDefinition (Poincar√© map). We call \\(\\mathcal{P}:\\Sigma\\to \\Sigma\\) the Poincar√© map of the periodic orbit \\(\\Gamma\\).\n\n\n\nPoincar√© maps\n\n\nA fixed point \\(\\mathbf{z}^*\\) of the map \\(\\mathcal{P}\\) corresponds to a limit cycle for the original system. Specifically, if \\(\\mathbf{z}^*\\) is a stable equilibrium for the discrete map \\[\n\\mathbf{z}^{(k+1)} = \\mathcal{P}(\\mathbf{z}^{(k)}),\n\\]\nthat is, \\(\\mathbf{z}^* = \\mathcal{P}(\\mathbf{z}^*)\\), then the limit cycle is stable. If \\(\\mathbf{z}^*\\) is asympotically stable, then the limit cycle is asymptotically stable. Otherwise, it is unstable.\n\n\n\n\n\n\nExample\n\n\n\nConsider the following ODE is polar coordinates: \\[\n\\left\\{\\begin{aligned}\nr'     &= r(1-r^2), \\\\\n\\theta' &= 1.\n\\end{aligned}\\right.\n\\]\nSince \\(\\theta' = 1\\), we make a turn after a period of \\(2\\pi\\). Thus, if we start from \\(r_0\\), we return at \\(r_1\\) after \\(\\Delta t = 2\\pi\\). Hence: \\[\n\\int_{r_0}^{r_1} \\frac{\\mathrm{d}r}{r(1-r^2)} = \\int_0^{2\\pi}\\mathrm{d}t = 2\\pi.\n\\]\nIntegrating we get: \\[\n\\mathcal{P}(r) = \\frac{1}{\\sqrt{1+e^{-4\\pi}(r^{-2}-1)}}.\n\\]\nFor \\(r^* = 1\\) we have a fixed point. We can show (graphically with the cobweb plot) that the equilibrium is attractive.\n\n\nGiven a parametrization of \\(\\mathcal{P}\\) in some local coordinate system \\(\\boldsymbol{\\xi} = (\\xi_1, \\ldots, \\xi_{n-1})\\) such that \\(\\boldsymbol{\\xi}=\\mathbf{0}\\) corresponds to \\(\\mathbf{z}^*\\), the origin is a fixed point of the map: \\(\\mathcal{P}(\\mathbf{0}) = \\mathbf{0}\\). Thanks to the contraction Theorem, the fixed-point is asymptotically stable if \\(\\mathcal{P}\\) is a contraction, which means that \\[\n\\mathbf{B} = \\left.\\frac{\\mathrm{d}\\mathcal{P}}{\\mathrm{d}\\boldsymbol{\\xi}}\\right|_{\\boldsymbol{\\xi}=\\mathbf{0}}\n\\]\nhas eigenvalues (multipliers) \\(\\mu_1, \\ldots, \\mu_{n-1}\\) with modulus strictly less than 1, that is \\(|\\mu_i|&lt;1\\). It is possible to show that the multipliers are independent on the choice of the cross-section, the local parametrization, and the point \\(\\mathbf{y}_0\\in \\Gamma\\). In other words, the multipliers are a characteristic of the limit cycle, from which we can deduce the stability.\nWe can now formalize the stability for a limit cycle. Consider a periodic solution \\(\\boldsymbol{\\phi}(t)\\) of the dynamical system \\[\n\\mathbf{y}' = \\mathbf{f}(\\mathbf{y}),\n\\]\nthat is \\(\\boldsymbol{\\phi}(t)\\) is a solution and \\(\\boldsymbol{\\phi}(t+T_0) = \\boldsymbol{\\phi}(t)\\) for some \\(T_0&gt;0\\), called the period of the limit cycle. Now we perturb the periodic orbit: \\[\n\\tilde{\\boldsymbol{\\phi}}(t) = \\boldsymbol{\\phi}(t) + \\mathbf{u}(t),\n\\]\nfor some perturbation \\(\\mathbf{u}(t)\\). Now notice that: \\[\n\\mathbf{u}'(t) = \\tilde{\\boldsymbol{\\phi}}(t) - \\boldsymbol{\\phi}(t) = \\mathbf{f}(\\boldsymbol{\\phi}(t) + \\mathbf{u}(t)) - \\mathbf{f}(\\boldsymbol{\\phi}(t)) = \\mathbf{A}(t)\\mathbf{u}(t) + \\mathcal{O}(\\| \\mathbf{u}(t) \\|^2 ),\n\\]\nwhere \\(\\mathbf{A} = J_\\mathbf{f}(\\boldsymbol{\\phi}(t))\\), and \\(\\mathbf{A}(t+T_0) = \\mathbf{A}(t)\\). The system \\(\\mathbf{u}' = \\mathbf{A}(t)\\mathbf{u}\\) is non-autonomous and linear. The set of solutions form the Wronskian matrix \\(\\mathbf{W}(t)\\) and are such that \\[\n\\mathbf{W}' = \\mathbf{A}(t)\\mathbf{W},\n\\]\nwith initial condition \\(\\mathbf{W}(0) = \\mathbf{I}\\). Any solution of \\(\\mathbf{u}' = \\mathbf{A}(t)\\mathbf{u}\\) is of the form \\(\\mathbf{u}(t) = \\mathbf{W}(t)\\mathbf{u}(0)\\). In particular, \\[\n\\mathbf{u}(T_0) = \\mathbf{W}(T_0)\\mathbf{u}(0).\n\\]\nWe call \\(\\mathbf{W}(T_0)\\) monodromy matrix for the limit cycle \\(\\Gamma\\).\n\nTheorem A.14 The monodromy matrix has eigenvalues \\(1, \\mu_1, \\ldots, \\mu_{n-1}\\), where \\(\\mu_i\\) are the multipliers of the Poincar√© map associated to the limit cycle \\(\\Gamma\\).\nThus, the size of the perturbation \\(\\mathbf{u}(t)\\), measured with \\(\\| \\mathbf{u}(t) \\|\\), will tend to zero if \\(|\\mu_i| &lt; 1\\). The unitary eigenvalue corresponds to the tangent direction to the limit cycle.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Introduction to ODEs</span>"
    ]
  },
  {
    "objectID": "bifurcations.html",
    "href": "bifurcations.html",
    "title": "Appendix B ‚Äî Introduction to bifurcations",
    "section": "",
    "text": "B.1 Background\nAs we have seen, dynamical system often depend on parameters. For instance, the spruce budworm model \\[\nu' = \\rho\\Bigl( 1 - \\frac{u}{q} \\Bigr) - \\frac{u^2}{1+u^2},\n\\]\ndepends on two parameters, \\(\\rho&gt;0\\) and \\(q&gt;0\\). The number of equilibria and their stability will depend on the parameters. However, most of the time a change of a parameter has little effect on the phase portrait: equilibria barely move, their stability is unchanged. In this case we say that the system is topologically stable. Otherwise, like for \\(\\rho=\\rho_1\\) or \\(\\rho = \\rho_2\\), we have a bifurcation.\nAnother example we have seen is the Gause-type prey-predator model: \\[\n\\left\\{\\begin{aligned}\nu' &= \\rho u(1-u) - \\frac{\\alpha\\delta u v}{1+\\delta u}, \\\\\nv' &= - v + \\frac{\\alpha\\delta u v}{1+\\delta u},\n\\end{aligned}\\right.\n\\]\nwhere we have 3 parameters. For \\(\\delta = \\frac{1}{\\alpha-1}\\) or \\(\\delta = \\frac{\\alpha+1}{\\alpha-1}\\) we also have bifurcations.\nIn general, a parametric dynamical system is an equation of the form \\[\n\\mathbf{y}' = \\mathbf{f}(\\mathbf{y},\\mathbf{p}),\n\\]\nwhere \\(\\mathbf{p}\\in\\mathbb{R}^m\\) is the set of parameters. For the spruce budworm equation, \\(\\mathbf{p} = (\\rho,q)\\). Here, we are only focusing on the case \\(m = 1\\), when \\(\\mathbf{p}\\) is a scalar. Bifurcations occuring in this case are of co-dimension 1.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Introduction to bifurcations</span>"
    ]
  },
  {
    "objectID": "bifurcations.html#structural-stability",
    "href": "bifurcations.html#structural-stability",
    "title": "Appendix B ‚Äî Introduction to bifurcations",
    "section": "B.2 Structural stability",
    "text": "B.2 Structural stability\nConsider the one-parameter ODE \\[\n\\mathbf{y}' = \\mathbf{f}(\\mathbf{y},p),\n\\]\nand suppose that the problem is well-posed for \\(p\\in\\mathcal{P}\\subseteq\\mathbb{R}\\) and \\(\\mathbf{f}\\).\nGiven \\(p=\\bar{p}\\), the system will have some phase portrait. Is it stable to small perturbations of the parameter \\(p\\)? In other words, if we consider a new system: \\[\n\\tilde{\\mathbf{y}}' = \\mathbf{f}(\\tilde{\\mathbf{y}},p),\n\\]\nwith \\(p \\in (\\bar{p}-\\varepsilon, \\bar{p}+\\varepsilon)\\), \\(\\varepsilon\\ll 1\\), is the ‚Äúerror‚Äù between \\(\\tilde{\\mathbf{y}}(t)\\) and \\(\\mathbf{y}(t)\\) going to zero as \\(\\varepsilon\\to 0\\) for any choice of the initial condition? That is, is the phase portrait of the perturbed system ‚Äúconverging‚Äù to the phase portrait of the original ODE? This is typically the case.\n\nTheorem B.1 (stability to perturbations) Suppose that \\(\\mathbf{f}(\\mathbf{y},p)\\) is continuous and locally Lipschitz in \\(\\mathbf{y}\\), uniformly in \\(p\\), and that the ODE is well-posed for any choice of \\(p\\) in the interval. Then the solution of the ODE \\(\\boldsymbol{\\phi}(t,\\mathbf{y}_0,p)\\to\\boldsymbol{\\phi}(t,\\mathbf{y}_0,\\bar{p})\\) uniformly as \\(p\\to \\bar{p}\\).\n\nThis theorem gives a zero-stability result, in the limit \\(p\\to \\bar{p}\\). How about the case of \\(p\\in(\\bar{p}-\\varepsilon, \\bar{p}+\\varepsilon)\\) but \\(\\varepsilon &gt; 0\\)? Is the perturbed ODE still ‚Äúclose‚Äù to the original one? We have the following definition\n\nDefinition B.1 (structural stability) The original ODE is structurally stable in some (closed) region of the phase space if and only if there exists \\(\\varepsilon &gt; 0\\) such that the perturbed system is topologically equivalent to the original ODE for all \\(p\\in (\\bar{p}-\\varepsilon, \\bar{p}+\\varepsilon)\\).\n\n\nDefinition B.2 (topological equivalence) Two ODEs are topologically equivalent if there exists a diffeomorphism that smoothly maps the phase portrait of one system into the other, preserving the direction of time.\n\nFor instance, the two phase portraits below are topologically equivalent, because we can smoothly deform one into the other. (Put a fork at the center, then twist like you would do with spaghetti.)\n\n\n\nTopological equivalence",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Introduction to bifurcations</span>"
    ]
  },
  {
    "objectID": "bifurcations.html#bifurcations",
    "href": "bifurcations.html#bifurcations",
    "title": "Appendix B ‚Äî Introduction to bifurcations",
    "section": "B.3 Bifurcations",
    "text": "B.3 Bifurcations\nOn the other hand, these below are not topologically equivalent, because if we shrink the limit cycle down to a point, we collide with another equilibrium at the center, making the map non-invertible.\n\n\n\nPhase portraits that are not topologically equivalent\n\n\nWhen we are a situation like the one above, we have a bifurcation.\n\nDefinition B.3 (bifurcation) If for some value \\(p=\\bar{p}\\) a system is not structurally stable, then for \\(p=\\bar{p}\\) we have a bifurcation.\n\nBifurcations result from the collision of invariant sets of the dynamical system, for instance 2 equilibria or 2 limit cycles colliding one into the other, for some value of the parameter. We can visualize this in a bifurcation plot, like the one below:\n\n\n\nbifurcation in 1D\n\n\nwhere we have on the abscissa the parameter, and on the ordinate the phase space. The curves are equilibria, and for \\(\\bar{p}\\) we have a bifurcation.\nThe bifurcation plot is just a ‚Äústack‚Äù of phase portraits for various values of \\(p\\). In some sense, a smooth transition between one layer to the other implies topological equivalence, otherwise we have bifurcations. For instance, for planar system we could have:\n\n\n\nbifurcation in 2D\n\n\nThe ‚Äúcone‚Äù is formed by limit cycles. Here, we have 3 bifurcations.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Introduction to bifurcations</span>"
    ]
  },
  {
    "objectID": "bifurcations.html#continuation-of-equilibria",
    "href": "bifurcations.html#continuation-of-equilibria",
    "title": "Appendix B ‚Äî Introduction to bifurcations",
    "section": "B.4 Continuation of equilibria",
    "text": "B.4 Continuation of equilibria\nConsider the dynamical system in 1D: \\[\ny' = f(y,p),\n\\]\nwith \\(p\\in\\mathbb{R}\\). An equilibrium \\(\\bar{y}\\) of the ODE for a given parameter \\(\\bar{p}\\) is a solution to the equation: \\[\n0 = f(\\bar{y},\\bar{p}).\n\\]\nSuppose that \\(f\\) is a smooth function in \\((y,p)\\). Thanks to the implicit function theorem, if \\(\\partial_p f(\\bar{y}, \\bar{p})\\neq 0\\) we can locally define a represent the curve \\(f(\\bar{y}, \\bar{p})=0\\) in the space \\((y,p)\\in\\mathbb{R}^2\\) with a function \\(y=\\phi(p)\\) such that \\(f(\\phi(p),p)=0\\) for \\(p\\in \\mathcal{B}_\\varepsilon(\\bar{p})\\), some neighborhood of \\(\\bar{p}\\).\nThe curve \\((\\phi(p),p)\\) represents the continuation of the equilibrium \\(\\bar{y}\\) as we change the parameter \\(p=\\bar{p}\\). All points along \\((\\phi(p),p)\\) are equilibria, by construction, for different values of the parameter \\(p\\).\nWe can extend the same construction in several dimensions, say \\(\\mathbf{y}\\in\\mathbb{R}^n\\) and \\(p\\in\\mathbb{R}\\). Let us by \\(\\mathbf{f}_\\mathbf{y}(\\mathbf{y},p)\\) the Jacobian of \\(\\mathbf{f}\\) with respect to \\(\\mathbf{y}\\). Suppose that \\(\\bar{\\mathbf{y}}\\) is an equilibrium for the ODE \\[\n\\mathbf{y}' = \\mathbf{f}(\\mathbf{y},\\bar{p}),\n\\]\nthat is, \\(\\mathbf{f}(\\bar{\\mathbf{y}},\\bar{p})=\\mathbf{0}\\), and that \\[\n\\det \\mathbf{f}_\\mathbf{y}(\\bar{\\mathbf{y}},\\bar{p}) \\neq 0,\n\\]\nthen there exists a function \\(\\mathbf{y}=\\boldsymbol{\\phi}(p)\\) such that \\(\\mathbf{f}(\\boldsymbol{\\phi}(p),p)=\\mathbf{0}\\) for \\(p\\in\\mathcal{B}_\\varepsilon(\\bar{p})\\).\nThe stability of the equilibria along the curve is also preserved: the eigenvalues of \\(\\mathbf{f}_\\mathbf{y}(\\mathbf{y},p)\\) depends continuously on \\(p\\) so the branch of equilibria \\(\\mathbf{y}=\\boldsymbol{\\phi}(p)\\) will inherit the same stability properties of \\(\\bar{\\mathbf{y}}\\).\n\n\n\nCurve of equilibria\n\n\nWhat if an eigenvalue of \\(\\mathbf{f}_\\mathbf{y}(\\boldsymbol{\\phi}(p),p)\\) will end up with zero real part along the curve?\nThen we have a bifurcation. We still denote this point with \\(\\bar{p}\\) and the corresponding equilibrium with \\(\\bar{\\mathbf{y}}\\). We have different options:\n\nTangent bifurcation. \\(\\mathbf{f}_\\mathbf{y}(\\bar{\\mathbf{y}},\\bar{p})\\) as eigenvalue 0 and \\(\\mathbf{f}_p(\\bar{\\mathbf{y}},\\bar{p}) \\neq 0\\).\nTranscritical bifurcation. \\(\\mathbf{f}_\\mathbf{y}(\\bar{\\mathbf{y}},\\bar{p})\\) as eigenvalue 0 and \\(\\mathbf{f}_p(\\bar{\\mathbf{y}},\\bar{p}) = 0\\).\nHopf bifurcation. \\(\\mathbf{f}_\\mathbf{y}(\\bar{\\mathbf{y}},\\bar{p})\\) as eigenvalue \\(\\pm i\\omega\\) (only \\(n\\ge 2\\)).\n\nNote that if one real eigenvalue is zero, then we cannot apply the implicit function theorem, and the curve \\(\\boldsymbol{\\phi}\\) cannot be defined at \\(p=\\bar{p}\\) (this is the case of the first 2 bifurcations.)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Introduction to bifurcations</span>"
    ]
  },
  {
    "objectID": "bifurcations.html#sec-tangbif",
    "href": "bifurcations.html#sec-tangbif",
    "title": "Appendix B ‚Äî Introduction to bifurcations",
    "section": "B.5 Tangent bifurcation",
    "text": "B.5 Tangent bifurcation\nThe simplest bifurcation is perhaps the tangent bifurcation. Consider the following ODE: \\[\ny' = f(y,p) = p - y^2,\n\\]\nfor \\(p\\in\\mathbb{R}\\). We have the following situation:\n\nIf \\(p&gt;0\\) there are 2 equilibria, \\(\\bar{y}^{\\pm} = \\pm\\sqrt{p}\\). Since \\(\\partial_y f = -2y\\), we have that \\(\\bar{y}^-\\) is unstable and \\(\\bar{y}^+\\) is asympotically stable.\nIf \\(p&lt;0\\) there is no real equilibrium.\nIf \\(p=\\bar{p}=0\\) we have a single equilibrium \\(\\bar{y}=0\\). The stability cannot be deduced from the linearization (why?), but from the sign of \\(f\\) we see that \\(\\bar{y}\\) is attractive for \\(y&gt;0\\) and repulsive for \\(y&lt;0\\). It is called saddle-node.\n\nSince \\(\\partial_y f(\\bar{y},\\bar{p})=0\\) but \\(\\partial_p f(\\bar{y},\\bar{p}) = 1 \\neq 0\\), for \\(p=\\bar{p}=0\\) we have a tangent bifurcation. Tangent bifurcations are also called limit points (in MatCont denoted by LP). The reason for the name should be clear from the bifurcation plot.\n\n\n\ntangent bifurcation\n\n\nTangent bifurcations are catastrophic, in the sense that a point at equilibrium right before the bifurcation will diverge from its state after the bifurcation (imagine to move a point along the stable branch from right to left.) Note that the curve is smooth, because \\(\\partial_p f(\\bar{y},\\bar{p})\\neq 0\\). Check the next figure, where we visualize \\(f(y,p)\\) around the bifurcation (in black the zero levelset, that is the curve of equilibria):\n\n\n\nimage",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Introduction to bifurcations</span>"
    ]
  },
  {
    "objectID": "bifurcations.html#sec-transbif",
    "href": "bifurcations.html#sec-transbif",
    "title": "Appendix B ‚Äî Introduction to bifurcations",
    "section": "B.6 Transcritical bifurcation",
    "text": "B.6 Transcritical bifurcation\nThe transcritical bifurcation occurs when an equilibrium has 0 real part and also \\(\\partial_p f(\\bar{y},\\bar{p})=0\\). The normal form is: \\[\ny' = py - y^2.\n\\]\nThe derivative of \\(f\\) is \\(f_y(y,p) = p-2y\\). We have 2 equilibria,\n\n\\(y_0=0\\), stable for \\(p&gt;0\\) and unstable for \\(p&lt;0\\).\n\\(\\bar{y}=p\\), unstable for \\(p&gt;0\\) and stable for \\(p&lt;0\\).\n\nFor \\(p=0\\) we only have one equilibrium \\(\\bar{y}=0\\), which is a saddle-node because it is attractive for \\(y&lt;0\\) and repulsive for \\(y&gt;0\\).\n\n\n\nimage\n\n\nTranscritical bifurcations are very common in biological modeling, because some equilibria (like the origin) do not depend on the parameters and are always present. Therefore, in the bifurcation plot their curve do not change, and can be intersected by other curves of equilibria. In fact, if \\(f(0,p)=0\\) for all \\(p\\), then also \\(f_p(0,p)=0\\). Note that the above model is the logistic equation.\nFrom the 3d visualization of \\(f\\):\n\n\n\nimage\n\n\nwe see a critical point at the bifurcation point. The eigenvectors give the tangents to the 2 curves of equilibria. In this way, it is possible to switch curve at the bifurcation point.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Introduction to bifurcations</span>"
    ]
  },
  {
    "objectID": "bifurcations.html#hopf-bifurcation",
    "href": "bifurcations.html#hopf-bifurcation",
    "title": "Appendix B ‚Äî Introduction to bifurcations",
    "section": "B.7 Hopf bifurcation",
    "text": "B.7 Hopf bifurcation\nThe Hopf bifurcation occurs when an equilibrium changes stability and a limit cycle appears. The normal form is: \\[\n\\left\\{ \\begin{aligned}\nx' &= \\sigma x + \\omega y + c x (x^2 + y^2), \\\\\ny' &= \\sigma y - \\omega x + c y (x^2 + y^2), \\\\\n\\end{aligned} \\right.\n\\]\nSuppose that \\(\\sigma\\), \\(\\mu\\), and \\(c\\) depend on some parameter \\(p\\in\\mathbb{R}\\).\nWe have that \\((x,y)=(0,0)\\) is an equilibrium for all choices of \\(p\\). The Jacobian at \\((0,0)\\) has eigenvalues \\[\n\\lambda^\\pm = \\sigma \\pm i \\omega,\n\\]\nWhen \\(\\sigma(p) = 0\\), the real part of the complex conjugate eigenvalues becomes zero: we have a Hopf bifurcation. To check the existence of a limit cycle, we consider the system in polar coordinates: \\[\n\\left\\{ \\begin{aligned}\nx &= r \\cos\\theta, \\\\\ny &= r \\sin\\theta,\n\\end{aligned} \\right.\n\\quad\\Rightarrow\\quad\n\\left\\{ \\begin{aligned}\nr &= \\sqrt{x^2 + y^2}, \\\\\n\\theta &= \\arctan (y/x).\n\\end{aligned} \\right.\n\\]\nTo change the variables, simply note that: \\[\n\\begin{split}\nr\\mathrm{d}r &= \\tfrac{1}{2}\\mathrm{d}(r^2) = x\\mathrm{d}x + y\\mathrm{d}y \\\\\n&= \\bigl(\\sigma (x^2 + y^2) + c (x^2 + y^2)(x^2 + y^2) \\bigr)\\mathrm{d}t = \\\\\n&= \\bigl(\\sigma r^2 + c r^4 \\bigr)\\mathrm{d}t,\n\\end{split}\n\\]\nand \\[\n\\begin{split}\n\\mathrm{d}\\theta &= \\frac{\\mathrm{d}y/x - y\\mathrm{d}x/x^2}{1 + (y/x)^2} = \\frac{x\\mathrm{d}y - y\\mathrm{d}x}{x^2 + y^2} \\\\\n&= \\frac{x(\\sigma y - \\omega x + cy(x^2+y^2)) - y(\\sigma x + \\omega y + cx(x^2+y^2))}{x^2 + y^2}\\mathrm{d}t \\\\\n&= -\\frac{\\omega (x^2 + y^2)}{x^2 + y^2}\\mathrm{d}t = -\\omega\\mathrm{d}t.\n\\end{split}\n\\]\nThe system in polar coordinates is: \\[\n\\left\\{ \\begin{aligned}\nr' &= \\sigma r + c r^3, \\\\\n\\theta' &= -\\omega. \\\\\n\\end{aligned} \\right.\n\\]\nAssuming \\(\\omega(p)\\neq 0\\), we have two equilibria:\n\n\\(r=0\\), which corresponds to the origin and we know it is stable for \\(\\sigma(p)&lt;0\\) and unstable for \\(\\sigma(p)&gt;0\\).\n\\(r=\\bar{r}=\\sqrt{-\\sigma/c}\\), which exists only for \\(c(p)\\neq 0\\) and \\(\\sigma(p)c(p)&lt;0\\). When there exists, it is stable if \\(\\sigma(p)&gt;0\\) and unstable if \\(\\sigma(p)&lt;0\\). This corresponds to a limit cycle of radius \\(\\bar{r}\\).\n\nIn conclusion, we have 2 types of Hopf bifurcations.\n\nSupercritical Hopf bifurcation \\(H^S\\): \\(\\sigma'(\\bar{p})&gt;0\\) and \\(c(\\bar{p})&lt;0\\). The stable equilibrium at the origin becomes unstable and a stable limit cycle is born. The bifurcation is not catastrophical.\n\n\n\nSubrcritical Hopf bifurcation \\(H_S\\): \\(\\sigma'(\\bar{p})&lt;0\\) and \\(c(\\bar{p})&gt;0\\). The unstable equilibrium at the origin becomes stable and an unstable limit cycle is born. The bifurcation is catastrophical.\n\n\nNote that for \\(c=0\\) at the bifurcation point we have an infinite number of limit cycles (non-isolated periodic orbit), like the classical oscillator. This is a degenerate Hopf bifurcation so the condition \\(c(\\bar{p})\\neq 0\\) is called non-degeneracy condition. The condition \\(\\sigma'(\\bar{p})\\neq 0\\) is called trasversality condition.\nFor a general system of ODEs: \\[\n\\left\\{ \\begin{aligned}\nx' &= f(x,y,p), \\\\\ny' &= g(x,y,p), \\\\\n\\end{aligned} \\right.\n\\]\nsuch that \\((\\bar{x},\\bar{y})=(0,0)\\) is an equilibrium (with no loss of generality we can take the equilibrium at the origin), thanks to linearization we can always write the system around the equilibrium in the form: \\[\n\\left\\{ \\begin{aligned}\nx' &= \\sigma(p) x + \\omega(p) y + \\tilde{f}(x,y,p), \\\\\ny' &= \\sigma(p) y - \\omega(p) x + \\tilde{g}(x,y,p), \\\\\n\\end{aligned} \\right.\n\\]\nwith \\(\\tilde{f}(0,0,p)=\\tilde{g}(0,0,p)=0\\). It is possible to further expand \\(\\tilde{f}\\) and \\(\\tilde{g}\\) so to find a term of the form \\(cr^3\\) as above. The coefficient \\(c\\) is called first Lyapunov exponent and it reads: \\[\n\\begin{split}\n16 c &= f_{xxx} + f_{xyy} + g_{xxy} + g_{yyy} \\\\\n&+ \\frac{1}{\\omega} \\Bigl( f_{xy}(f_{xx}+f_{yy}) - g_{xy}(g_{xx}+g_{yy}) - f_{xx}g_{xx} - f_{yy}g_{yy} \\Bigr).\n\\end{split}\n\\]\nThen we have the following:\n\nTheorem B.2 (Hopf) Suppose that \\(\\sigma'(p)\\neq 0\\) (trasversality) and \\(c\\neq 0\\) (non-degeneracy). Then there exists a branch \\(\\Gamma_p\\) of periodic solutions with period \\(T(p)\\) for \\(p\\) such that \\(|p-\\bar{p}|\\) is small and \\(p &gt; \\bar{p}\\) if \\(\\sigma'(\\bar{p})c &lt; 0\\) (resp. \\(p &lt; \\bar{p}\\) if \\(\\sigma'(\\bar{p})c &gt; 0\\)). Furthermore, \\(\\Gamma_p \\to \\bar{y}\\) for \\(p\\to\\bar{p}\\) and \\(T(p)\\to \\frac{2\\pi}{\\omega(\\bar{p})}\\). If \\(c&lt;0\\) \\(\\Gamma_p\\) are attracting; if \\(c&gt;0\\) \\(\\Gamma_p\\) are repelling.\n\nWe observed the Hopf bifurcation in Gause-type prey-predator models and in negative feedback networks.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Introduction to bifurcations</span>"
    ]
  },
  {
    "objectID": "bifurcations.html#limit-cycle-tangent-bifurcation",
    "href": "bifurcations.html#limit-cycle-tangent-bifurcation",
    "title": "Appendix B ‚Äî Introduction to bifurcations",
    "section": "B.8 Limit cycle tangent bifurcation",
    "text": "B.8 Limit cycle tangent bifurcation\nConsider the following model: \\[\n\\left\\{ \\begin{aligned}\nr' &= r\\sigma\\bigl(1 + c r^2 - r^4 \\bigr), \\\\\n\\theta' &= -\\omega. \\\\\n\\end{aligned} \\right.\n\\]\nWe have 3 equilibria (note that necessarily \\(r\\ge0\\)): the origin \\(r=0\\) and: \\[\n\\bar{r} = \\sqrt{\\frac{c \\pm \\sqrt{c^2+4\\sigma}}{2}}.\n\\]\nIf we study the existence of limit cycles (LCs) in the \\((\\sigma,c)\\) plane we have:\n\nwhere:\n\n\\(H^S\\): supercritical Hopf bifurcation,\n\\(H_S\\): subcritical Hopf bifurcation,\n\\(T\\): tangent bifurcation between limit cycles.\n\nIn terms of stability we have:\n\nAs we cross \\(T\\) we have 2 limit cycles colliding and disappearing, exactly like the tangent bifurcation between equilibria. (In fact, this is what happens in polar coordinates.)\nThe tangent bifurcation can be visualized as follows:\n\nWe have seen this type of bifurcation in a negative feedback loop. It is catastrophic.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Introduction to bifurcations</span>"
    ]
  },
  {
    "objectID": "Laboratories/00_Intro_Matlab/introMATLAB.html",
    "href": "Laboratories/00_Intro_Matlab/introMATLAB.html",
    "title": "Appendix C ‚Äî Introduction to MATLAB",
    "section": "",
    "text": "C.1 Overview\nMATLAB is a software for scientific computing. It is a self-contained environment, with thousands of optimized functions for many tasks. MATLAB is a proprietary software, so you need a license to use it. Most universities offer an educational license (see this website for UniTrento.)\nThere are free alternatives too:\nYou are free to use any Matlab alternative, and rely on it only for MatCont.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>Introduction to MATLAB</span>"
    ]
  },
  {
    "objectID": "Laboratories/00_Intro_Matlab/introMATLAB.html#overview",
    "href": "Laboratories/00_Intro_Matlab/introMATLAB.html#overview",
    "title": "Appendix C ‚Äî Introduction to MATLAB",
    "section": "",
    "text": "GNU Octave, an open-source software aiming at 1:1 compatibility with Matlab. You can use it, but beware that some MATLAB packages may not work (e.g., MatCont).\nPython, probably the most popular programming language, has many libraries for scientific computing, such as numpy, scipy, matplotlib, jax, and many others. It would be the best alternative to Matlab, if it weren‚Äôt for MatCont, that we will extensively use.\nJulia, it is a python competitor, with a similar syntax but much better performance. Many libraries have a Julia wrapper, but again not MatCont.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>Introduction to MATLAB</span>"
    ]
  },
  {
    "objectID": "Laboratories/00_Intro_Matlab/introMATLAB.html#installation",
    "href": "Laboratories/00_Intro_Matlab/introMATLAB.html#installation",
    "title": "Appendix C ‚Äî Introduction to MATLAB",
    "section": "C.2 Installation",
    "text": "C.2 Installation\nMatlab installation is straightforward on most OSes. Please follow the instruction on the website. We do not need special toolboxes. In case, you can install them later.\nYou can also use the online version of MATLAB. It comes with an online storage called MATLAB Drive that you can use to sync your files.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>Introduction to MATLAB</span>"
    ]
  },
  {
    "objectID": "Laboratories/00_Intro_Matlab/introMATLAB.html#quick-tour",
    "href": "Laboratories/00_Intro_Matlab/introMATLAB.html#quick-tour",
    "title": "Appendix C ‚Äî Introduction to MATLAB",
    "section": "C.3 Quick tour",
    "text": "C.3 Quick tour\nOnce open, Matlab will look like this:\n\n\n\nThe MATLAB interface\n\n\nYou can notice different panels:\n\nCommand Window is where we type commands.\nCurrent Folder is the current working directory.\nWorkspace is a summary of all variables.\n\nThe system is waiting for an input &gt;&gt;.\n\nC.3.1 Basics\nMATLAB stands for MAtrix LABoratory, so its philosophy is to store everything numeric as a matrix. (This is not entirely true anymore, recent versions of Matlab introduced many new types.) A scalar is a \\(1\\times 1\\) matrix. Matlab a weakly typed (and interpreted) programming language. A variable is a generic, and no type is needed like in C:\n&gt;&gt; a = 2.45\na =\n    2.4500\nAs we inout a = 2.45, the system responds with its representation. Note that this is not the internal precision! All numbers with a period are stored as double. To suppress the output, just put a semicolon:\n&gt;&gt; a = 2.45;\nWe can execute more commands per line (note the comma):\n&gt;&gt; a = 2.45 , b = 3.1; A = 1.2;\n\n\n\n\n\n\nWarning\n\n\n\nMatlab is case-sensitive: the variable a and A are different. This is the standard behavior also for file names in POSIX systems like UNIX, but not for Windows. Also on macOS you need to be careful: by default the file system is case-insensitive like Windows.\n\n\nIf a number is assigned to nothing, it will go to the variable ans:\n&gt;&gt; 2.45;\n&gt;&gt; whos\n  Name      Size            Bytes  Class     Attributes\n  ans       1x1                 8  double              \n&gt;&gt; ans\nans =\n    2.4500\nSome variables are predefined, like pi, \\(\\pi\\), and 1i or 1j, the imaginary unit \\(i\\). Note that also i and j are valid for Matlab, but dangerous: i is often an iteration variable in a for-loop:\n&gt;&gt; a = 5 + 2*i\na =\n   5.0000 + 2.0000i\n\n&gt;&gt; i = 2;\n&gt;&gt; b = 5 + 2*i\nb =\n     9\n\n&gt;&gt; c = 5 + 2*1i\nc =\n   5.0000 + 2.0000i\nYou can clear a variable by using clear myvar, or all variables with clear. The command clc clears the screen.\nIf you are not sure of a command, use tab-completion: type the beginning of a command, then type ‚ÄúTAB‚Äù. You can also use help and doc.\n\n\nC.3.2 Output format\nBy default, numbers are visualized in short format. The command format can change the behavior:\n&gt;&gt; format long\n&gt;&gt; 1/7\nans =\n   0.142857142857143\nBelow in the table different options:\n\n\n\nFormat\nans\n\n\n\n\nformat rat\n1/7\n\n\nformat short\n0.1429\n\n\nformat short e\n1.4286e-01\n\n\nformat short g\n0.14286\n\n\nformat long\n0.142857142857143\n\n\nformat long e\n1.428571428571428e-01\n\n\nformat long g\n0.142857142857143\n\n\n\n\n\nC.3.3 Vector and matrices\nWe have many ways to generate a vector in Matlab. The most direct is:\n&gt;&gt; b = [1 5 6 7];\n&gt;&gt; c = [1, 5, 6, 7];\nThe two vectors are identical: the comma is optional. The dimension is \\(1\\times 4\\), that is 1 row and 4 columns. For column vector, we need to use the semi-colon (now mandatory):\n&gt;&gt; d = [1; 5; 6; 7];\n&gt;&gt; whos\n  Name      Size            Bytes  Class     Attributes\n  b         1x4                32  double              \n  c         1x4                32  double              \n  d         4x1                32  double              \nNote that the memory requirement is 32 bytes, that it \\(4\\cdot 8\\) bytes, where 8 bytes is a 64-bit floating-point number (double).\nWe can generate a vector also as follows:\n&lt;start&gt; : &lt;increment&gt; : &lt;end&gt;\nIf we omit the increment, it is assumed one:\n&gt;&gt; 1:8\nans =\n     1     2     3     4     5     6     7     8\n\n&gt;&gt; 1:2:8\nans =\n     1     3     5     7\n\n&gt;&gt; 0.5:-0.1:0\nans =\n    0.5000    0.4000    0.3000    0.2000    0.1000         0\nThe command above is quite powerful: it also compensates for rounding off errors. Alternatively, we can use linspace:\n&gt;&gt; linspace(0,1,5)\nans =\n         0    0.2500    0.5000    0.7500    1.0000\nMatrices easily follows:\n&gt;&gt; A = [ 1 2 3 4; 5 6 7 8 ; 9 10 11 12; 13 14 15 16 ]\nA =\n     1     2     3     4\n     5     6     7     8\n     9    10    11    12\n    13    14    15    16\nwhich is returning a \\(4\\times 4\\) matrix. More complex matrices can be created with ad-hoc functions like eye, ones, zeros, reshape, and so on.\n&gt;&gt; reshape(A, 2, 8)\nans =\n\n     1     9     2    10     3    11     4    12\n     5    13     6    14     7    15     8    16\n\n&gt;&gt; B = [ 1:3 0; 4:-1:1 ]\nB =\n     1     2     3     0\n     4     3     2     1\n\n&gt;&gt; eye(2,3)\nans =\n     1     0     0\n     0     1     0\n\n&gt;&gt; ones(2,2)\nans =\n     1     1\n     1     1\n\n\n\n\n\n\nExercise\n\n\n\nTry to create the following matrix:\n\n\n\nimage\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n&gt;&gt; n = 3;\n&gt;&gt; A = [ones(n), zeros(n); ...\n         eye(n), [1:n; n*ones(n-1,n) ];\n\n\n\n\n\n\n\nC.3.4 Matrix manipulation\nWe can access and modify matrices quite naturally:\n&gt;&gt; v = linspace(0, 1, 5);\n&gt;&gt; v(2)\nans =\n    0.2500\nThe operator ( ) is much more poweful: we can use slices:\n&gt;&gt; v([1 3 5])\nans =\n         0    0.5000    1.0000\n\n&gt;&gt; v(1:3)\nans =\n         0    0.2500    0.5000\n\n&gt;&gt; v(2:end)\nans =\n    0.2500    0.5000    0.7500    1.0000\n\n&gt;&gt; v([1 2; 3 2])\nans =\n         0    0.2500\n    0.5000    0.2500\nWe also can use vectors or matrices of indices. For instance, if indices are [1 3 5], the output is [v(1) v(3) v(5)]. With matrices is similar:\n&gt;&gt; A = [ 1 2 3 ; 4 5 6 ; 7 8 9]\nA =\n     1     2     3\n     4     5     6\n     7     8     9\n\n&gt;&gt; A(1,1)\nans =\n     1\n\n&gt;&gt; A(1:3,1:2)\nans =\n     1     2\n     4     5\n     7     8\n\n&gt;&gt; A(1:end-1,2:end)\nans =\n     2     3\n     5     6\n\n&gt;&gt; A([1 2],[2 3])\nans =\n     2     3\n     5     6\nIn the last case we extract the first and the second row, and then crossed with the second and third columns.\n&gt;&gt; A(1:end)\nans =\n     1     4     7     2     5     8     3     6     9\n\n&gt;&gt; A([2 3; 9 6])\nans =\n     4     7\n     9     8\nThe operation A(1:end) is equivalent to A(:), except for returning a column vector instead of a row vector.\n\n\nC.3.5 Operations on Matrices\nLet‚Äôs now discuss algebraic operations on matrices. The basic rules are as follows, for two matrices \\(\\mathbf{A} \\in \\mathbb{C}^{n\\times n}\\) and \\(\\mathbf{B}\\in\\mathbb{C}^{p\\times q}\\):\n\nAddition is defined if and only if \\(n = p\\) and \\(m = q\\), and \\(\\mathbf{A}+\\mathbf{B} \\in \\mathbb{C}^{n \\times m}\\).\nMultiplication is defined if and only if \\(m = p\\), and \\(\\mathbf{A}\\mathbf{B} \\in \\mathbb{C}^{n \\times q}\\).\nTranspose \\(\\mathbf{A}^T \\in \\mathbb{C}^{m \\times n}\\), with \\((\\mathbf{A}^T)_{ij} = (\\mathbf{A})_{ji}\\).\nConjugate transpose \\(\\mathbf{A}^H \\in \\mathbb{C}^{m \\times n}\\), with \\((\\mathbf{A}^H)_{ij} = \\overline{(\\mathbf{A})_{ji}}\\).\n\nThe last two operations can be performed as follows:\n&gt;&gt; A = [ 1+1i, 3; 1, 1-2i ];\n&gt;&gt; A.'\nans =\n   1.0000 + 1.0000i   1.0000          \n   3.0000             1.0000 - 2.0000i\n&gt;&gt; A'\nans =\n   1.0000 - 1.0000i   1.0000          \n   3.0000             1.0000 + 2.0000i\nIf the matrix is real, these two operations are the same. In fact, it is common to use ' as the transpose of a matrix:\n&gt;&gt; a = [ 1 2 5 ];\n&gt;&gt; b = [ 4 6 4 ];\n&gt;&gt; H = [ 1 2 3 ; 2 4 7 ; 1 4 3 ];\n&gt;&gt; G = [ 4 6 2 ; 8 4 1 ; 3 2 9 ];\n&gt;&gt; a + b\nans =\n     5     8     9\n&gt;&gt; a - b\nans =\n    -3    -4     1\n&gt;&gt; H + G\nans =\n     5     8     5\n    10     8     8\n     4     6    12\n&gt;&gt; H * G\nans =\n    29    20    31\n    61    42    71\n    45    28    33\n&gt;&gt; G * a\nError using *\nInner matrix dimensions must agree.\nThis is the most common error. In this case, the vector a is \\(1 \\times 3\\), while H is \\(3 \\times 3\\), which is not valid.\n&gt;&gt; G * a'\nans =\n    26\n    21\n    52\n&gt;&gt; a * G\nans =\n    35    24    49\n&gt;&gt; 2 * a\nans =\n     2     4    10\n&gt;&gt; a' * b\nans =\n     4     6     4\n     8    12     8\n    20    30    20\n&gt;&gt; a * b'\nans =\n    36\n&gt;&gt; A^2\nans =\n     8    22    26\n    17    48    55\n    12    30    40\n&gt;&gt; a^2\nError using ^\nInputs must be a scalar and a square matrix.\nTo compute elementwise POWER, use POWER (.^) instead.\nThe last message tells us that exponentiation is intended as matrix multiplication and is only defined for square matrices. It suggests using the dot version instead. In fact, scalar binary operations like multiplication, division, and exponentiation become element-wise operations for vectors and matrices when you put a dot in front of the operation symbol:\n&gt;&gt; a .* b\nans =\n     4    12    20\n&gt;&gt; a ./ b\nans =\n    0.2500    0.3333    1.2500\n&gt;&gt; a .^ 2\nans =\n     1     4    25\n&gt;&gt; a .^ b\nans =\n     1    64   625\nThe only thing to keep in mind is that the two operands should have the same dimensions, or at least one of them should be a scalar (in which case it is repeated).\n\n\n\n\n\n\nExercise\n\n\n\nTry to create the following vector, for arbitrary \\(n\\): \\[\n\\Bigl[\\:\n\\overbrace{\n\\underbrace{1, 1, \\ldots, 1}_n,\n\\underbrace{2, 2, \\ldots, 2}_n,\n\\ldots,\n\\underbrace{n, n, \\ldots, n}_n}^n\n\\:\\Bigr].\n\\]\n\n\n\n\n\n\nSolution\n\n\n\n\n\n&gt;&gt; n = 5;\n&gt;&gt; v = reshape(ones(n,1)*(1:n),1,[])\n\n\n\n\n\n\n\nC.3.6 Elementary Mathematical Functions\nOne of the strengths of MATLAB is the vast number of mathematical functions (both elementary and advanced) it offers. Unless specified otherwise, almost all of these functions operate element-wise. For example, given a matrix A, exp(A) is not the matrix exponential (which is calculated with expm(A)), but rather a matrix whose elements are the exponentials of the original elements. Here are some elementary mathematical functions:\n\nabs(A): Absolute value of the elements of A\nsqrt(A): Square root of the elements of A\nexp(A): Exponential function applied to the elements of A\nlog(A): Natural logarithm of the elements of A\nlog10(A): Base 10 logarithm of the elements of A\nlog2(A): Base 2 logarithm of the elements of A\nsin(A): Sine of the elements of A\ncos(A): Cosine of the elements of A\ntan(A): Tangent of the elements of A\nasin(A): Arcsine of the elements of A (in radians)\nacos(A): Arccosine of the elements of A (in radians)\natan(A): Arctangent of the elements of A (in radians)\nsinh(A): Hyperbolic sine of the elements of A\ncosh(A): Hyperbolic cosine of the elements of A\ntanh(A): Hyperbolic tangent of the elements of A\n\n\n\n\n\n\n\nExercise\n\n\n\n\nCompute \\(\\dfrac{e^5 + \\sin(\\pi)}{\\sqrt{\\log_2 30}-10}\\) and \\(e^{\\log_{10}50} + e^{\\log 30} + e^{\\log_2 40}\\).\nDefine \\(\\mathbf{x} = [1, 3, 4]\\) and \\(\\mathbf{y} = [1, 1, 2]\\). Compute \\(2x_i \\log_2(|y_i|+1) - y_i\\log_{10}(x_i+2)\\) and \\(\\arctan\\left( \\dfrac{x_i}{y_i}\\right) - \\sin^2\\left( x_i\\sqrt[3]{|y_i|^2} \\right)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n&gt;&gt; % Point 1\n&gt;&gt; (exp(5) + sin(pi))/(sqrt(log2(30))-10)\n&gt;&gt; exp(log10(50)) + exp(log(30)) + exp(log2(40))\n&gt;&gt; % Point 2\n&gt;&gt; x = [ 1 3 4 ];\n&gt;&gt; y = [ 1 1 2 ];\n&gt;&gt; 2*x.*log2(abs(y)+1) - y.*log10(x+2)\n&gt;&gt; atan(x./y) - sin(x.*abs(y).^(2/3)).^2\n\n\n\n\n\n\n\nC.3.7 Elementary Mathematical Functions (Continued)\nLet A, B be two matrices, and b be a vector.\n\nsize(A): Returns a two-element vector, where the first element is the number of rows in A, and the second element is the number of columns.\nsize(A, 1): Returns the first element of size(A), which is the number of rows.\nsize(A, 2): Returns the second element of size(A), which is the number of columns.\nlength(b): Returns the number of elements in the vector b.\nmax(b): Returns the largest element in the vector b.\nmin(b): Returns the smallest element in the vector b.\nmax(A): Returns a row vector containing the maximum element of each column of A.\nmin(A): Returns a row vector containing the minimum element of each column of A.\nmax(A, B): Returns a matrix of the same dimensions as A and B, containing the element-wise maximum.\nmin(A, B): Returns a matrix of the same dimensions as A and B, containing the element-wise minimum.\nmax(A, [], 2): Returns a column vector containing the maximum element of each row of A. If you replace 2 with 1, you get max(A).\nmin(A, [], 2): Returns a column vector containing the minimum element of each row of A. If you replace 2 with 1, you get min(A).\nsum(b): Returns a scalar equal to the sum of the elements in the vector b.\nsum(A): Returns a row vector whose elements are the column-wise sum of the elements in matrix A.\nsum(A, 2): Returns a column vector whose elements are the row-wise sum of the elements in matrix A.\ndiag(A): Returns the vector of the diagonal elements of matrix A.\ndiag(A, k): Returns the vector of the \\(k\\)-th super-diagonal of matrix A.\ndiag(A, -k): Returns the vector of the \\(k\\)-th sub-diagonal of matrix A.\ndiag(b): Returns a matrix with the elements of vector b on its diagonal.\ndiag(b, k): Returns a matrix with the elements of vector b on its \\(k\\)-th super-diagonal.\ndiag(b, -k): Returns a matrix with the elements of vector b on its \\(k\\)-th sub-diagonal.\ntril(A): Returns the lower triangular part of matrix A, making all elements strictly above the diagonal zero (even in a rectangular matrix).\ntriu(A): Returns the upper triangular part of matrix A, making all elements strictly below the diagonal zero (even in a rectangular matrix).\n\nThere are many other important functions such as det (determinant of a square matrix), trace (trace of a matrix), norm (for calculating norms), and so on. We will introduce these functions as needed.\n\n\n\n\n\n\nExercise\n\n\n\nTry to use the function magic(n). Then sum by row, column, diagonal, etc. What do you observe?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n&gt;&gt; n = 4;\n&gt;&gt; A = magic(n);\n&gt;&gt; sum(A, 2)\n&gt;&gt; sum(A, 1)\n&gt;&gt; sum(diag(A))\nWe notice that the sum is always \\(34\\), at least for \\(n=4\\). In general, the magic square contains numbers from \\(1\\) to \\(n^2\\), and the magic number is \\(\\frac{1}{2}n(n^2+1)\\). This is true even if we sum over the antidiagonal\n&gt;&gt; sum(diag(fliplr(A)))\nThe function fliplr (flip left‚Äìright) inverts the right with the left in a given matrix.\n\n\n\n\n\n\n\nC.3.8 Functions and Scripts\nMany of the commands we have introduced are defined as functions, which are procedures that take input data and provide output. In MATLAB, there are several ways to define a function.\nSuppose we want to create a function fun(x) that returns the value of \\(f(x) = x\\sin(x) + \\cos^2(x)\\) for a given \\(x\\). The ‚Äúlegacy‚Äù method involves defining a string corresponding to the function and then evaluating it using the eval command:\n&gt;&gt; fun = 'x*sin(x) + (cos(x))^2';\n&gt;&gt; x = 1.0;\n&gt;&gt; eval(fun)\nans =\n    1.1334\nHowever, there is an issue when x is a vector:\n&gt;&gt; x = [ 1 2 3 ];\n&gt;&gt; eval(fun)\nError using *\nInner matrix dimensions must agree.\nTo handle vector input correctly, we need to ‚Äúvectorize‚Äù the expression by using element-wise operations:\n&gt;&gt; fun = 'x.*sin(x) + cos(x).^2';\n&gt;&gt; x = [ 1 2 3 ];\n&gt;&gt; eval(fun)\nans =\n    1.1334    1.9918    1.4034\nAlternatively, you can use the vectorize command to transform the function into its vectorized version.\nHowever, a more convenient way is to define functions using ‚Äúanonymous functions‚Äù:\n&gt;&gt; fun = @(x) x.*sin(x) + cos(x).^2;\n&gt;&gt; x = [ 1 2 3 ];\n&gt;&gt; fun(x)\nans =\n    1.1334    1.9918    1.4034\nThe @ symbol is called a ‚Äúfunction handle.‚Äù Note that the vectorize command does not work with anonymous functions.\nThere are at least two other methods for defining functions, such as using inline and the Symbolic Math Toolbox. We won‚Äôt delve into these methods here.\nInstead, let‚Äôs explore using scripts to define functions. An ‚Äúscript‚Äù is a text file with a .m extension containing a list of commands that MATLAB will execute when the file is called. You can edit the script using any text editor or with MATLAB‚Äôs built-in editor by running the following command:\n&gt;&gt; edit\nSuppose, for example, that the file myscript.m contains the following:\nclear; clc;\n% This is a comment:\n% sum of squares of the diagonal\n% of the magic matrix\nn = 4;\nA = magic(n);\nsumma = sum(diag(A).^2)\nIf the file is in the working directory, you can run it as follows:\n&gt;&gt; myscript\nsumma =\n   414\nThe script is executed as if you had entered the commands directly in the terminal.\nYou can also have scripts that contain only a function with the following syntax:\nfunction y = myfun(x)\n  y = x.*sin(x) + (cos(x)).^2;\nend\nIn this case, you must save the file with the same name as the function (e.g., myfun.m).\nNow you can use the function:\n&gt;&gt; x = [ 1 2 3 ];\n&gt;&gt; myfun(x)\nans =\n    1.1334    1.9918    1.4034\nFunction scripts can have multiple arguments and return multiple outputs:\nfunction [A, b] = vecmat(n, m)\n  % Generate a matrix A of size m x n and\n  % a vector b of size n x 1 with random values.\n  A = rand(n, m);\n  b = rand(n, 1);\nend\nYou can call this function as follows:\n&gt;&gt; [K, f] = vecmat(2, 3)\nK =\n    0.4387    0.7655    0.1869\n    0.3816    0.7952    0.4898\nf =\n    0.4456\n    0.6463\nScript functions can contain at most one function. It is also a good practice to use lowercase names without spaces. You can use functions with multiple arguments and return values as needed.\nThere are many other advanced features and techniques for creating and using functions in MATLAB, but these are the basics to get you started.\n\n\nC.3.9 Loops and Control Structures\nMATLAB is a full-fledged interpreted programming language, and it provides many control flow operations.\n&gt;&gt; for i = 1:3, magic(i), end;\nans =\n     1\nans =\n     1     3\n     4     2\nans =\n     8     1     6\n     3     5     7\n     4     9     2\nIn the example above, a for loop iterates from 1 to 3, and for each iteration, the magic function is called.\n&gt;&gt; n = 1; while n &lt; 4, magic(n), n = n + 1; end;\nans =\n     1\nans =\n     1     3\n     4     2\nans =\n     8     1     6\n     3     5     7\n     4     9     2\nIn this example, a while loop is used. It continues to execute as long as the condition n &lt; 4 is true. Inside the loop, the magic function is called, and n is incremented.\n&gt;&gt; if rand(1) &gt; 0.5, disp('Greater than 0.5'); else, disp('Less than 0.5'); end;\nGreater than 0.5\nThe if statement is used to perform conditional execution. In this example, it checks if a random number is greater than 0.5 and displays a message accordingly.\nAs we need more details, we will add them gradually. Typically, writing these control structures directly in the command line can be cumbersome and less readable. It‚Äôs preferable to write everything in a script with proper indentation and execute the script as needed.\n\n\n\n\n\n\nExercise\n\n\n\n\nWrite a function named cubediag that takes a matrix A as input and returns the sum of the cubes of the diagonal elements.\nWrite another function named cubeantidiag that returns the sum of the cubes of the anti-diagonal elements (the diagonal that is symmetric to the main diagonal).\n\nYou can define these functions in separate script files or directly in the MATLAB environment.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nfunction s = cubediag(A)\n  s = sum(diag(A).^2);\nend\n\nfunction s = cubeantidiag(A)\n  % we could also use fliplr\n  Aflip = A(:,end:-1:1);\n  s = cubediag(Aflip);\nend\n\nfunction s = cubeantidiag(A)\n  % case of square matrices\n  n = size(A,1);\n  s = sum(A(end‚àín+1:1‚àín:n));\nend\n\n\n\n\n\n\n\nC.3.10 Plots\nTo create a plot of a real-valued function of a real variable in MATLAB, you can use the plot command. In its basic form, you provide two vectors, x and y, of the same size that represent the data points.\n&gt;&gt; x = linspace(-1, 1, 100);\n&gt;&gt; parab = @(x) x.^2;\n&gt;&gt; plot(x, parab(x));\n\n\n\nParabola Plot\n\n\nYou can customize the style, size, and color of the line using various options typically provided after the data points:\n&gt;&gt; x = linspace(-1, 1, 20);\n&gt;&gt; plot(x, parab(x), 'r*--', 'LineWidth', 2.0, 'MarkerSize', 15.0);\n&gt;&gt; grid on;\n&gt;&gt; xlabel('X-Axis');\n&gt;&gt; ylabel('Y-Axis');\n&gt;&gt; title('A Parabola', 'FontWeight', 'bold');\n\n\n\nStyled Parabola Plot\n\n\nTo see the possible combinations of line styles and markers, you can type help plot or doc plot. However, the most commonly used is the third argument, which is a string like r*--, which translates to:\n\nr: Red line\n*: Asterisk-shaped markers\n--: Dashed line\n\nThe order of styles is fixed, but each of them is optional. For example, by simply typing y, you will get a yellow line.\nTo overlay multiple plots, you can follow several approaches:\n\nDirect Overlay:\n&gt;&gt; x = linspace(-1, 1, 20);\n&gt;&gt; parab = @(x) x.^2;\n&gt;&gt; cubic = @(x) x.^3;\n&gt;&gt; plot(x, parab(x), x, cubic(x));\nCreating a Data Matrix:\n&gt;&gt; plot(x, [parab(x); cubic(x)]);\nUsing the hold Command:\n&gt;&gt; plot(x, parab(x));\n&gt;&gt; hold all;\n&gt;&gt; plot(x, cubic(x));\n&gt;&gt; hold off;\n\nYou can also add a legend to distinguish the plots:\n&gt;&gt; legend('Parabola', 'Cubic');\nYou can add multiple plots to the same graph by using the hold command:\n&gt;&gt; plot(x, parab(x));\n&gt;&gt; hold on;\n&gt;&gt; plot(x, cubic(x));\n&gt;&gt; hold off;\nYou can add a legend to differentiate between the plots:\n&gt;&gt; legend('Parabola', 'Cubic');\nAnother important type of plot, especially when evaluating the behavior of errors concerning parameters, is the use of logarithmic or semilogarithmic scales. For a given function \\(f(x)\\) with the graph \\(y = f(x)\\):\n\nsemilogx represents the points after changing the variable \\(x \\mapsto \\log x\\).\nsemilogy represents the points after changing the variable \\(y \\mapsto \\log y\\).\nloglog performs both of the above changes.\n\nFor example, the graph \\(y = e^{\\alpha x}\\), in a \\(y\\)-logarithmic scale, becomes \\(\\tilde{y} := \\log y = \\alpha x\\), which is a straight line.\n&gt;&gt; x = linspace(1, 10, 100);\n&gt;&gt; semilogy(x, exp(x), x, exp(2*x), 'LineWidth', 2.0);\n&gt;&gt; grid on;\n&gt;&gt; legend('Slope 1', 'Slope 2', 'Location', 'NorthWest');\n\n\n\nLogarithmic Plot\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nPlot the graph of the function \\(f(x) = 2 + (x-3)\\sin(5(x-3))\\) for \\(0 \\leq x \\leq 6\\). Overlay dashed lines that bound this function.\nConsider the function \\(f(x) = (\\log x)^2\\) for \\(0.1 \\leq x \\leq 10\\). What do you expect from the graph in logarithmic scaling? Plot and verify.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n&gt;&gt; x = linspace(0, 6, 100);\n&gt;&gt; f = 2 + (x-3).*sin(5*(x-3));\n&gt;&gt; r = [ 2 + (x-3); 2 - (x-3) ];\n&gt;&gt; plot(x, f, 'k-', x, r, 'k--', 'LineWidth', 2.0);\n\n\n\nimage\n\n\nGiven \\(\\hat{x} = \\log x\\) and \\(\\hat{y}=\\log y\\), the plot in \\(x\\)-log scale is \\(y = (\\log 2)^2 = \\hat{x}^2\\). We expect a parabola.\n&gt;&gt; x = 10.^linspace(-2,2,100);\n&gt;&gt; y = log(x).^2;\n&gt;&gt; semilogx(x, y, 'LineWidth', 2.0);\n\n\n\nimage",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>Introduction to MATLAB</span>"
    ]
  },
  {
    "objectID": "Laboratories/00_Intro_Matlab/introMATLAB.html#approximation-error",
    "href": "Laboratories/00_Intro_Matlab/introMATLAB.html#approximation-error",
    "title": "Appendix C ‚Äî Introduction to MATLAB",
    "section": "C.4 Approximation error",
    "text": "C.4 Approximation error\nWhen working with approximations, it‚Äôs essential to measure the accuracy of an approximation \\(\\hat{x}\\) to a number \\(x \\in \\mathbb{R}\\). Two common metrics are:\n\nAbsolute Error: \\(E_{\\text{abs}}(\\hat{x}) = |x - \\hat{x}|\\)\nRelative Error: \\(E_{\\text{rel}}(\\hat{x}) = \\frac{|x - \\hat{x}|}{|x|}\\) for \\(x \\neq 0\\)\n\nSignificant figures are a way to represent the accuracy of an approximation. Given an approximation \\(\\hat{x}\\) to \\(x\\), it has \\(p\\) significant figures if:\n\\[\nE_{\\text{abs}}(\\hat{x}) \\leq \\frac{1}{2} \\times 10^{s-p+1}\n\\]\nWhere \\(s\\) is the largest integer such that \\(|x| \\geq 10^s\\).\nWhen \\(x\\) is not known (common in many practical applications), significant figures can be determined by considering successive approximations. For a sequence \\(\\{x_i\\}_{i=0}^\\infty\\) that converges to \\(x\\), you can calculate the absolute error at each step \\(|x_i - x_{i-1}|\\).\n\n\n\n\n\n\nExercise\n\n\n\nComplete the following table (use format short for relative error):\n\n\n\n\n\n\n\n\n\n\n\\(x\\)\n\\(\\hat{x}\\)\nRelative Error\nAbsolute Error\nSignificant Figures\n\n\n\n\n1.6925\n1.69285\n‚Äì\n‚Äì\n‚Äì\n\n\n23.130\n23.129\n‚Äì\n‚Äì\n‚Äì\n\n\n23.130\n23.1299\n‚Äì\n‚Äì\n‚Äì\n\n\n23.130\n23.129999\n‚Äì\n‚Äì\n‚Äì\n\n\n0.00345\n0.00343\n‚Äì\n‚Äì\n‚Äì\n\n\n0.01008\n0.01012\n‚Äì\n‚Äì\n‚Äì\n\n\n0.01008\n0.01002\n‚Äì\n‚Äì\n‚Äì\n\n\n0.01008\n0.0102\n‚Äì\n‚Äì\n‚Äì\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(x\\)\n\\(\\hat{x}\\)\nRelative Error\nAbsolute Error\nSignificant Figures\n\n\n\n\n1.6925\n1.69285\n\\(0.21\\cdot 10^{-3}\\)\n\\(0.4\\cdot 10^{-3}\\)\n4\n\n\n23.130\n23.129\n\\(0.43\\cdot 10^{-4}\\)\n\\(0.1\\cdot 10^{-2}\\)\n4\n\n\n23.130\n23.1299\n\\(0.43\\cdot 10^{-5}\\)\n\\(0.1\\cdot 10^{-3}\\)\n5\n\n\n23.130\n23.129999\n\\(0.43\\cdot 10^{-7}\\)\n\\(0.1\\cdot 10^{-5}\\)\n7\n\n\n0.00345\n0.00343\n\\(0.58\\cdot 10^{-2}\\)\n\\(0.1\\cdot 10^{-4}\\)\n2\n\n\n0.01008\n0.01012\n\\(0.40\\cdot 10^{-2}\\)\n\\(0.4\\cdot 10^{-4}\\)\n3\n\n\n0.01008\n0.01002\n\\(0.60\\cdot 10^{-2}\\)\n\\(0.6\\cdot 10^{-4}\\)\n2\n\n\n0.01008\n0.0102\n\\(0.12\\cdot 10^{-1}\\)\n\\(0.1\\cdot 10^{-3}\\)\n1\n\n\n\n\n\n\n\n\n\nC.4.1 Floating-Point Arithmetic\nNatural numbers (\\(\\mathbb{N}\\)), although infinite, can be represented exactly on a computer within a predefined range of values. For example, a 32-bit integer can represent numbers from 0 to \\(2^{32}-1\\). Similarly, it can represent signed integers (\\(\\mathbb{Z}\\)) from \\(-2^{31}\\) to \\(2^{31}-1\\).\nReal numbers (\\(\\mathbb{R}\\)), on the other hand, are too numerous to be represented exactly within any chosen range. There are at least two possibilities: the first involves fixing the number of digits after the decimal point (fixed-point). The second considers the proper subset \\(\\mathbb{F} \\subset \\mathbb{R}\\) of floating-point numbers:\n\\[y = \\pm m \\times \\beta^{e-t},\\]\nwhere \\(\\beta\\) is the base, \\(t\\) is the precision, and \\(e \\in [e_{\\text{min}}, e_{\\text{max}}]\\) is the exponent. For example, with \\(\\beta=2\\), \\(t=3\\), \\(e_{\\text{min}}=-1\\), and \\(e_{\\text{max}}=3\\), you can represent numbers like:\n\\[\n\\begin{split}\n0&, 0.25, 0.3125, 0.3750, 0.4375, 0.5, 0.625, 0.750, 0.875, \\\\\n1.0&, 1.25, 1.50, 1.75, 2.0, 2.5, 3.0, 4.0, 5.0, 6.0, 7.0.\n\\end{split}\n\\]\nThe following graph illustrates the spacing of these numbers:\n\n\n\nGraphical representation of floats\n\n\nThe IEEE-754 standard sets values for these parameters in two significant cases: single precision (float in C), where \\(\\beta=2\\), \\(t=24\\), \\(e_{\\text{min}}=-125\\), and \\(e_{\\text{max}}=128\\), and double precision (double in C), where \\(\\beta=2\\), \\(t=53\\), \\(e_{\\text{min}}=-1021\\), and \\(e_{\\text{max}}=1024\\).\nMachine epsilon (\\(\\varepsilon_{\\text{M}}\\)) is the smallest positive number such that \\(1 + \\varepsilon_{\\text{M}} \\neq 1\\) in the floating-point system. It represents the distance between 1 and the next representable number in the system. For single precision, \\(\\varepsilon_{\\text{M}}\\) is approximately \\(1.19 \\times 10^{-7}\\), while for double precision, it‚Äôs approximately \\(2.22 \\times 10^{-16}\\).\nIn MATLAB, you can check these values using the eps function:\n&gt;&gt; eps\nans = \n   2.2204e-16 \nThe limits of representable numbers can also be checked with realmin and realmax:\n&gt;&gt; realmax\nans =\n  1.7977e+308\n&gt;&gt; realmin\nans =\n  2.2251e-308\nAs observed from the previous graph, the spacing between numbers in \\(\\mathbb{F}\\) is not constant. MATLAB‚Äôs eps command allows you to determine these spacings:\n&gt;&gt; eps(1)\nans = \n   2.2204e-16\n&gt;&gt; eps(10)\nans = \n   1.7764e-15\n&gt;&gt; eps(100)\nans = \n   1.4211e-14\n&gt;&gt; eps(1000)\nans =\n   1.1369e-13\nThe set \\(\\mathbb{F}\\) also includes some exceptional cases:\n&gt;&gt; 1e400\nans = \n   Inf\n&gt;&gt; 1e-400\nans =\n     0\n&gt;&gt; 1/0\nans =\n   Inf\n&gt;&gt; 0/0\nans =\n   NaN\n\nThe number Inf represents infinity, meaning it‚Äôs beyond realmax. It‚Äôs not considered an error, and arithmetic operations involving infinity are still valid, although they can often lead to undesirable results.\nWhen dealing with numbers smaller than realmin, they are approximated to 0.\nThe situation NaN (Not-a-Number) indicates that the result is undefined or has no meaningful value.\n\nFloating-point arithmetic has its limitations, including a lack of associativity for some operations, which can lead to significant errors in some cases. For example, when subtracting two nearly equal numbers, you can experience loss of precision due to the finite precision of the representation:\n&gt;&gt; x = 1.0e-15;\n&gt;&gt; (1+x)-1\nans =\n   1.1102e-15\n&gt;&gt; (1-1)+x\nans =\n   1.0000e-15\nThis issue is known as cancellation error and should be avoided whenever possible in numerical computations. The problem is indeed quite serious, and it‚Äôs a common issue in numerical computations, especially when dealing with small numbers. For example, you have two functions:\n\\[f(x) = \\frac{1-\\cos x}{x^2}, \\quad g(x) = \\frac{1}{2}\\left(\\frac{\\sin(x/2)}{x/2}\\right)^2.\\]\nBoth of these functions are identical. However, when you evaluate them for \\(x = 1.2 \\times 10^{-8}\\) in MATLAB, you get different results:\n&gt;&gt; x = 1.2e-8;\n&gt;&gt; (1 - cos(x)) / x^2;\nans =\n    0.7710\n\n&gt;&gt; 0.5 * (sin(x / 2) / (x / 2))^2;\nans =\n    0.5000\nThe first result is clearly incorrect, and it violates the bounds of the function, which should be between 0 and 0.5. This is a classic example of the problem of numerical precision and the limitations of finite-precision arithmetic. Small values of x lead to significant errors due to rounding and truncation.\nTo address such issues, numerical analysts often use techniques like Taylor series expansions, higher precision arithmetic, or specialized algorithms to improve the accuracy of computations involving small numbers.\nIt is good practice to avoid cases like these as much as possible, as they are quite common. For example:\n\\[\n\\begin{aligned}\nf(x) &= \\frac{e^x-1}{x}, &&\\text{for $x \\approx 1$,} \\\\\nx_{1,2} &= \\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}, &&\\text{for $b^2 \\approx 4ac$ and $b \\approx \\sqrt{b^2-4ac}$,} \\\\\ns_n^2 &= \\frac{1}{n-1}\\left( \\sum\\nolimits_{i=1}^n x_i^2 - \\frac{1}{n}\\left(\\sum\\nolimits_{i=1}^n x_i \\right)^2 \\right) &&\n\\end{aligned}\n\\]\nIn the last case, when calculating variance, it is even possible to obtain negative results, which have no mathematical sense.\n\n\n\n\n\n\nExercise (Archimedes‚Äô Method for \\(\\pi\\) Approximation)\n\n\n\nThe strategy used by Archimedes to approximate \\(\\pi\\) involves considering regular polygons inscribed and circumscribed within the unit circle. In fact, if \\(n\\) is the number of sides, the perimeter \\(P_n\\) approaches \\(2\\pi\\) as \\(n \\to \\infty\\).\nStarting with a hexagon and successively doubling the number of sides, it can be found that as \\(i \\to \\infty\\), \\(6 \\cdot 2^i \\cdot t_i\\) approaches \\(\\pi\\), where \\(t_i\\) satisfies the following relation:\n\\[\nt_0 = \\frac{1}{\\sqrt{3}}, \\qquad\nt_{i+1} = \\frac{\\sqrt{t_i^2+1}-1}{t_i}.\n\\]\nHere are the tasks related to this approximation:\n\nImplement this algorithm in MATLAB and compare the approximation with \\(\\pi\\) provided by the pi constant. What do you observe?\nReplace the recursive formula with its equivalent:\n\n\\[\nt_0 = \\frac{1}{\\sqrt{3}}, \\qquad\nt_{i+1} = \\frac{t_i}{\\sqrt{t_i^2+1}+1}.\n\\]\nComment on any differences observed.\nNote: Use format long to display the differences.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe value given by Matlab for \\(\\pi\\) is:\n&gt;&gt; format long\n&gt;&gt; pi\nans =\n   3.141592653589793\nWe compute now \\(\\pi\\) with method 1. pia and method 2. pib:\n&gt;&gt; format long;\n&gt;&gt; n = 30;\n&gt;&gt; ta = 1/sqrt(3);\n&gt;&gt; tb = 1/sqrt(3);\n&gt;&gt; for i = 1:n\n     % first method\n     ta = (sqrt(ta^2+1)-1)/ta;\n     pia = 6*2^i*ta;\n     % second method\n     tb = tb/(sqrt(tb^2+1)+1);\n     pib = 6*2^i*tb;\n&gt;&gt; end\n&gt;&gt; fprintf('Method 1, pi = %f, err = %e\\n', pia, abs(pia-pi));\n&gt;&gt; fprintf('Method 2, pi = %f, err = %e\\n', pib, abs(pib-pi));\nWe see that the first method suffers of cancellation errors.\n\n\n\n\n\n\n\n\n\n\n\nExercise (Polynomial Expansion)\n\n\n\nConsider the polynomial \\(p(x) = (x-1)^7\\) for \\(x \\in [0.998, 1.012]\\). Expand the polynomial using the binomial formula and compare it to the original unexpanded polynomial. Comment on any differences.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nExpanding we have \\[\np(x) = x^7 - 7x^6 + 21x^5 - 35x^4 + 35x^3 - 21x^2 + 7x - 1.\n\\]\nNow we compare\n&gt;&gt; x = 0.998:0.0001:1.012;\n&gt;&gt; plot(x, x.^7-7*x.^6+21*x.^5-35*x.^4+35*x.^3-21*x.^2+7*x-1, ...\n        x, (x-1).^7, 'LineWidth', 2.0);\n&gt;&gt; grid on;\n&gt;&gt; legend('Expanded', 'Original');\n\n\n\nimage",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>Introduction to MATLAB</span>"
    ]
  }
]